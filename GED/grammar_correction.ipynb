{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPG36IkfB0qs"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YH_f76CuAtr4",
    "outputId": "d52442bc-56c7-4100-ee0d-38c10c85ffb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-7cky4F3Hea8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqNCc2oPApHa",
    "outputId": "13f0ed13-664e-4359-ec2e-2fab921c1bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a1UoiIpGIPgl"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('grammer_error_dataset/train.csv')\n",
    "df_val = pd.read_csv('grammer_error_dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRnLeS81Ic9t"
   },
   "outputs": [],
   "source": [
    "unique,counts = np.unique(df_train.grammatically_incorrect,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpHlvedYRko1"
   },
   "outputs": [],
   "source": [
    "unq,cnt = np.unique(df_val.grammatically_incorrect,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzKD5z7SgjOi",
    "outputId": "7504622f-fbf6-4df7-972c-e60e35709e00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9524, 2851]), array([539, 158]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts,cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "BfPcmDVoId3D",
    "outputId": "e6479fa2-d424-46bd-afb3-0debe01fa9a7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas._libs.algos import pad\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "barlist = ax.bar(['Grammatically Correct','Grammatically Incorrect'],counts,width=0.3)\n",
    "barlist_2 = ax.bar(['Grammatically Correct','Grammatically Incorrect'],cnt,width=0.3)\n",
    "barlist[1].set_color('r')\n",
    "barlist_2[1].set_color('r')\n",
    "#ax.set_title('Number of Grammatically Correct/Incorrect abstracts',pad=30)\n",
    "ax.set_ylabel('Number of abstracts')\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "plt.show()\n",
    "fig.savefig('foo.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VAerO3ZBdfd"
   },
   "source": [
    "### Visualize Co-occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xu0Ph1slBwmx"
   },
   "outputs": [],
   "source": [
    "cooccurrence_matrix= np.dot(arr_err.transpose(),arr_err)\n",
    "cooccurrence_matrix_diagonal = np.diagonal(cooccurrence_matrix)\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    cooccurrence_matrix_percentage = np.nan_to_num(np.true_divide(cooccurrence_matrix, cooccurrence_matrix_diagonal[:, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHVI3JGBD12Y"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHQ8fAv9ElmB",
    "outputId": "6c129938-cdc4-492f-a8ac-96c45838444f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[880, 223, 224, 223, 213],\n",
       "       [223, 965, 255, 243, 259],\n",
       "       [224, 255, 948, 242, 250],\n",
       "       [223, 243, 242, 956, 231],\n",
       "       [213, 259, 250, 231, 943]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(arr_err.transpose(),arr_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "7MimUknye5N-",
    "outputId": "59fb3b6d-4ef2-48b6-8211-c3f5daf7ff85"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_headers = ['Error 1','Error 2','Error 3','Error 4','Error 5']\n",
    "plt.figure(figsize=(12,8))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(corr_mat, annot=True, ax = ax,fmt='g',cmap='PuBuGn'); #annot=True to annotate cells\n",
    "sns.set(font_scale=1.4)\n",
    "# labels, title and ticks\n",
    "#ax.set_title('Confusion Matrix - Coarse-grained Classification',pad = 30).set_fontsize('20')\n",
    "ax.set_xticks(np.arange(len(label_headers)),minor=True)\n",
    "ax.set_yticks(np.arange(len(label_headers)),minor=True)\n",
    "\n",
    "ax.set_xticklabels(label_headers,ha='center')\n",
    "ax.set_yticklabels(label_headers,ha='center')\n",
    "\n",
    "plt.savefig('cm.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR1pyxXIQRU5"
   },
   "outputs": [],
   "source": [
    "unq,cnt = np.unique(np.array([''.join(row) for row in arr_err.astype(str).tolist()]),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gqyao6xlQ-mr"
   },
   "outputs": [],
   "source": [
    "dict_labels = dict(zip(unq,cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEoOVCvJSN7K",
    "outputId": "0b9bcbc0-443f-4cbc-af5d-c7ee63e14874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10000' '10001' '10010' '10011' '10100' '10101' '10110' '10111' '11000'\n",
      " '11001' '11010' '11011' '11100' '11101' '11110' '11111']\n"
     ]
    }
   ],
   "source": [
    "# Err1\n",
    "keys_with_err1 = np.where(np.char.startswith(unq,'1'))\n",
    "\n",
    "for i in keys_with_err1:\n",
    "  print(unq[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "KCtM3afHc0PO",
    "outputId": "861d952f-1040-4597-c24c-8bf57fcb9e10"
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e0ca019bd37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# The device name should look like the following:\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PKCwvnQFjRm",
    "outputId": "74346ef2-ac31-4176-a8cb-f3d58cd8db7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():        \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLS93owyg9qz",
    "outputId": "840d1d9d-5955-4c81-b219-954828c76e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BF8A-PcHl0b"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "cad52571456142d88142a33c1879d6c2",
      "0b98666d4e6f45318838ef815df1733a",
      "079868b647f84c9abe244c211927ad87",
      "228ac4df07c64e0fa7c437cd41f9c989",
      "e060ea4f3b6c468aa6cb42025affb08e",
      "1995bda78a184218bfd9c3ecfe3eac72",
      "70aefddc86304776ab0ea575af02c1cc",
      "d2afe42dbbbc44f886ecf44e47e6af11",
      "0c977b73879d448c9157f84841f4a810",
      "fce305678b1a429bb91d1994ae3f06d8",
      "b893dbebfd524633bb209688e7efd543",
      "4bc17b26fb58460fb1481b26c4273e7e",
      "68138007ea8d4831bf070709133d1044",
      "551f59f1100548ad93bb38169dc7796c",
      "111b052a5afb42e89700f8605e9a4b1d",
      "e0f115bc99a549f6b61e874f4b51f5ee",
      "cb9bd771ca224433bb8f70435175f4e7",
      "b09ccec106ae4d49bf097b315aae9ecc",
      "2ffa06c691ac47bd8433ed37f323aa5a",
      "c3e9b59739124ea0b3fbfe0897cb94c0",
      "32ab58b7457547138c3320f4bda6f418",
      "d65d791428784b84abf983ed73c80e90",
      "cccc4c58c0ee421c8f960e27148d5c60",
      "3e839a1ae58742358aef97945644dfb0",
      "6ff74b5bd64c4d33ab223183f7770a1e",
      "c1fb1df089a7451c9706df2ca25a71ae",
      "98c521a59c8549a4899803b7e4d78cd6",
      "460a41290c7a4fe0bac5fa3465f0f856",
      "56437b6cfa574086b0c735c1ffa76610",
      "9b6df9a148a349e2bfae507c82690606",
      "bc100704a10a47dfbcc10eb05c0e8da6",
      "435750df0bbb4852bbeeb2652c6d098c",
      "5d348978a2c34704afb30160669301d5"
     ]
    },
    "id": "460800eTHwqa",
    "outputId": "67e28f03-59ae-489e-e2ec-9654f3df958c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad52571456142d88142a33c1879d6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc17b26fb58460fb1481b26c4273e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/329k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccc4c58c0ee421c8f960e27148d5c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/327 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('anferico/bert-for-patents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJUBwr7WwE6F"
   },
   "source": [
    "###**1. Coarse-grained Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMXMEPoNILQL"
   },
   "outputs": [],
   "source": [
    "sentences = df_train.abstracts.values\n",
    "labels = df_train.grammatically_incorrect.values\n",
    "sentences_val = df_val.abstracts.values\n",
    "labels_val = df_val.grammatically_incorrect.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNtAiKU7IZwE",
    "outputId": "e2612c63-dc43-45b9-bf00-cd01f2d7809d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  disclosed is a composition and method for reducing cravings for a craved substance , particularly foods , with preparations containing 5 - hydroxytryptophan . the 5 - htp is added to the same or similar substance that is actually craved to reduce the craving for the craved substance while further satisfying the craving by consumption of a reduced amount of the craved substance .\n",
      "Tokenized:  ['disclosed', 'is', 'a', 'composition', 'and', 'method', 'for', 'reducing', 'craving', '##s', 'for', 'a', 'craved', 'substance', ',', 'particularly', 'foods', ',', 'with', 'preparations', 'containing', '5', '-', 'hydroxy', '##try', '##pt', '##op', '##han', '.', 'the', '5', '-', 'h', '##tp', 'is', 'added', 'to', 'the', 'same', 'or', 'similar', 'substance', 'that', 'is', 'actually', 'craved', 'to', 'reduce', 'the', 'craving', 'for', 'the', 'craved', 'substance', 'while', 'further', 'satisfying', 'the', 'craving', 'by', 'consumption', 'of', 'a', 'reduced', 'amount', 'of', 'the', 'craved', 'substance', '.']\n",
      "Token IDs:  [21027, 1668, 1042, 5177, 1663, 3783, 1670, 7826, 26034, 1680, 1670, 1042, 24820, 9080, 1015, 3056, 9105, 1015, 1672, 12594, 4485, 1024, 1016, 29961, 10794, 13541, 7026, 4484, 1017, 1661, 1024, 1016, 1049, 25521, 1668, 2459, 1665, 1661, 1833, 1695, 2379, 9080, 1673, 1668, 2606, 24820, 1665, 5212, 1661, 26034, 1670, 1661, 24820, 9080, 1761, 2247, 16752, 1661, 26034, 1676, 8046, 1662, 1042, 4024, 3480, 1662, 1661, 24820, 9080, 1017]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', sentences[0])\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M486zBFbIpAW",
    "outputId": "2c36a520-e8b7-4f59-c379-d53e5e0a1b1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  disclosed is a composition and method for reducing cravings for a craved substance , particularly foods , with preparations containing 5 - hydroxytryptophan . the 5 - htp is added to the same or similar substance that is actually craved to reduce the craving for the craved substance while further satisfying the craving by consumption of a reduced amount of the craved substance .\n",
      "Token IDs: tensor([    2, 21027,  1668,  1042,  5177,  1663,  3783,  1670,  7826, 26034,\n",
      "         1680,  1670,  1042, 24820,  9080,  1015,  3056,  9105,  1015,  1672,\n",
      "        12594,  4485,  1024,  1016, 29961, 10794, 13541,  7026,  4484,  1017,\n",
      "         1661,  1024,  1016,  1049, 25521,  1668,  2459,  1665,  1661,  1833,\n",
      "         1695,  2379,  9080,  1673,  1668,  2606, 24820,  1665,  5212,  1661,\n",
      "        26034,  1670,  1661, 24820,  9080,  1761,  2247, 16752,  1661, 26034,\n",
      "         1676,  8046,  1662,     3])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 64,          \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',  \n",
    "                   )\n",
    "    \n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uf8VFA8CI-oo",
    "outputId": "9e91f9a6-b0e3-4ca5-ac87-a3c2b694d316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  a process for granulating ammonium phosphate containing composition for use as a fertilizer which comprises feeding a slurry or melt of ammonium phosphate , wherein said slurry or melt is capable of further reaction with ammonia , into a kneading mill , feeding recycled particles from the subsequent classification and crushing procedure into said kneading mill , wherein the said slurry or melt added to the mill is from 5 to 95 % wt . of the total slurry or melt , feeding ammonia into said mill to react with at least a portion of the reactable components of said slurry or melt , generating a heat of reaction , subjecting said mixture to a kneading action within said mill until the heat of reaction and the kneading action cause a repeated disposition of fertilizer material onto the solids and cause a repeated drying of said deposit , passing the product from said kneading mill into a rotary drum granulator , feeding additional ammonium phosphate containing slurry or melt to a said granulator in an amount of from 95 % to 5 % wt . based on the total slurry or melt , feeding additional amounts of ammonia to said granulator wherein at least a portion of the reactable components of said slurry or melt is reacted , giving off a heat of reaction , subjecting said mixture to a rotary action until said heat of reaction and tumbling action of the rotary granulator effects a further deposition of fertilizer material onto the solids , passing the product from said granulator into a dryer so as to remove excess water , subjecting said dried product to a classification apparatus so as to separate out particles which are outside the product size range , recycling at least a portion of said particle mixture to said kneading mill , and recovering said dried particles of product size . commensurate apparatus is provided to perform said process .\n",
      "Token IDs: tensor([    2,  1042,  2497,  1670, 12269, 10589, 30003, 17009,  4485,  5177,\n",
      "         1670,  1889,  1669,  1042, 31787,  1694,  8346,  8186,  1042, 30111,\n",
      "         1695, 14564,  1662, 30003, 17009,  1015, 16391,  1721, 30111,  1695,\n",
      "        14564,  1668,  4879,  1662,  2247,  4333,  1672, 25539,  1015,  1711,\n",
      "         1042, 32076,  4636,  1015,  8186, 21872,  8974,  1678,  1661,  4410,\n",
      "         5244,  1663, 14192,  7374,  1711,  1721, 32076,  4636,  1015, 16391,\n",
      "         1661,  1721, 30111,     3])\n"
     ]
    }
   ],
   "source": [
    "input_ids_val = []\n",
    "attention_masks_val = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_val:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "  \n",
    "    input_ids_val.append(encoded_dict['input_ids'])\n",
    "    attention_masks_val.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "print('Original: ', sentences_val[0])\n",
    "print('Token IDs:', input_ids_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F72yrnFRMWLH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulRs2iXaMnsI"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b8oKhU4M5yr"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zm5xJNkhNGpB"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7YEAs3AI8Rl",
    "outputId": "d7650b9b-f1e2-45cc-b077-cb1347d0c272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.12.0+cu113)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.1.1)\n",
      "Building wheels for collected packages: torchviz\n",
      "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=7352449c11d29635b587d675c45a5f6bdf933a60fc574782bb579ae72ee34b93\n",
      "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
      "Successfully built torchviz\n",
      "Installing collected packages: torchviz\n",
      "Successfully installed torchviz-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJhbcRppI-qR"
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6fb47e74166d4625810fc6fab9225a48",
      "da9cd7a96c3d40e28d51d06ad0987337",
      "8dae5d96462a4c99b927b9c0c3e81cfc",
      "802ca04b8d6a4960ac766fe7d41fb046",
      "0b864a89a684486d8c4fdb21c326a149",
      "046489e804ed43a083268204cd680585",
      "013f9ea7fe5d4250a73d70335f4fae23",
      "0b04362cc9e74eea94b02bd61c675de4",
      "e549ee1e7d544af5ad6f767d798bda57",
      "15b0f907018e49adb5280bcb9c584e2d",
      "1fcd3afb6cfa4dd6a96647a9301c1e38"
     ]
    },
    "id": "F5_T_Ql-NLrD",
    "outputId": "b75ffdac-73e9-49ce-a2d6-9c7672cf03f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb47e74166d4625810fc6fab9225a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at anferico/bert-for-patents were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at anferico/bert-for-patents and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(39859, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \n",
    "    num_labels = 2, \n",
    "                   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, \n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BecdXiFmJGS_",
    "outputId": "5bac618d-fa33-48e5-92bc-f12945bca42b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.513533 to fit\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_dataloader):\n",
    "  \n",
    "  if i == 0:\n",
    "    yhat = model(batch[0].to(device),token_type_ids=None,attention_mask=batch[1].to(device),labels=batch[2].to(device))\n",
    "    make_dot(yhat[1], params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")\n",
    "  else:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6d519AWNVj0",
    "outputId": "4aef7605-2df8-4395-9af1-482292245f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 393 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (39859, 1024)\n",
      "bert.embeddings.position_embeddings.weight               (512, 1024)\n",
      "bert.embeddings.token_type_embeddings.weight               (2, 1024)\n",
      "bert.embeddings.LayerNorm.weight                             (1024,)\n",
      "bert.embeddings.LayerNorm.bias                               (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.query.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.key.bias                 (1024,)\n",
      "bert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.value.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\n",
      "bert.encoder.layer.0.attention.output.dense.bias             (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (4096,)\n",
      "bert.encoder.layer.0.output.dense.weight                (1024, 4096)\n",
      "bert.encoder.layer.0.output.dense.bias                       (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                (1024, 1024)\n",
      "bert.pooler.dense.bias                                       (1024,)\n",
      "classifier.weight                                          (2, 1024)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDuatlTzPZT6",
    "outputId": "f915c23f-b687-4306-a601-cf3588ebaa55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, \n",
    "                  eps = 1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCAa8yauPfTc"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUTytRZ8Pi9h"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vpyjX6tPohd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-tfGyVsSUVq"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lhc9DSpXPq3J",
    "outputId": "0289ff33-fb30-47fc-99b6-242e4e9f0143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    387.    Elapsed: 0:00:48.\n",
      "  Batch    80  of    387.    Elapsed: 0:01:35.\n",
      "  Batch   120  of    387.    Elapsed: 0:02:25.\n",
      "  Batch   160  of    387.    Elapsed: 0:03:13.\n",
      "  Batch   200  of    387.    Elapsed: 0:04:02.\n",
      "  Batch   240  of    387.    Elapsed: 0:04:51.\n",
      "  Batch   280  of    387.    Elapsed: 0:05:40.\n",
      "  Batch   320  of    387.    Elapsed: 0:06:29.\n",
      "  Batch   360  of    387.    Elapsed: 0:07:18.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epcoh took: 0:07:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9787\n",
      "  Validation Loss: 0.0810\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    387.    Elapsed: 0:00:49.\n",
      "  Batch    80  of    387.    Elapsed: 0:01:38.\n",
      "  Batch   120  of    387.    Elapsed: 0:02:27.\n",
      "  Batch   160  of    387.    Elapsed: 0:03:15.\n",
      "  Batch   200  of    387.    Elapsed: 0:04:04.\n",
      "  Batch   240  of    387.    Elapsed: 0:04:53.\n",
      "  Batch   280  of    387.    Elapsed: 0:05:42.\n",
      "  Batch   320  of    387.    Elapsed: 0:06:31.\n",
      "  Batch   360  of    387.    Elapsed: 0:07:20.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 0:07:53\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9844\n",
      "  Validation Loss: 0.0774\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:16:01 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "  \n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "     \n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "    \n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():    \n",
    "            outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "iBxB1Yk5dydj",
    "outputId": "c9a31616-0b98-4227-814d-5872543d0bd5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwNdWq0md3Gb"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "#df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/thesis_PQAI/grammer_error_dataset/test.csv')\n",
    "\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/thesis_PQAI/Relevancy_checker/queries.txt','r') as file:\n",
    "  queries = file.readlines()\n",
    "queries_formatted = []\n",
    "for i in queries:\n",
    "  queries_formatted.append(i.strip())\n",
    "fake_labels = np.zeros(len(queries_formatted))\n",
    "sentences = queries_formatted\n",
    "#labels_test = df_test.grammatically_incorrect.values\n",
    "labels_test = fake_labels\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_test.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "labels_test = torch.tensor(labels_test)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids_test, attention_masks_test,labels_test)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLlwP34BfDhQ",
    "outputId": "12f20770-ef7b-4d49-dd97-a307079241ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 21 test sentences...\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids_test)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  #label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhvklpmXf_0q"
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3JpvqeLN5q1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,recall_score,precision_score,accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NH7i6WfiOJY8",
    "outputId": "3ff3e258-731f-4096-daa0-9cd9ff66db74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       539\n",
      "           1       0.97      0.94      0.95       158\n",
      "\n",
      "    accuracy                           0.98       697\n",
      "   macro avg       0.97      0.96      0.97       697\n",
      "weighted avg       0.98      0.98      0.98       697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(flat_true_labels, flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKa-n-EmOax_",
    "outputId": "1ff5ee62-d79a-4869-9ef9-ba7d0cecce40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9308176100628931\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(flat_true_labels,flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrIEvgY0bEn5",
    "outputId": "38973127-7737-4cda-9f1e-1d8aefd592a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9367088607594937\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(flat_true_labels,flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P3lyfUibG_G",
    "outputId": "e210c260-8823-4ff5-e4f1-9fe951615de9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96987087517934\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(flat_true_labels,flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Bhsw90tb5Gs",
    "outputId": "ca85209c-785f-440b-a97e-8bc6d5cb6d86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9337539432176657"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(flat_true_labels,flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRZVhhnlPwAk"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "YWY3KZW_dhrA",
    "outputId": "5d80a8b2-b547-4911-d9c0-b910207e4e48"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix(flat_true_labels,flat_predictions), annot=True, ax = ax,fmt='g',cmap='Pastel1'); #annot=True to annotate cells\n",
    "sns.set(font_scale=2)\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels').set_fontsize('18')\n",
    "ax.set_ylabel('True labels').set_fontsize('18')\n",
    "#ax.set_title('Confusion Matrix - Coarse-grained Classification',pad = 30).set_fontsize('20')\n",
    "ax.xaxis.set_ticklabels(['Gramm. Correct','Gramm. Incorrect'],rotation=0)\n",
    "ax.yaxis.set_ticklabels(['Gramm. Correct','Gramm. Incorrect'],rotation=0)\n",
    "plt.savefig('cm.png',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xI8U8nsrdxDw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaCIRxltwm6m"
   },
   "source": [
    "###2. Fine-grained Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qM4596Twqk7"
   },
   "outputs": [],
   "source": [
    "sentences = df_train.abstracts.values\n",
    "labels = df_train[['err_1','err_2','err_3','err_4','err_5']].values\n",
    "sentences_val = df_val.abstracts.values\n",
    "labels_val = df_val[['err_1','err_2','err_3','err_4','err_5']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAIp6J7_xyAc",
    "outputId": "36003256-b587-4e80-c35d-b6be7dfc4975"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M2XD28Ex0FY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0lL1yV3yXIt",
    "outputId": "369a9663-3508-4bc5-b32f-855969dce452"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2299: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  disclosed is a composition and method for reducing cravings for a craved substance , particularly foods , with preparations containing 5 - hydroxytryptophan . the 5 - htp is added to the same or similar substance that is actually craved to reduce the craving for the craved substance while further satisfying the craving by consumption of a reduced amount of the craved substance .\n",
      "Token IDs: tensor([    2, 21027,  1668,  1042,  5177,  1663,  3783,  1670,  7826, 26034,\n",
      "         1680,  1670,  1042, 24820,  9080,  1015,  3056,  9105,  1015,  1672,\n",
      "        12594,  4485,  1024,  1016, 29961, 10794, 13541,  7026,  4484,  1017,\n",
      "         1661,  1024,  1016,  1049, 25521,  1668,  2459,  1665,  1661,  1833,\n",
      "         1695,  2379,  9080,  1673,  1668,  2606, 24820,  1665,  5212,  1661,\n",
      "        26034,  1670,  1661, 24820,  9080,  1761,  2247, 16752,  1661, 26034,\n",
      "         1676,  8046,  1662,     3])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioAWmimEyXIw",
    "outputId": "957fc583-8ed2-4018-881b-ad071b6b8948"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2299: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  a process for granulating ammonium phosphate containing composition for use as a fertilizer which comprises feeding a slurry or melt of ammonium phosphate , wherein said slurry or melt is capable of further reaction with ammonia , into a kneading mill , feeding recycled particles from the subsequent classification and crushing procedure into said kneading mill , wherein the said slurry or melt added to the mill is from 5 to 95 % wt . of the total slurry or melt , feeding ammonia into said mill to react with at least a portion of the reactable components of said slurry or melt , generating a heat of reaction , subjecting said mixture to a kneading action within said mill until the heat of reaction and the kneading action cause a repeated disposition of fertilizer material onto the solids and cause a repeated drying of said deposit , passing the product from said kneading mill into a rotary drum granulator , feeding additional ammonium phosphate containing slurry or melt to a said granulator in an amount of from 95 % to 5 % wt . based on the total slurry or melt , feeding additional amounts of ammonia to said granulator wherein at least a portion of the reactable components of said slurry or melt is reacted , giving off a heat of reaction , subjecting said mixture to a rotary action until said heat of reaction and tumbling action of the rotary granulator effects a further deposition of fertilizer material onto the solids , passing the product from said granulator into a dryer so as to remove excess water , subjecting said dried product to a classification apparatus so as to separate out particles which are outside the product size range , recycling at least a portion of said particle mixture to said kneading mill , and recovering said dried particles of product size . commensurate apparatus is provided to perform said process .\n",
      "Token IDs: tensor([    2,  1042,  2497,  1670, 12269, 10589, 30003, 17009,  4485,  5177,\n",
      "         1670,  1889,  1669,  1042, 31787,  1694,  8346,  8186,  1042, 30111,\n",
      "         1695, 14564,  1662, 30003, 17009,  1015, 16391,  1721, 30111,  1695,\n",
      "        14564,  1668,  4879,  1662,  2247,  4333,  1672, 25539,  1015,  1711,\n",
      "         1042, 32076,  4636,  1015,  8186, 21872,  8974,  1678,  1661,  4410,\n",
      "         5244,  1663, 14192,  7374,  1711,  1721, 32076,  4636,  1015, 16391,\n",
      "         1661,  1721, 30111,     3])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_val = []\n",
    "attention_masks_val = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences_val:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_val.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_val.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences_val[0])\n",
    "print('Token IDs:', input_ids_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91dooKUQyXIy"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_e1Yoh2yXI0"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17gULlZN01-f",
    "outputId": "01e31b1d-06f9-4662-d694-923dd99d0fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,  1042,  2497,  1670, 12269, 10589, 30003, 17009,  4485,  5177,\n",
       "          1670,  1889,  1669,  1042, 31787,  1694,  8346,  8186,  1042, 30111,\n",
       "          1695, 14564,  1662, 30003, 17009,  1015, 16391,  1721, 30111,  1695,\n",
       "         14564,  1668,  4879,  1662,  2247,  4333,  1672, 25539,  1015,  1711,\n",
       "          1042, 32076,  4636,  1015,  8186, 21872,  8974,  1678,  1661,  4410,\n",
       "          5244,  1663, 14192,  7374,  1711,  1721, 32076,  4636,  1015, 16391,\n",
       "          1661,  1721, 30111,     3]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor([0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "vdT16MkqyXI1",
    "outputId": "d3334626-6cd5-48b9-9538-91dc9394ac6b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cd79b953993a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# We'll take training samples in random order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m train_dataloader = DataLoader(\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# The training samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Select batches randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;31m# Trains with this batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY9rnuFayXI3"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hmHLIjmEe8I"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAIISgpSD_0j"
   },
   "outputs": [],
   "source": [
    "class FineGrainedBertClassificationModel(nn.Module):\n",
    "  def __init__(self,num_labels) -> None:\n",
    "    super(FineGrainedBertClassificationModel,self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(\"anferico/bert-for-patents\")\n",
    "    self.linear1 = nn.Linear(1024, 512)\n",
    "    self.linear2 = nn.Linear(512,num_labels)\n",
    "    #self.sig = nn.functional.sigmoid()\n",
    "  \n",
    "  def forward(self,ids,masks,labels):\n",
    "    sequence_output, pooled_output = self.bert(ids,attention_mask=masks,return_dict=False)\n",
    "    linear1_output = self.linear1(sequence_output[:,0,:].view(-1,1024)) \n",
    "    linear2_output = self.linear2(linear1_output)\n",
    "    output = torch.sigmoid(linear2_output)\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    loss = loss_fn(output.to(torch.float32),labels.to(torch.float32))\n",
    "    return [loss,output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow8pgQP2yXI5"
   },
   "outputs": [],
   "source": [
    "model = FineGrainedBertClassificationModel(num_labels=5)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NN-KmoDcyXI8",
    "outputId": "1cb1d136-6c74-4b3c-a038-2e4b6021d461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 395 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (39859, 1024)\n",
      "bert.embeddings.position_embeddings.weight               (512, 1024)\n",
      "bert.embeddings.token_type_embeddings.weight               (2, 1024)\n",
      "bert.embeddings.LayerNorm.weight                             (1024,)\n",
      "bert.embeddings.LayerNorm.bias                               (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.query.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.key.bias                 (1024,)\n",
      "bert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.value.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\n",
      "bert.encoder.layer.0.attention.output.dense.bias             (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (4096,)\n",
      "bert.encoder.layer.0.output.dense.weight                (1024, 4096)\n",
      "bert.encoder.layer.0.output.dense.bias                       (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "linear1.weight                                           (512, 1024)\n",
      "linear1.bias                                                  (512,)\n",
      "linear2.weight                                              (5, 512)\n",
      "linear2.bias                                                    (5,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQX0CkBPyXI9",
    "outputId": "928f420f-13a6-4539-acfa-264a1b86630b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnGaoIgqyXI9"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AGTz5tI-WVN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_u2LOxq5yXI-"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_hard = 1 * (preds > 0.5)\n",
    "    return accuracy_score(labels,pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5E1K0FURyXI_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SSC_SByXI_"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_uKAp9ryXI_",
    "outputId": "adf89970-a48a-4476-d970-35378ecf01f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    774.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    774.    Elapsed: 0:00:58.\n",
      "  Batch   120  of    774.    Elapsed: 0:01:28.\n",
      "  Batch   160  of    774.    Elapsed: 0:01:57.\n",
      "  Batch   200  of    774.    Elapsed: 0:02:26.\n",
      "  Batch   240  of    774.    Elapsed: 0:02:55.\n",
      "  Batch   280  of    774.    Elapsed: 0:03:24.\n",
      "  Batch   320  of    774.    Elapsed: 0:03:53.\n",
      "  Batch   360  of    774.    Elapsed: 0:04:22.\n",
      "  Batch   400  of    774.    Elapsed: 0:04:51.\n",
      "  Batch   440  of    774.    Elapsed: 0:05:20.\n",
      "  Batch   480  of    774.    Elapsed: 0:05:50.\n",
      "  Batch   520  of    774.    Elapsed: 0:06:19.\n",
      "  Batch   560  of    774.    Elapsed: 0:06:48.\n",
      "  Batch   600  of    774.    Elapsed: 0:07:17.\n",
      "  Batch   640  of    774.    Elapsed: 0:07:46.\n",
      "  Batch   680  of    774.    Elapsed: 0:08:15.\n",
      "  Batch   720  of    774.    Elapsed: 0:08:44.\n",
      "  Batch   760  of    774.    Elapsed: 0:09:13.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epcoh took: 0:09:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9279\n",
      "  Validation Loss: 0.0535\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    774.    Elapsed: 0:00:29.\n",
      "  Batch    80  of    774.    Elapsed: 0:00:58.\n",
      "  Batch   120  of    774.    Elapsed: 0:01:27.\n",
      "  Batch   160  of    774.    Elapsed: 0:01:56.\n",
      "  Batch   200  of    774.    Elapsed: 0:02:25.\n",
      "  Batch   240  of    774.    Elapsed: 0:02:54.\n",
      "  Batch   280  of    774.    Elapsed: 0:03:23.\n",
      "  Batch   320  of    774.    Elapsed: 0:03:52.\n",
      "  Batch   360  of    774.    Elapsed: 0:04:21.\n",
      "  Batch   400  of    774.    Elapsed: 0:04:50.\n",
      "  Batch   440  of    774.    Elapsed: 0:05:19.\n",
      "  Batch   480  of    774.    Elapsed: 0:05:48.\n",
      "  Batch   520  of    774.    Elapsed: 0:06:17.\n",
      "  Batch   560  of    774.    Elapsed: 0:06:46.\n",
      "  Batch   600  of    774.    Elapsed: 0:07:16.\n",
      "  Batch   640  of    774.    Elapsed: 0:07:45.\n",
      "  Batch   680  of    774.    Elapsed: 0:08:14.\n",
      "  Batch   720  of    774.    Elapsed: 0:08:43.\n",
      "  Batch   760  of    774.    Elapsed: 0:09:12.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:09:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9435\n",
      "  Validation Loss: 0.0477\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    774.    Elapsed: 0:00:29.\n",
      "  Batch    80  of    774.    Elapsed: 0:00:58.\n",
      "  Batch   120  of    774.    Elapsed: 0:01:27.\n",
      "  Batch   160  of    774.    Elapsed: 0:01:56.\n",
      "  Batch   200  of    774.    Elapsed: 0:02:26.\n",
      "  Batch   240  of    774.    Elapsed: 0:02:55.\n",
      "  Batch   280  of    774.    Elapsed: 0:03:24.\n",
      "  Batch   320  of    774.    Elapsed: 0:03:53.\n",
      "  Batch   360  of    774.    Elapsed: 0:04:22.\n",
      "  Batch   400  of    774.    Elapsed: 0:04:51.\n",
      "  Batch   440  of    774.    Elapsed: 0:05:20.\n",
      "  Batch   480  of    774.    Elapsed: 0:05:49.\n",
      "  Batch   520  of    774.    Elapsed: 0:06:18.\n",
      "  Batch   560  of    774.    Elapsed: 0:06:47.\n",
      "  Batch   600  of    774.    Elapsed: 0:07:16.\n",
      "  Batch   640  of    774.    Elapsed: 0:07:45.\n",
      "  Batch   680  of    774.    Elapsed: 0:08:14.\n",
      "  Batch   720  of    774.    Elapsed: 0:08:43.\n",
      "  Batch   760  of    774.    Elapsed: 0:09:12.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:09:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.9392\n",
      "  Validation Loss: 0.0491\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:28:35 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        \n",
    "        outputs = model(ids=b_input_ids,masks=b_input_mask,labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "      \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "      \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids =b_input_ids,masks=b_input_mask,labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "      \n",
    "        flat_acc = flat_accuracy(logits, label_ids)\n",
    "        total_eval_accuracy += flat_acc\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "rRRkUF2RyXJB",
    "outputId": "0ef4386e-ef5e-4b6a-c148-ad79960e86f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-bbb876bc-4a3b-4083-8cc1-4e4d7e43c8e2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0:09:23</td>\n",
       "      <td>0:00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0:09:22</td>\n",
       "      <td>0:00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0:09:22</td>\n",
       "      <td>0:00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbb876bc-4a3b-4083-8cc1-4e4d7e43c8e2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-bbb876bc-4a3b-4083-8cc1-4e4d7e43c8e2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-bbb876bc-4a3b-4083-8cc1-4e4d7e43c8e2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.09         0.05           0.93       0:09:23         0:00:09\n",
       "2               0.04         0.05           0.94       0:09:22         0:00:09\n",
       "3               0.02         0.05           0.94       0:09:22         0:00:09"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "UoJLMICQyXJC",
    "outputId": "7f76010c-c0b5-4675-d67f-d72151c14758"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rX2GycQyXJC",
    "outputId": "18a25100-cb43-41f1-f528-fde05a8ee842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 697\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2299: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df_test = pd.read_csv('grammer_error_dataset/test.csv')\n",
    "\n",
    "print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))\n",
    "\n",
    "\n",
    "sentences = df_test.abstracts.values\n",
    "labels_test = df_test[['err_1','err_2','err_3','err_4','err_5']].values\n",
    "\n",
    "\n",
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 64,          \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',    \n",
    "                   )\n",
    "     \n",
    "    input_ids_test.append(encoded_dict['input_ids'])\n",
    "    \n",
    " \n",
    "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "labels_test = torch.tensor(labels_test)\n",
    " \n",
    "batch_size = 16\n",
    "\n",
    "prediction_data = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y5sioTYIR35"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load('/content/drive/MyDrive/Colab Notebooks/thesis_PQAI/ged-fg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3-TBUpwyXJD",
    "outputId": "1772514c-a285-413d-e30a-b119078ef8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 697 test sentences...\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids_test)))\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "\n",
    "  b_input_ids = batch[0].to(device)\n",
    "  b_input_mask = batch[1].to(device)\n",
    "  b_labels = batch[2].to(device)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      \n",
    "      outputs = model(ids =b_input_ids,masks=b_input_mask,labels=b_labels)\n",
    "      logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZygBWJayXJE"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(ids =input_ids_test.to(device),masks=attention_masks_test.to(device),labels=labels_test.to(device))\n",
    "    logits = outputs[1]\n",
    "preds = logits.detach().cpu().numpy()\n",
    "test_label_ids = labels_test.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmVL4o-eSsaW",
    "outputId": "a3676c82-fa31-4486-fa00-e99e423534c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93974175035868"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_hard = 1 * (preds > 0.5)\n",
    "accuracy_score(test_label_ids,pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKHVsAJPyXJF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,multilabel_confusion_matrix,recall_score,precision_score,f1_score,average_precision_score,jaccard_score,hamming_loss,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23K7MpL6yXJG",
    "outputId": "53ec7353-4657-4e38-9fbc-559ea8846701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87        51\n",
      "           1       0.90      0.87      0.89        54\n",
      "           2       0.92      0.89      0.91        55\n",
      "           3       1.00      0.87      0.93        52\n",
      "           4       0.98      0.98      0.98        51\n",
      "\n",
      "   micro avg       0.94      0.89      0.91       263\n",
      "   macro avg       0.94      0.89      0.91       263\n",
      "weighted avg       0.94      0.89      0.91       263\n",
      " samples avg       0.21      0.20      0.20       263\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_label_ids,pred_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cIxXJ5f8pFU"
   },
   "outputs": [],
   "source": [
    "cm = multilabel_confusion_matrix(test_label_ids,pred_hard)\n",
    "labels = ['Err_1','err_2','err_3','err_4','err_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7QTev_JJgBh",
    "outputId": "ccea0e4e-c57e-4d45-da19-9fd3fc1ac3ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[641,   5],\n",
       "        [  8,  43]],\n",
       "\n",
       "       [[638,   5],\n",
       "        [  7,  47]],\n",
       "\n",
       "       [[638,   4],\n",
       "        [  6,  49]],\n",
       "\n",
       "       [[645,   0],\n",
       "        [  7,  45]],\n",
       "\n",
       "       [[645,   1],\n",
       "        [  1,  50]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aptCMhT68fU"
   },
   "source": [
    "###**3. Baseline - Official implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkUuXc7k7Aqf"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH8kUYf37Pkw"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPH9t5nP7aFf"
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0tPFuY77bSm"
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOP5QI2Y76g7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPJN8Spa7-S8"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuQWrg5Y8Diu"
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    #print(\"Loading data...\")\n",
    "    #x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "    x_train = df_train.abstracts.values\n",
    "    x_train = [clean_str(sent) for sent in x_train]\n",
    "    y_train = np.eye(2)[df_train.grammatically_incorrect.values]\n",
    "    x_dev = df_val.abstracts.values\n",
    "    x_dev = [clean_str(sent) for sent in x_dev]\n",
    "    y_dev = np.eye(2)[df_val.grammatically_incorrect.values]\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = 250\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    #sent_for_vocab = np.concatenate((x_train,x_dev))\n",
    "\n",
    "    x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "    x_dev = np.array(list(vocab_processor.fit_transform(x_dev)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    #np.random.seed(10)\n",
    "    #shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    #x_shuffled = x[shuffle_indices]\n",
    "    #y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    #dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    #x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    #y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    #del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    #print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    #print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFzVxbhBUojr"
   },
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "val_accs = []\n",
    "train_losses = []\n",
    "train_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WWkEIbQ7olR"
   },
   "outputs": [],
   "source": [
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=True,\n",
    "          log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=128,\n",
    "                filter_sizes=list(map(int, [3,4,5])),\n",
    "                num_filters=128,\n",
    "                l2_reg_lambda=0.0)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 0.5\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                train_losses.append(loss)\n",
    "                train_accs.append(accuracy)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                \n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                val_losses.append(loss)\n",
    "                val_accs.append(accuracy)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x_train, y_train)), 64, 50)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "              \n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 100 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % 100 == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5p4RsZhDHFf",
    "outputId": "a8625e82-59b4-4780-b77b-67fc1193b63e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-186e778fa5b9>:18: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yX4g0fbH8NKl",
    "outputId": "d9bbeb95-b346-4ef7-87a6-31871e3b3451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-7f855fd71ba3>:58: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-7f855fd71ba3>:74: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /content/runs/1648203506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "2022-03-25T10:31:24.816743: step 4989, loss 0.00177542, acc 1\n",
      "2022-03-25T10:31:24.966374: step 4990, loss 0.000221838, acc 1\n",
      "2022-03-25T10:31:25.127160: step 4991, loss 0.0012879, acc 1\n",
      "2022-03-25T10:31:25.272311: step 4992, loss 0.00334632, acc 1\n",
      "2022-03-25T10:31:25.425638: step 4993, loss 0.0156785, acc 1\n",
      "2022-03-25T10:31:25.571941: step 4994, loss 0.00308173, acc 1\n",
      "2022-03-25T10:31:25.719924: step 4995, loss 0.00150071, acc 1\n",
      "2022-03-25T10:31:25.872771: step 4996, loss 0.000475497, acc 1\n",
      "2022-03-25T10:31:26.025745: step 4997, loss 0.00113114, acc 1\n",
      "2022-03-25T10:31:26.175061: step 4998, loss 0.00108887, acc 1\n",
      "2022-03-25T10:31:26.321976: step 4999, loss 0.00112564, acc 1\n",
      "2022-03-25T10:31:26.470677: step 5000, loss 0.000645797, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:31:26.612773: step 5000, loss 0.494102, acc 0.921512\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5000\n",
      "\n",
      "2022-03-25T10:31:26.883154: step 5001, loss 0.0345586, acc 0.984375\n",
      "2022-03-25T10:31:27.039776: step 5002, loss 0.000185663, acc 1\n",
      "2022-03-25T10:31:27.181946: step 5003, loss 0.00113912, acc 1\n",
      "2022-03-25T10:31:27.332755: step 5004, loss 0.000533967, acc 1\n",
      "2022-03-25T10:31:27.477053: step 5005, loss 0.00221157, acc 1\n",
      "2022-03-25T10:31:27.624764: step 5006, loss 0.0756296, acc 0.984375\n",
      "2022-03-25T10:31:27.777761: step 5007, loss 0.0072217, acc 1\n",
      "2022-03-25T10:31:27.926529: step 5008, loss 0.000310418, acc 1\n",
      "2022-03-25T10:31:28.074750: step 5009, loss 0.00170766, acc 1\n",
      "2022-03-25T10:31:28.216816: step 5010, loss 0.00416978, acc 1\n",
      "2022-03-25T10:31:28.358308: step 5011, loss 0.012488, acc 0.984375\n",
      "2022-03-25T10:31:28.512337: step 5012, loss 0.00708036, acc 1\n",
      "2022-03-25T10:31:28.672657: step 5013, loss 0.0025081, acc 1\n",
      "2022-03-25T10:31:28.820181: step 5014, loss 0.000942352, acc 1\n",
      "2022-03-25T10:31:28.971474: step 5015, loss 0.00132575, acc 1\n",
      "2022-03-25T10:31:29.124337: step 5016, loss 0.000523856, acc 1\n",
      "2022-03-25T10:31:29.264494: step 5017, loss 0.00476411, acc 1\n",
      "2022-03-25T10:31:29.415552: step 5018, loss 0.000111855, acc 1\n",
      "2022-03-25T10:31:29.569105: step 5019, loss 0.00156895, acc 1\n",
      "2022-03-25T10:31:29.710878: step 5020, loss 0.00135116, acc 1\n",
      "2022-03-25T10:31:29.858764: step 5021, loss 0.0044078, acc 1\n",
      "2022-03-25T10:31:30.007228: step 5022, loss 0.000116808, acc 1\n",
      "2022-03-25T10:31:30.173858: step 5023, loss 0.00140954, acc 1\n",
      "2022-03-25T10:31:30.331968: step 5024, loss 0.00115543, acc 1\n",
      "2022-03-25T10:31:30.474844: step 5025, loss 0.00354092, acc 1\n",
      "2022-03-25T10:31:30.619413: step 5026, loss 0.0171894, acc 0.984375\n",
      "2022-03-25T10:31:30.774979: step 5027, loss 0.00292713, acc 1\n",
      "2022-03-25T10:31:30.928036: step 5028, loss 0.00627875, acc 1\n",
      "2022-03-25T10:31:31.080441: step 5029, loss 0.000458054, acc 1\n",
      "2022-03-25T10:31:31.228207: step 5030, loss 0.00078727, acc 1\n",
      "2022-03-25T10:31:31.378481: step 5031, loss 0.00110258, acc 1\n",
      "2022-03-25T10:31:31.518859: step 5032, loss 0.045322, acc 0.984375\n",
      "2022-03-25T10:31:31.670267: step 5033, loss 0.000779817, acc 1\n",
      "2022-03-25T10:31:31.815724: step 5034, loss 0.00307041, acc 1\n",
      "2022-03-25T10:31:31.960225: step 5035, loss 0.000446566, acc 1\n",
      "2022-03-25T10:31:32.109778: step 5036, loss 0.000633839, acc 1\n",
      "2022-03-25T10:31:32.251822: step 5037, loss 0.00563331, acc 1\n",
      "2022-03-25T10:31:32.405826: step 5038, loss 0.000550527, acc 1\n",
      "2022-03-25T10:31:32.558405: step 5039, loss 0.00834619, acc 1\n",
      "2022-03-25T10:31:32.703132: step 5040, loss 0.00232618, acc 1\n",
      "2022-03-25T10:31:32.846485: step 5041, loss 0.00403801, acc 1\n",
      "2022-03-25T10:31:32.997660: step 5042, loss 0.00791898, acc 1\n",
      "2022-03-25T10:31:33.146226: step 5043, loss 0.00227954, acc 1\n",
      "2022-03-25T10:31:33.265760: step 5044, loss 0.000549878, acc 1\n",
      "2022-03-25T10:31:33.408596: step 5045, loss 0.000925135, acc 1\n",
      "2022-03-25T10:31:33.562959: step 5046, loss 0.000432792, acc 1\n",
      "2022-03-25T10:31:33.704584: step 5047, loss 0.000870185, acc 1\n",
      "2022-03-25T10:31:33.856552: step 5048, loss 0.0175132, acc 1\n",
      "2022-03-25T10:31:33.997251: step 5049, loss 0.000176084, acc 1\n",
      "2022-03-25T10:31:34.155884: step 5050, loss 0.00068354, acc 1\n",
      "2022-03-25T10:31:34.296354: step 5051, loss 0.000154591, acc 1\n",
      "2022-03-25T10:31:34.448288: step 5052, loss 0.00174056, acc 1\n",
      "2022-03-25T10:31:34.595439: step 5053, loss 0.000727615, acc 1\n",
      "2022-03-25T10:31:34.743265: step 5054, loss 0.00637532, acc 1\n",
      "2022-03-25T10:31:34.889965: step 5055, loss 0.00225026, acc 1\n",
      "2022-03-25T10:31:35.038069: step 5056, loss 0.00113417, acc 1\n",
      "2022-03-25T10:31:35.180716: step 5057, loss 0.000325446, acc 1\n",
      "2022-03-25T10:31:35.329959: step 5058, loss 0.00349926, acc 1\n",
      "2022-03-25T10:31:35.467760: step 5059, loss 0.000175995, acc 1\n",
      "2022-03-25T10:31:35.608298: step 5060, loss 0.00256511, acc 1\n",
      "2022-03-25T10:31:35.755159: step 5061, loss 5.13707e-05, acc 1\n",
      "2022-03-25T10:31:35.909535: step 5062, loss 0.000625823, acc 1\n",
      "2022-03-25T10:31:36.048521: step 5063, loss 0.000394446, acc 1\n",
      "2022-03-25T10:31:36.196745: step 5064, loss 0.000458466, acc 1\n",
      "2022-03-25T10:31:36.345250: step 5065, loss 0.00128981, acc 1\n",
      "2022-03-25T10:31:36.491270: step 5066, loss 0.000856167, acc 1\n",
      "2022-03-25T10:31:36.640220: step 5067, loss 0.00971599, acc 1\n",
      "2022-03-25T10:31:36.787289: step 5068, loss 0.00107935, acc 1\n",
      "2022-03-25T10:31:36.924502: step 5069, loss 0.000188003, acc 1\n",
      "2022-03-25T10:31:37.074251: step 5070, loss 0.00885276, acc 1\n",
      "2022-03-25T10:31:37.218498: step 5071, loss 0.000339961, acc 1\n",
      "2022-03-25T10:31:37.370655: step 5072, loss 0.00102189, acc 1\n",
      "2022-03-25T10:31:37.523537: step 5073, loss 0.00363221, acc 1\n",
      "2022-03-25T10:31:37.673945: step 5074, loss 0.000115134, acc 1\n",
      "2022-03-25T10:31:37.824252: step 5075, loss 0.0199152, acc 0.984375\n",
      "2022-03-25T10:31:37.972022: step 5076, loss 0.00105829, acc 1\n",
      "2022-03-25T10:31:38.126437: step 5077, loss 0.00457637, acc 1\n",
      "2022-03-25T10:31:38.278994: step 5078, loss 0.0078882, acc 1\n",
      "2022-03-25T10:31:38.425655: step 5079, loss 0.00160066, acc 1\n",
      "2022-03-25T10:31:38.579413: step 5080, loss 0.00173902, acc 1\n",
      "2022-03-25T10:31:38.743210: step 5081, loss 0.000715927, acc 1\n",
      "2022-03-25T10:31:38.901857: step 5082, loss 0.00358692, acc 1\n",
      "2022-03-25T10:31:39.067010: step 5083, loss 0.00981512, acc 1\n",
      "2022-03-25T10:31:39.219691: step 5084, loss 0.0132896, acc 0.984375\n",
      "2022-03-25T10:31:39.381680: step 5085, loss 0.000301093, acc 1\n",
      "2022-03-25T10:31:39.535747: step 5086, loss 0.0316232, acc 0.984375\n",
      "2022-03-25T10:31:39.697428: step 5087, loss 0.00224155, acc 1\n",
      "2022-03-25T10:31:39.840669: step 5088, loss 0.000845227, acc 1\n",
      "2022-03-25T10:31:39.995554: step 5089, loss 0.000304649, acc 1\n",
      "2022-03-25T10:31:40.145298: step 5090, loss 0.001228, acc 1\n",
      "2022-03-25T10:31:40.301597: step 5091, loss 0.000185873, acc 1\n",
      "2022-03-25T10:31:40.460545: step 5092, loss 0.000386327, acc 1\n",
      "2022-03-25T10:31:40.623735: step 5093, loss 0.000889492, acc 1\n",
      "2022-03-25T10:31:40.776171: step 5094, loss 0.000777963, acc 1\n",
      "2022-03-25T10:31:40.936251: step 5095, loss 0.0026068, acc 1\n",
      "2022-03-25T10:31:41.085839: step 5096, loss 0.000330098, acc 1\n",
      "2022-03-25T10:31:41.237398: step 5097, loss 0.00133779, acc 1\n",
      "2022-03-25T10:31:41.397182: step 5098, loss 0.00215488, acc 1\n",
      "2022-03-25T10:31:41.550234: step 5099, loss 0.0302276, acc 0.984375\n",
      "2022-03-25T10:31:41.701357: step 5100, loss 0.00466448, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:31:41.858238: step 5100, loss 0.520967, acc 0.924419\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5100\n",
      "\n",
      "2022-03-25T10:31:42.121522: step 5101, loss 0.00391322, acc 1\n",
      "2022-03-25T10:31:42.259533: step 5102, loss 0.00169072, acc 1\n",
      "2022-03-25T10:31:42.413195: step 5103, loss 0.00380143, acc 1\n",
      "2022-03-25T10:31:42.565933: step 5104, loss 0.000507718, acc 1\n",
      "2022-03-25T10:31:42.705607: step 5105, loss 0.00217922, acc 1\n",
      "2022-03-25T10:31:42.858934: step 5106, loss 0.00152846, acc 1\n",
      "2022-03-25T10:31:43.007160: step 5107, loss 0.00115817, acc 1\n",
      "2022-03-25T10:31:43.163075: step 5108, loss 0.00858866, acc 1\n",
      "2022-03-25T10:31:43.310182: step 5109, loss 0.00230854, acc 1\n",
      "2022-03-25T10:31:43.468709: step 5110, loss 0.00250228, acc 1\n",
      "2022-03-25T10:31:43.619669: step 5111, loss 0.000797836, acc 1\n",
      "2022-03-25T10:31:43.783725: step 5112, loss 0.00087671, acc 1\n",
      "2022-03-25T10:31:43.924913: step 5113, loss 0.000873752, acc 1\n",
      "2022-03-25T10:31:44.078298: step 5114, loss 0.00100897, acc 1\n",
      "2022-03-25T10:31:44.222718: step 5115, loss 0.00493521, acc 1\n",
      "2022-03-25T10:31:44.370351: step 5116, loss 0.0184847, acc 0.984375\n",
      "2022-03-25T10:31:44.511625: step 5117, loss 0.000228477, acc 1\n",
      "2022-03-25T10:31:44.667373: step 5118, loss 0.00215801, acc 1\n",
      "2022-03-25T10:31:44.813234: step 5119, loss 0.000999709, acc 1\n",
      "2022-03-25T10:31:44.964452: step 5120, loss 0.00657302, acc 1\n",
      "2022-03-25T10:31:45.122269: step 5121, loss 0.00139215, acc 1\n",
      "2022-03-25T10:31:45.262436: step 5122, loss 0.0014775, acc 1\n",
      "2022-03-25T10:31:45.409741: step 5123, loss 0.00327012, acc 1\n",
      "2022-03-25T10:31:45.558704: step 5124, loss 0.000147248, acc 1\n",
      "2022-03-25T10:31:45.707746: step 5125, loss 0.00432133, acc 1\n",
      "2022-03-25T10:31:45.854692: step 5126, loss 0.00165252, acc 1\n",
      "2022-03-25T10:31:46.004766: step 5127, loss 0.000200026, acc 1\n",
      "2022-03-25T10:31:46.151279: step 5128, loss 0.000408451, acc 1\n",
      "2022-03-25T10:31:46.298228: step 5129, loss 0.000563056, acc 1\n",
      "2022-03-25T10:31:46.447579: step 5130, loss 0.00120704, acc 1\n",
      "2022-03-25T10:31:46.594410: step 5131, loss 0.000558536, acc 1\n",
      "2022-03-25T10:31:46.745852: step 5132, loss 0.000185809, acc 1\n",
      "2022-03-25T10:31:46.894351: step 5133, loss 0.000241768, acc 1\n",
      "2022-03-25T10:31:47.037218: step 5134, loss 0.00341625, acc 1\n",
      "2022-03-25T10:31:47.185625: step 5135, loss 0.000287853, acc 1\n",
      "2022-03-25T10:31:47.330954: step 5136, loss 0.00893361, acc 1\n",
      "2022-03-25T10:31:47.475670: step 5137, loss 0.000703741, acc 1\n",
      "2022-03-25T10:31:47.625392: step 5138, loss 0.000121043, acc 1\n",
      "2022-03-25T10:31:47.777750: step 5139, loss 0.00110671, acc 1\n",
      "2022-03-25T10:31:47.924688: step 5140, loss 0.00177801, acc 1\n",
      "2022-03-25T10:31:48.062598: step 5141, loss 0.00302508, acc 1\n",
      "2022-03-25T10:31:48.213722: step 5142, loss 0.00171013, acc 1\n",
      "2022-03-25T10:31:48.358549: step 5143, loss 0.000423654, acc 1\n",
      "2022-03-25T10:31:48.507396: step 5144, loss 0.00223535, acc 1\n",
      "2022-03-25T10:31:48.660021: step 5145, loss 0.000507907, acc 1\n",
      "2022-03-25T10:31:48.807205: step 5146, loss 0.00450263, acc 1\n",
      "2022-03-25T10:31:48.952523: step 5147, loss 0.00205066, acc 1\n",
      "2022-03-25T10:31:49.108489: step 5148, loss 0.00188522, acc 1\n",
      "2022-03-25T10:31:49.259577: step 5149, loss 0.0145358, acc 1\n",
      "2022-03-25T10:31:49.409293: step 5150, loss 0.0012859, acc 1\n",
      "2022-03-25T10:31:49.573439: step 5151, loss 0.00058435, acc 1\n",
      "2022-03-25T10:31:49.719349: step 5152, loss 0.0198719, acc 0.984375\n",
      "2022-03-25T10:31:49.870822: step 5153, loss 0.000391895, acc 1\n",
      "2022-03-25T10:31:50.021415: step 5154, loss 0.000150195, acc 1\n",
      "2022-03-25T10:31:50.163822: step 5155, loss 0.000413144, acc 1\n",
      "2022-03-25T10:31:50.322538: step 5156, loss 0.0207121, acc 0.984375\n",
      "2022-03-25T10:31:50.469672: step 5157, loss 0.00109153, acc 1\n",
      "2022-03-25T10:31:50.625587: step 5158, loss 0.00051957, acc 1\n",
      "2022-03-25T10:31:50.788082: step 5159, loss 0.000889912, acc 1\n",
      "2022-03-25T10:31:50.928875: step 5160, loss 0.000261899, acc 1\n",
      "2022-03-25T10:31:51.079021: step 5161, loss 0.000342401, acc 1\n",
      "2022-03-25T10:31:51.228924: step 5162, loss 0.00174504, acc 1\n",
      "2022-03-25T10:31:51.380088: step 5163, loss 0.000528653, acc 1\n",
      "2022-03-25T10:31:51.522183: step 5164, loss 0.00170618, acc 1\n",
      "2022-03-25T10:31:51.672065: step 5165, loss 0.000609327, acc 1\n",
      "2022-03-25T10:31:51.815320: step 5166, loss 0.000914408, acc 1\n",
      "2022-03-25T10:31:51.960316: step 5167, loss 0.00307038, acc 1\n",
      "2022-03-25T10:31:52.102250: step 5168, loss 0.00059774, acc 1\n",
      "2022-03-25T10:31:52.240097: step 5169, loss 0.00432117, acc 1\n",
      "2022-03-25T10:31:52.394236: step 5170, loss 0.000360712, acc 1\n",
      "2022-03-25T10:31:52.536767: step 5171, loss 0.00302204, acc 1\n",
      "2022-03-25T10:31:52.693469: step 5172, loss 0.00193735, acc 1\n",
      "2022-03-25T10:31:52.850583: step 5173, loss 0.00111554, acc 1\n",
      "2022-03-25T10:31:52.998676: step 5174, loss 0.000597859, acc 1\n",
      "2022-03-25T10:31:53.145347: step 5175, loss 0.00444628, acc 1\n",
      "2022-03-25T10:31:53.291397: step 5176, loss 0.00299667, acc 1\n",
      "2022-03-25T10:31:53.431969: step 5177, loss 0.00714008, acc 1\n",
      "2022-03-25T10:31:53.588326: step 5178, loss 0.000358876, acc 1\n",
      "2022-03-25T10:31:53.735705: step 5179, loss 0.00398644, acc 1\n",
      "2022-03-25T10:31:53.887835: step 5180, loss 0.0187416, acc 0.984375\n",
      "2022-03-25T10:31:54.038028: step 5181, loss 0.00181248, acc 1\n",
      "2022-03-25T10:31:54.189546: step 5182, loss 0.00486366, acc 1\n",
      "2022-03-25T10:31:54.339123: step 5183, loss 0.000377787, acc 1\n",
      "2022-03-25T10:31:54.483416: step 5184, loss 0.00323671, acc 1\n",
      "2022-03-25T10:31:54.634563: step 5185, loss 0.00251088, acc 1\n",
      "2022-03-25T10:31:54.777354: step 5186, loss 0.000660898, acc 1\n",
      "2022-03-25T10:31:54.923362: step 5187, loss 0.00521138, acc 1\n",
      "2022-03-25T10:31:55.069256: step 5188, loss 0.000270357, acc 1\n",
      "2022-03-25T10:31:55.212341: step 5189, loss 0.00223694, acc 1\n",
      "2022-03-25T10:31:55.361675: step 5190, loss 0.0153674, acc 0.984375\n",
      "2022-03-25T10:31:55.514208: step 5191, loss 0.000781528, acc 1\n",
      "2022-03-25T10:31:55.663715: step 5192, loss 0.000650034, acc 1\n",
      "2022-03-25T10:31:55.808792: step 5193, loss 0.013444, acc 0.984375\n",
      "2022-03-25T10:31:55.956457: step 5194, loss 0.000527494, acc 1\n",
      "2022-03-25T10:31:56.109813: step 5195, loss 0.00144474, acc 1\n",
      "2022-03-25T10:31:56.254260: step 5196, loss 0.000351956, acc 1\n",
      "2022-03-25T10:31:56.401186: step 5197, loss 0.00369191, acc 1\n",
      "2022-03-25T10:31:56.552960: step 5198, loss 0.000564124, acc 1\n",
      "2022-03-25T10:31:56.710352: step 5199, loss 0.011897, acc 0.984375\n",
      "2022-03-25T10:31:56.849983: step 5200, loss 0.0001657, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:31:56.986313: step 5200, loss 0.539325, acc 0.922965\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5200\n",
      "\n",
      "2022-03-25T10:31:57.233327: step 5201, loss 0.000359983, acc 1\n",
      "2022-03-25T10:31:57.379407: step 5202, loss 0.000225662, acc 1\n",
      "2022-03-25T10:31:57.526017: step 5203, loss 0.0133568, acc 0.984375\n",
      "2022-03-25T10:31:57.674282: step 5204, loss 0.0182153, acc 0.984375\n",
      "2022-03-25T10:31:57.831967: step 5205, loss 0.000277563, acc 1\n",
      "2022-03-25T10:31:57.977688: step 5206, loss 0.000733511, acc 1\n",
      "2022-03-25T10:31:58.115527: step 5207, loss 0.00108903, acc 1\n",
      "2022-03-25T10:31:58.268199: step 5208, loss 0.000506547, acc 1\n",
      "2022-03-25T10:31:58.412668: step 5209, loss 0.00202717, acc 1\n",
      "2022-03-25T10:31:58.560470: step 5210, loss 0.0110774, acc 1\n",
      "2022-03-25T10:31:58.713756: step 5211, loss 0.000936981, acc 1\n",
      "2022-03-25T10:31:58.869809: step 5212, loss 0.000569902, acc 1\n",
      "2022-03-25T10:31:59.015433: step 5213, loss 0.00256841, acc 1\n",
      "2022-03-25T10:31:59.175891: step 5214, loss 0.00051671, acc 1\n",
      "2022-03-25T10:31:59.317462: step 5215, loss 0.000607565, acc 1\n",
      "2022-03-25T10:31:59.472750: step 5216, loss 0.00167493, acc 1\n",
      "2022-03-25T10:31:59.616976: step 5217, loss 0.0334634, acc 0.984375\n",
      "2022-03-25T10:31:59.768466: step 5218, loss 0.00124685, acc 1\n",
      "2022-03-25T10:31:59.911771: step 5219, loss 0.000583908, acc 1\n",
      "2022-03-25T10:32:00.061074: step 5220, loss 0.00287323, acc 1\n",
      "2022-03-25T10:32:00.208465: step 5221, loss 0.00020302, acc 1\n",
      "2022-03-25T10:32:00.359008: step 5222, loss 0.00129003, acc 1\n",
      "2022-03-25T10:32:00.505692: step 5223, loss 0.00170363, acc 1\n",
      "2022-03-25T10:32:00.648964: step 5224, loss 0.00400287, acc 1\n",
      "2022-03-25T10:32:00.794681: step 5225, loss 0.0019452, acc 1\n",
      "2022-03-25T10:32:00.943391: step 5226, loss 0.000218702, acc 1\n",
      "2022-03-25T10:32:01.086666: step 5227, loss 0.000823007, acc 1\n",
      "2022-03-25T10:32:01.236389: step 5228, loss 0.000300538, acc 1\n",
      "2022-03-25T10:32:01.372673: step 5229, loss 0.00157363, acc 1\n",
      "2022-03-25T10:32:01.518767: step 5230, loss 0.00156383, acc 1\n",
      "2022-03-25T10:32:01.663106: step 5231, loss 0.00165913, acc 1\n",
      "2022-03-25T10:32:01.818823: step 5232, loss 0.00108445, acc 1\n",
      "2022-03-25T10:32:01.968081: step 5233, loss 0.001142, acc 1\n",
      "2022-03-25T10:32:02.110995: step 5234, loss 0.000187573, acc 1\n",
      "2022-03-25T10:32:02.258415: step 5235, loss 0.000690178, acc 1\n",
      "2022-03-25T10:32:02.406549: step 5236, loss 0.000864741, acc 1\n",
      "2022-03-25T10:32:02.561787: step 5237, loss 0.00124502, acc 1\n",
      "2022-03-25T10:32:02.675322: step 5238, loss 0.002941, acc 1\n",
      "2022-03-25T10:32:02.826559: step 5239, loss 0.000558775, acc 1\n",
      "2022-03-25T10:32:02.977178: step 5240, loss 9.78686e-05, acc 1\n",
      "2022-03-25T10:32:03.116933: step 5241, loss 0.00277676, acc 1\n",
      "2022-03-25T10:32:03.262608: step 5242, loss 0.000691756, acc 1\n",
      "2022-03-25T10:32:03.403902: step 5243, loss 0.00764221, acc 1\n",
      "2022-03-25T10:32:03.548358: step 5244, loss 0.000104994, acc 1\n",
      "2022-03-25T10:32:03.687120: step 5245, loss 0.00010933, acc 1\n",
      "2022-03-25T10:32:03.833509: step 5246, loss 0.000394618, acc 1\n",
      "2022-03-25T10:32:03.984188: step 5247, loss 0.000813474, acc 1\n",
      "2022-03-25T10:32:04.127101: step 5248, loss 0.00187595, acc 1\n",
      "2022-03-25T10:32:04.273912: step 5249, loss 0.000881592, acc 1\n",
      "2022-03-25T10:32:04.412913: step 5250, loss 0.000648115, acc 1\n",
      "2022-03-25T10:32:04.552207: step 5251, loss 0.000210502, acc 1\n",
      "2022-03-25T10:32:04.709078: step 5252, loss 0.000670542, acc 1\n",
      "2022-03-25T10:32:04.858700: step 5253, loss 0.000192169, acc 1\n",
      "2022-03-25T10:32:05.019379: step 5254, loss 0.00108061, acc 1\n",
      "2022-03-25T10:32:05.180514: step 5255, loss 0.00545503, acc 1\n",
      "2022-03-25T10:32:05.326465: step 5256, loss 0.00076909, acc 1\n",
      "2022-03-25T10:32:05.473261: step 5257, loss 0.00234225, acc 1\n",
      "2022-03-25T10:32:05.622997: step 5258, loss 0.000250739, acc 1\n",
      "2022-03-25T10:32:05.768608: step 5259, loss 0.000437976, acc 1\n",
      "2022-03-25T10:32:05.911683: step 5260, loss 0.000546506, acc 1\n",
      "2022-03-25T10:32:06.051460: step 5261, loss 0.000934524, acc 1\n",
      "2022-03-25T10:32:06.201321: step 5262, loss 0.000237719, acc 1\n",
      "2022-03-25T10:32:06.347440: step 5263, loss 0.00955481, acc 1\n",
      "2022-03-25T10:32:06.496296: step 5264, loss 0.00202931, acc 1\n",
      "2022-03-25T10:32:06.636919: step 5265, loss 0.00448861, acc 1\n",
      "2022-03-25T10:32:06.788851: step 5266, loss 0.000177063, acc 1\n",
      "2022-03-25T10:32:06.944760: step 5267, loss 0.00573335, acc 1\n",
      "2022-03-25T10:32:07.084008: step 5268, loss 0.00438544, acc 1\n",
      "2022-03-25T10:32:07.225102: step 5269, loss 0.000376702, acc 1\n",
      "2022-03-25T10:32:07.379163: step 5270, loss 0.00222393, acc 1\n",
      "2022-03-25T10:32:07.539644: step 5271, loss 0.000489912, acc 1\n",
      "2022-03-25T10:32:07.682987: step 5272, loss 0.00707186, acc 1\n",
      "2022-03-25T10:32:07.825609: step 5273, loss 0.00133738, acc 1\n",
      "2022-03-25T10:32:07.969355: step 5274, loss 0.000314096, acc 1\n",
      "2022-03-25T10:32:08.111511: step 5275, loss 0.000216203, acc 1\n",
      "2022-03-25T10:32:08.268804: step 5276, loss 0.00473299, acc 1\n",
      "2022-03-25T10:32:08.425493: step 5277, loss 0.000673462, acc 1\n",
      "2022-03-25T10:32:08.572848: step 5278, loss 0.000706022, acc 1\n",
      "2022-03-25T10:32:08.722240: step 5279, loss 0.00081654, acc 1\n",
      "2022-03-25T10:32:08.868234: step 5280, loss 0.000229993, acc 1\n",
      "2022-03-25T10:32:09.009577: step 5281, loss 0.0163112, acc 0.984375\n",
      "2022-03-25T10:32:09.159923: step 5282, loss 0.000414901, acc 1\n",
      "2022-03-25T10:32:09.296923: step 5283, loss 0.00251976, acc 1\n",
      "2022-03-25T10:32:09.451265: step 5284, loss 8.59488e-05, acc 1\n",
      "2022-03-25T10:32:09.607822: step 5285, loss 0.00184334, acc 1\n",
      "2022-03-25T10:32:09.754040: step 5286, loss 0.000531443, acc 1\n",
      "2022-03-25T10:32:09.905232: step 5287, loss 0.000209097, acc 1\n",
      "2022-03-25T10:32:10.057984: step 5288, loss 0.000965955, acc 1\n",
      "2022-03-25T10:32:10.201798: step 5289, loss 0.00401952, acc 1\n",
      "2022-03-25T10:32:10.355969: step 5290, loss 0.0013532, acc 1\n",
      "2022-03-25T10:32:10.501235: step 5291, loss 0.000919363, acc 1\n",
      "2022-03-25T10:32:10.659938: step 5292, loss 0.00129537, acc 1\n",
      "2022-03-25T10:32:10.809978: step 5293, loss 0.000296188, acc 1\n",
      "2022-03-25T10:32:10.956588: step 5294, loss 0.000197548, acc 1\n",
      "2022-03-25T10:32:11.110210: step 5295, loss 0.000682957, acc 1\n",
      "2022-03-25T10:32:11.253106: step 5296, loss 0.000100297, acc 1\n",
      "2022-03-25T10:32:11.405233: step 5297, loss 0.000190323, acc 1\n",
      "2022-03-25T10:32:11.556444: step 5298, loss 0.000154864, acc 1\n",
      "2022-03-25T10:32:11.709413: step 5299, loss 0.00285588, acc 1\n",
      "2022-03-25T10:32:11.850479: step 5300, loss 0.00766372, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:32:11.982328: step 5300, loss 0.518059, acc 0.924419\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5300\n",
      "\n",
      "2022-03-25T10:32:12.240205: step 5301, loss 0.00124834, acc 1\n",
      "2022-03-25T10:32:12.395430: step 5302, loss 0.00119666, acc 1\n",
      "2022-03-25T10:32:12.548974: step 5303, loss 0.00142558, acc 1\n",
      "2022-03-25T10:32:12.693957: step 5304, loss 0.00937416, acc 1\n",
      "2022-03-25T10:32:12.850693: step 5305, loss 0.00027827, acc 1\n",
      "2022-03-25T10:32:12.990756: step 5306, loss 0.000251168, acc 1\n",
      "2022-03-25T10:32:13.150907: step 5307, loss 0.000988957, acc 1\n",
      "2022-03-25T10:32:13.288268: step 5308, loss 0.00811896, acc 1\n",
      "2022-03-25T10:32:13.435298: step 5309, loss 0.000821862, acc 1\n",
      "2022-03-25T10:32:13.579924: step 5310, loss 0.000205047, acc 1\n",
      "2022-03-25T10:32:13.733239: step 5311, loss 0.00264445, acc 1\n",
      "2022-03-25T10:32:13.876581: step 5312, loss 0.000679718, acc 1\n",
      "2022-03-25T10:32:14.019995: step 5313, loss 0.00142503, acc 1\n",
      "2022-03-25T10:32:14.192633: step 5314, loss 0.00122416, acc 1\n",
      "2022-03-25T10:32:14.327503: step 5315, loss 0.00027559, acc 1\n",
      "2022-03-25T10:32:14.476073: step 5316, loss 0.000142556, acc 1\n",
      "2022-03-25T10:32:14.624009: step 5317, loss 0.000834415, acc 1\n",
      "2022-03-25T10:32:14.773383: step 5318, loss 0.00232023, acc 1\n",
      "2022-03-25T10:32:14.926468: step 5319, loss 0.0108047, acc 1\n",
      "2022-03-25T10:32:15.069760: step 5320, loss 0.000491068, acc 1\n",
      "2022-03-25T10:32:15.218797: step 5321, loss 2.51009e-05, acc 1\n",
      "2022-03-25T10:32:15.365805: step 5322, loss 0.00834732, acc 1\n",
      "2022-03-25T10:32:15.505399: step 5323, loss 0.000445948, acc 1\n",
      "2022-03-25T10:32:15.657242: step 5324, loss 0.000311858, acc 1\n",
      "2022-03-25T10:32:15.798873: step 5325, loss 0.000642599, acc 1\n",
      "2022-03-25T10:32:15.949018: step 5326, loss 9.15641e-05, acc 1\n",
      "2022-03-25T10:32:16.097573: step 5327, loss 0.0091489, acc 1\n",
      "2022-03-25T10:32:16.258838: step 5328, loss 0.000525927, acc 1\n",
      "2022-03-25T10:32:16.406536: step 5329, loss 0.000753377, acc 1\n",
      "2022-03-25T10:32:16.549529: step 5330, loss 0.000903953, acc 1\n",
      "2022-03-25T10:32:16.693224: step 5331, loss 0.00229592, acc 1\n",
      "2022-03-25T10:32:16.842973: step 5332, loss 0.000409379, acc 1\n",
      "2022-03-25T10:32:16.996780: step 5333, loss 0.000875199, acc 1\n",
      "2022-03-25T10:32:17.139575: step 5334, loss 0.0024741, acc 1\n",
      "2022-03-25T10:32:17.287308: step 5335, loss 0.00289636, acc 1\n",
      "2022-03-25T10:32:17.438239: step 5336, loss 0.000294691, acc 1\n",
      "2022-03-25T10:32:17.581373: step 5337, loss 0.0598965, acc 0.984375\n",
      "2022-03-25T10:32:17.727178: step 5338, loss 0.00404026, acc 1\n",
      "2022-03-25T10:32:17.880124: step 5339, loss 0.00159525, acc 1\n",
      "2022-03-25T10:32:18.022768: step 5340, loss 0.00103153, acc 1\n",
      "2022-03-25T10:32:18.168161: step 5341, loss 0.000758471, acc 1\n",
      "2022-03-25T10:32:18.337574: step 5342, loss 0.0124831, acc 1\n",
      "2022-03-25T10:32:18.478244: step 5343, loss 0.000520952, acc 1\n",
      "2022-03-25T10:32:18.626953: step 5344, loss 0.00526851, acc 1\n",
      "2022-03-25T10:32:18.768834: step 5345, loss 0.00673361, acc 1\n",
      "2022-03-25T10:32:18.923913: step 5346, loss 0.00228069, acc 1\n",
      "2022-03-25T10:32:19.065736: step 5347, loss 0.00130543, acc 1\n",
      "2022-03-25T10:32:19.212595: step 5348, loss 0.00147775, acc 1\n",
      "2022-03-25T10:32:19.366128: step 5349, loss 0.000625459, acc 1\n",
      "2022-03-25T10:32:19.508617: step 5350, loss 0.00128577, acc 1\n",
      "2022-03-25T10:32:19.659928: step 5351, loss 0.000610885, acc 1\n",
      "2022-03-25T10:32:19.806792: step 5352, loss 0.000999927, acc 1\n",
      "2022-03-25T10:32:19.955493: step 5353, loss 0.00184048, acc 1\n",
      "2022-03-25T10:32:20.097887: step 5354, loss 0.000522379, acc 1\n",
      "2022-03-25T10:32:20.239487: step 5355, loss 0.00439174, acc 1\n",
      "2022-03-25T10:32:20.397676: step 5356, loss 0.000168069, acc 1\n",
      "2022-03-25T10:32:20.543532: step 5357, loss 0.00277044, acc 1\n",
      "2022-03-25T10:32:20.690388: step 5358, loss 0.00192174, acc 1\n",
      "2022-03-25T10:32:20.838381: step 5359, loss 0.000228767, acc 1\n",
      "2022-03-25T10:32:20.983995: step 5360, loss 0.00355589, acc 1\n",
      "2022-03-25T10:32:21.131366: step 5361, loss 0.000180135, acc 1\n",
      "2022-03-25T10:32:21.277395: step 5362, loss 0.000525929, acc 1\n",
      "2022-03-25T10:32:21.445144: step 5363, loss 0.00135701, acc 1\n",
      "2022-03-25T10:32:21.589039: step 5364, loss 0.00705828, acc 1\n",
      "2022-03-25T10:32:21.733704: step 5365, loss 0.000718093, acc 1\n",
      "2022-03-25T10:32:21.881904: step 5366, loss 0.0174955, acc 0.984375\n",
      "2022-03-25T10:32:22.024379: step 5367, loss 0.000396272, acc 1\n",
      "2022-03-25T10:32:22.175049: step 5368, loss 8.16688e-05, acc 1\n",
      "2022-03-25T10:32:22.311436: step 5369, loss 0.000285738, acc 1\n",
      "2022-03-25T10:32:22.477939: step 5370, loss 0.00795859, acc 1\n",
      "2022-03-25T10:32:22.616167: step 5371, loss 0.00328717, acc 1\n",
      "2022-03-25T10:32:22.765232: step 5372, loss 0.000298074, acc 1\n",
      "2022-03-25T10:32:22.910839: step 5373, loss 0.00275653, acc 1\n",
      "2022-03-25T10:32:23.058754: step 5374, loss 0.000392418, acc 1\n",
      "2022-03-25T10:32:23.207716: step 5375, loss 0.00165486, acc 1\n",
      "2022-03-25T10:32:23.351767: step 5376, loss 0.000299547, acc 1\n",
      "2022-03-25T10:32:23.518071: step 5377, loss 0.00146513, acc 1\n",
      "2022-03-25T10:32:23.660397: step 5378, loss 0.0017534, acc 1\n",
      "2022-03-25T10:32:23.805778: step 5379, loss 6.70281e-05, acc 1\n",
      "2022-03-25T10:32:23.954859: step 5380, loss 0.00216598, acc 1\n",
      "2022-03-25T10:32:24.109007: step 5381, loss 0.000486891, acc 1\n",
      "2022-03-25T10:32:24.259440: step 5382, loss 6.82216e-05, acc 1\n",
      "2022-03-25T10:32:24.421016: step 5383, loss 0.00109786, acc 1\n",
      "2022-03-25T10:32:24.574795: step 5384, loss 0.000186328, acc 1\n",
      "2022-03-25T10:32:24.729938: step 5385, loss 0.00117808, acc 1\n",
      "2022-03-25T10:32:24.887695: step 5386, loss 0.000647086, acc 1\n",
      "2022-03-25T10:32:25.048285: step 5387, loss 0.00565063, acc 1\n",
      "2022-03-25T10:32:25.194313: step 5388, loss 0.00310813, acc 1\n",
      "2022-03-25T10:32:25.341143: step 5389, loss 0.00149456, acc 1\n",
      "2022-03-25T10:32:25.489632: step 5390, loss 0.00012611, acc 1\n",
      "2022-03-25T10:32:25.647928: step 5391, loss 0.00264861, acc 1\n",
      "2022-03-25T10:32:25.800732: step 5392, loss 0.00134246, acc 1\n",
      "2022-03-25T10:32:25.957032: step 5393, loss 0.000531809, acc 1\n",
      "2022-03-25T10:32:26.110609: step 5394, loss 0.000459588, acc 1\n",
      "2022-03-25T10:32:26.267544: step 5395, loss 0.00180901, acc 1\n",
      "2022-03-25T10:32:26.424559: step 5396, loss 0.0026076, acc 1\n",
      "2022-03-25T10:32:26.593735: step 5397, loss 0.000847311, acc 1\n",
      "2022-03-25T10:32:26.749405: step 5398, loss 0.000529224, acc 1\n",
      "2022-03-25T10:32:26.902573: step 5399, loss 0.000789707, acc 1\n",
      "2022-03-25T10:32:27.060685: step 5400, loss 0.00180337, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:32:27.209409: step 5400, loss 0.536571, acc 0.924419\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5400\n",
      "\n",
      "2022-03-25T10:32:27.476587: step 5401, loss 0.00937473, acc 1\n",
      "2022-03-25T10:32:27.644632: step 5402, loss 0.0350393, acc 0.96875\n",
      "2022-03-25T10:32:27.806786: step 5403, loss 0.000225141, acc 1\n",
      "2022-03-25T10:32:27.954112: step 5404, loss 0.00202947, acc 1\n",
      "2022-03-25T10:32:28.114327: step 5405, loss 0.00119648, acc 1\n",
      "2022-03-25T10:32:28.284796: step 5406, loss 0.00950092, acc 1\n",
      "2022-03-25T10:32:28.436383: step 5407, loss 0.00115534, acc 1\n",
      "2022-03-25T10:32:28.592770: step 5408, loss 0.000190492, acc 1\n",
      "2022-03-25T10:32:28.744602: step 5409, loss 0.00132544, acc 1\n",
      "2022-03-25T10:32:28.892107: step 5410, loss 0.00286948, acc 1\n",
      "2022-03-25T10:32:29.047590: step 5411, loss 0.000288714, acc 1\n",
      "2022-03-25T10:32:29.200324: step 5412, loss 0.0165873, acc 0.984375\n",
      "2022-03-25T10:32:29.358190: step 5413, loss 0.00279083, acc 1\n",
      "2022-03-25T10:32:29.512869: step 5414, loss 0.00297156, acc 1\n",
      "2022-03-25T10:32:29.678315: step 5415, loss 0.00778299, acc 1\n",
      "2022-03-25T10:32:29.838635: step 5416, loss 0.00111158, acc 1\n",
      "2022-03-25T10:32:29.991529: step 5417, loss 0.0011457, acc 1\n",
      "2022-03-25T10:32:30.150141: step 5418, loss 0.00537432, acc 1\n",
      "2022-03-25T10:32:30.305425: step 5419, loss 0.000872081, acc 1\n",
      "2022-03-25T10:32:30.455706: step 5420, loss 0.00040685, acc 1\n",
      "2022-03-25T10:32:30.606081: step 5421, loss 0.00464412, acc 1\n",
      "2022-03-25T10:32:30.759383: step 5422, loss 0.00545962, acc 1\n",
      "2022-03-25T10:32:30.918426: step 5423, loss 0.00169791, acc 1\n",
      "2022-03-25T10:32:31.070190: step 5424, loss 0.00116787, acc 1\n",
      "2022-03-25T10:32:31.224724: step 5425, loss 0.00038237, acc 1\n",
      "2022-03-25T10:32:31.372822: step 5426, loss 0.000380537, acc 1\n",
      "2022-03-25T10:32:31.522561: step 5427, loss 0.00084237, acc 1\n",
      "2022-03-25T10:32:31.689706: step 5428, loss 0.000776986, acc 1\n",
      "2022-03-25T10:32:31.842336: step 5429, loss 0.000824782, acc 1\n",
      "2022-03-25T10:32:31.984974: step 5430, loss 0.00431714, acc 1\n",
      "2022-03-25T10:32:32.135980: step 5431, loss 0.0138631, acc 0.984375\n",
      "2022-03-25T10:32:32.247752: step 5432, loss 0.00187972, acc 1\n",
      "2022-03-25T10:32:32.401936: step 5433, loss 0.000474896, acc 1\n",
      "2022-03-25T10:32:32.558936: step 5434, loss 0.000152964, acc 1\n",
      "2022-03-25T10:32:32.719245: step 5435, loss 0.00239883, acc 1\n",
      "2022-03-25T10:32:32.873788: step 5436, loss 0.000124881, acc 1\n",
      "2022-03-25T10:32:33.028032: step 5437, loss 0.00226068, acc 1\n",
      "2022-03-25T10:32:33.189310: step 5438, loss 0.000473476, acc 1\n",
      "2022-03-25T10:32:33.349636: step 5439, loss 0.00486027, acc 1\n",
      "2022-03-25T10:32:33.501728: step 5440, loss 3.01351e-05, acc 1\n",
      "2022-03-25T10:32:33.659716: step 5441, loss 0.000181281, acc 1\n",
      "2022-03-25T10:32:33.813234: step 5442, loss 0.000315236, acc 1\n",
      "2022-03-25T10:32:33.966899: step 5443, loss 0.000299248, acc 1\n",
      "2022-03-25T10:32:34.120491: step 5444, loss 0.00259625, acc 1\n",
      "2022-03-25T10:32:34.277814: step 5445, loss 0.00109972, acc 1\n",
      "2022-03-25T10:32:34.420179: step 5446, loss 7.0484e-05, acc 1\n",
      "2022-03-25T10:32:34.579759: step 5447, loss 0.0123831, acc 0.984375\n",
      "2022-03-25T10:32:34.739573: step 5448, loss 0.000668312, acc 1\n",
      "2022-03-25T10:32:34.894720: step 5449, loss 0.00158717, acc 1\n",
      "2022-03-25T10:32:35.044985: step 5450, loss 0.00291923, acc 1\n",
      "2022-03-25T10:32:35.195589: step 5451, loss 0.000209699, acc 1\n",
      "2022-03-25T10:32:35.340659: step 5452, loss 0.000578099, acc 1\n",
      "2022-03-25T10:32:35.491827: step 5453, loss 0.00100577, acc 1\n",
      "2022-03-25T10:32:35.638468: step 5454, loss 0.00553443, acc 1\n",
      "2022-03-25T10:32:35.806628: step 5455, loss 0.000501793, acc 1\n",
      "2022-03-25T10:32:35.962360: step 5456, loss 0.016793, acc 0.984375\n",
      "2022-03-25T10:32:36.116296: step 5457, loss 0.000270525, acc 1\n",
      "2022-03-25T10:32:36.265439: step 5458, loss 0.00146589, acc 1\n",
      "2022-03-25T10:32:36.425399: step 5459, loss 0.000312088, acc 1\n",
      "2022-03-25T10:32:36.576037: step 5460, loss 0.000272919, acc 1\n",
      "2022-03-25T10:32:36.728333: step 5461, loss 8.73307e-05, acc 1\n",
      "2022-03-25T10:32:36.876366: step 5462, loss 0.000838468, acc 1\n",
      "2022-03-25T10:32:37.027482: step 5463, loss 0.00108955, acc 1\n",
      "2022-03-25T10:32:37.177681: step 5464, loss 0.00156901, acc 1\n",
      "2022-03-25T10:32:37.328411: step 5465, loss 0.000268788, acc 1\n",
      "2022-03-25T10:32:37.486614: step 5466, loss 0.000342036, acc 1\n",
      "2022-03-25T10:32:37.641965: step 5467, loss 0.000235219, acc 1\n",
      "2022-03-25T10:32:37.797773: step 5468, loss 0.0012068, acc 1\n",
      "2022-03-25T10:32:37.946864: step 5469, loss 0.000203019, acc 1\n",
      "2022-03-25T10:32:38.093544: step 5470, loss 0.000616309, acc 1\n",
      "2022-03-25T10:32:38.241556: step 5471, loss 0.000874682, acc 1\n",
      "2022-03-25T10:32:38.389661: step 5472, loss 0.00414902, acc 1\n",
      "2022-03-25T10:32:38.545337: step 5473, loss 0.00258809, acc 1\n",
      "2022-03-25T10:32:38.704649: step 5474, loss 0.000178987, acc 1\n",
      "2022-03-25T10:32:38.866433: step 5475, loss 5.19488e-05, acc 1\n",
      "2022-03-25T10:32:39.025381: step 5476, loss 0.00113726, acc 1\n",
      "2022-03-25T10:32:39.185277: step 5477, loss 0.00688088, acc 1\n",
      "2022-03-25T10:32:39.334819: step 5478, loss 0.00136528, acc 1\n",
      "2022-03-25T10:32:39.487489: step 5479, loss 0.000687656, acc 1\n",
      "2022-03-25T10:32:39.639646: step 5480, loss 0.00135599, acc 1\n",
      "2022-03-25T10:32:39.794219: step 5481, loss 0.00834079, acc 1\n",
      "2022-03-25T10:32:39.946004: step 5482, loss 0.00116026, acc 1\n",
      "2022-03-25T10:32:40.106744: step 5483, loss 0.00142309, acc 1\n",
      "2022-03-25T10:32:40.272206: step 5484, loss 0.000176815, acc 1\n",
      "2022-03-25T10:32:40.427285: step 5485, loss 0.00116608, acc 1\n",
      "2022-03-25T10:32:40.575908: step 5486, loss 0.00126603, acc 1\n",
      "2022-03-25T10:32:40.732201: step 5487, loss 0.000221866, acc 1\n",
      "2022-03-25T10:32:40.895739: step 5488, loss 0.00255954, acc 1\n",
      "2022-03-25T10:32:41.046986: step 5489, loss 0.00168882, acc 1\n",
      "2022-03-25T10:32:41.196382: step 5490, loss 0.000822649, acc 1\n",
      "2022-03-25T10:32:41.349793: step 5491, loss 0.000507885, acc 1\n",
      "2022-03-25T10:32:41.509169: step 5492, loss 0.00795015, acc 1\n",
      "2022-03-25T10:32:41.661061: step 5493, loss 0.000320381, acc 1\n",
      "2022-03-25T10:32:41.815052: step 5494, loss 0.000552686, acc 1\n",
      "2022-03-25T10:32:41.963420: step 5495, loss 0.00193675, acc 1\n",
      "2022-03-25T10:32:42.115145: step 5496, loss 0.00113716, acc 1\n",
      "2022-03-25T10:32:42.264577: step 5497, loss 0.000231605, acc 1\n",
      "2022-03-25T10:32:42.413359: step 5498, loss 0.00295242, acc 1\n",
      "2022-03-25T10:32:42.562332: step 5499, loss 0.00474587, acc 1\n",
      "2022-03-25T10:32:42.705869: step 5500, loss 0.00759068, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:32:42.853318: step 5500, loss 0.509176, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5500\n",
      "\n",
      "2022-03-25T10:32:43.115919: step 5501, loss 0.00078161, acc 1\n",
      "2022-03-25T10:32:43.264225: step 5502, loss 0.000753235, acc 1\n",
      "2022-03-25T10:32:43.399494: step 5503, loss 0.00429901, acc 1\n",
      "2022-03-25T10:32:43.548341: step 5504, loss 0.000689654, acc 1\n",
      "2022-03-25T10:32:43.695742: step 5505, loss 9.08079e-05, acc 1\n",
      "2022-03-25T10:32:43.843883: step 5506, loss 0.0119429, acc 1\n",
      "2022-03-25T10:32:43.992162: step 5507, loss 0.000155853, acc 1\n",
      "2022-03-25T10:32:44.140886: step 5508, loss 0.000280518, acc 1\n",
      "2022-03-25T10:32:44.284620: step 5509, loss 0.00233618, acc 1\n",
      "2022-03-25T10:32:44.431538: step 5510, loss 0.000394475, acc 1\n",
      "2022-03-25T10:32:44.581375: step 5511, loss 0.00618773, acc 1\n",
      "2022-03-25T10:32:44.735381: step 5512, loss 0.00237669, acc 1\n",
      "2022-03-25T10:32:44.883770: step 5513, loss 0.000134436, acc 1\n",
      "2022-03-25T10:32:45.027948: step 5514, loss 0.000642917, acc 1\n",
      "2022-03-25T10:32:45.169199: step 5515, loss 0.000772298, acc 1\n",
      "2022-03-25T10:32:45.321766: step 5516, loss 0.000483503, acc 1\n",
      "2022-03-25T10:32:45.466295: step 5517, loss 0.000583001, acc 1\n",
      "2022-03-25T10:32:45.618222: step 5518, loss 0.000430485, acc 1\n",
      "2022-03-25T10:32:45.761230: step 5519, loss 0.000308986, acc 1\n",
      "2022-03-25T10:32:45.908221: step 5520, loss 0.00347008, acc 1\n",
      "2022-03-25T10:32:46.063133: step 5521, loss 0.00120093, acc 1\n",
      "2022-03-25T10:32:46.214149: step 5522, loss 0.000562376, acc 1\n",
      "2022-03-25T10:32:46.359516: step 5523, loss 0.00028368, acc 1\n",
      "2022-03-25T10:32:46.504716: step 5524, loss 0.000261995, acc 1\n",
      "2022-03-25T10:32:46.646617: step 5525, loss 0.00082723, acc 1\n",
      "2022-03-25T10:32:46.794697: step 5526, loss 0.00243277, acc 1\n",
      "2022-03-25T10:32:46.935278: step 5527, loss 6.8925e-05, acc 1\n",
      "2022-03-25T10:32:47.090785: step 5528, loss 0.000126363, acc 1\n",
      "2022-03-25T10:32:47.234495: step 5529, loss 0.0028565, acc 1\n",
      "2022-03-25T10:32:47.388984: step 5530, loss 0.000325405, acc 1\n",
      "2022-03-25T10:32:47.532180: step 5531, loss 0.000148095, acc 1\n",
      "2022-03-25T10:32:47.681180: step 5532, loss 0.000907954, acc 1\n",
      "2022-03-25T10:32:47.825755: step 5533, loss 0.00134973, acc 1\n",
      "2022-03-25T10:32:47.963762: step 5534, loss 0.00215656, acc 1\n",
      "2022-03-25T10:32:48.105479: step 5535, loss 0.000822025, acc 1\n",
      "2022-03-25T10:32:48.259461: step 5536, loss 0.00287914, acc 1\n",
      "2022-03-25T10:32:48.406013: step 5537, loss 0.000334262, acc 1\n",
      "2022-03-25T10:32:48.557776: step 5538, loss 9.30575e-05, acc 1\n",
      "2022-03-25T10:32:48.697895: step 5539, loss 0.000376613, acc 1\n",
      "2022-03-25T10:32:48.845672: step 5540, loss 7.17798e-05, acc 1\n",
      "2022-03-25T10:32:48.985276: step 5541, loss 0.000546093, acc 1\n",
      "2022-03-25T10:32:49.143250: step 5542, loss 0.00245036, acc 1\n",
      "2022-03-25T10:32:49.281038: step 5543, loss 0.000296987, acc 1\n",
      "2022-03-25T10:32:49.436731: step 5544, loss 0.0432112, acc 0.984375\n",
      "2022-03-25T10:32:49.575892: step 5545, loss 0.000299123, acc 1\n",
      "2022-03-25T10:32:49.729738: step 5546, loss 0.00205947, acc 1\n",
      "2022-03-25T10:32:49.875873: step 5547, loss 9.03932e-05, acc 1\n",
      "2022-03-25T10:32:50.021298: step 5548, loss 0.000200518, acc 1\n",
      "2022-03-25T10:32:50.172840: step 5549, loss 0.00020251, acc 1\n",
      "2022-03-25T10:32:50.314430: step 5550, loss 0.000258258, acc 1\n",
      "2022-03-25T10:32:50.450959: step 5551, loss 0.000164934, acc 1\n",
      "2022-03-25T10:32:50.610261: step 5552, loss 0.000352714, acc 1\n",
      "2022-03-25T10:32:50.755482: step 5553, loss 7.77612e-05, acc 1\n",
      "2022-03-25T10:32:50.901188: step 5554, loss 0.000938498, acc 1\n",
      "2022-03-25T10:32:51.049416: step 5555, loss 0.00459911, acc 1\n",
      "2022-03-25T10:32:51.194211: step 5556, loss 0.0259401, acc 0.984375\n",
      "2022-03-25T10:32:51.338953: step 5557, loss 0.000480754, acc 1\n",
      "2022-03-25T10:32:51.488377: step 5558, loss 0.00552682, acc 1\n",
      "2022-03-25T10:32:51.633015: step 5559, loss 0.00147886, acc 1\n",
      "2022-03-25T10:32:51.777623: step 5560, loss 0.000232336, acc 1\n",
      "2022-03-25T10:32:51.924111: step 5561, loss 0.00611187, acc 1\n",
      "2022-03-25T10:32:52.068948: step 5562, loss 0.0111083, acc 1\n",
      "2022-03-25T10:32:52.212288: step 5563, loss 0.00569963, acc 1\n",
      "2022-03-25T10:32:52.363667: step 5564, loss 0.000250024, acc 1\n",
      "2022-03-25T10:32:52.507569: step 5565, loss 0.0004742, acc 1\n",
      "2022-03-25T10:32:52.648533: step 5566, loss 0.000118911, acc 1\n",
      "2022-03-25T10:32:52.791586: step 5567, loss 0.000270158, acc 1\n",
      "2022-03-25T10:32:52.945281: step 5568, loss 0.000935529, acc 1\n",
      "2022-03-25T10:32:53.089957: step 5569, loss 0.00155405, acc 1\n",
      "2022-03-25T10:32:53.246816: step 5570, loss 0.00107242, acc 1\n",
      "2022-03-25T10:32:53.388220: step 5571, loss 0.000251347, acc 1\n",
      "2022-03-25T10:32:53.540286: step 5572, loss 0.000582724, acc 1\n",
      "2022-03-25T10:32:53.682672: step 5573, loss 0.000764444, acc 1\n",
      "2022-03-25T10:32:53.827510: step 5574, loss 0.000191197, acc 1\n",
      "2022-03-25T10:32:53.965336: step 5575, loss 0.000688306, acc 1\n",
      "2022-03-25T10:32:54.120165: step 5576, loss 0.000317964, acc 1\n",
      "2022-03-25T10:32:54.265670: step 5577, loss 0.000302152, acc 1\n",
      "2022-03-25T10:32:54.409423: step 5578, loss 0.000837773, acc 1\n",
      "2022-03-25T10:32:54.548535: step 5579, loss 0.00025528, acc 1\n",
      "2022-03-25T10:32:54.704376: step 5580, loss 0.0023729, acc 1\n",
      "2022-03-25T10:32:54.852446: step 5581, loss 0.00442292, acc 1\n",
      "2022-03-25T10:32:55.003948: step 5582, loss 0.001079, acc 1\n",
      "2022-03-25T10:32:55.161414: step 5583, loss 0.00205903, acc 1\n",
      "2022-03-25T10:32:55.312664: step 5584, loss 0.000910384, acc 1\n",
      "2022-03-25T10:32:55.456185: step 5585, loss 0.00108052, acc 1\n",
      "2022-03-25T10:32:55.601218: step 5586, loss 0.000374301, acc 1\n",
      "2022-03-25T10:32:55.754812: step 5587, loss 0.000248457, acc 1\n",
      "2022-03-25T10:32:55.900772: step 5588, loss 0.000878016, acc 1\n",
      "2022-03-25T10:32:56.044955: step 5589, loss 0.00701199, acc 1\n",
      "2022-03-25T10:32:56.193989: step 5590, loss 0.000959681, acc 1\n",
      "2022-03-25T10:32:56.346767: step 5591, loss 0.000478172, acc 1\n",
      "2022-03-25T10:32:56.487892: step 5592, loss 0.00335674, acc 1\n",
      "2022-03-25T10:32:56.627714: step 5593, loss 0.00104936, acc 1\n",
      "2022-03-25T10:32:56.780390: step 5594, loss 0.00046558, acc 1\n",
      "2022-03-25T10:32:56.938154: step 5595, loss 0.000258885, acc 1\n",
      "2022-03-25T10:32:57.074968: step 5596, loss 0.00138282, acc 1\n",
      "2022-03-25T10:32:57.222338: step 5597, loss 0.000251953, acc 1\n",
      "2022-03-25T10:32:57.375362: step 5598, loss 0.00165896, acc 1\n",
      "2022-03-25T10:32:57.516304: step 5599, loss 0.00037689, acc 1\n",
      "2022-03-25T10:32:57.667694: step 5600, loss 0.000837132, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:32:57.799042: step 5600, loss 0.535923, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5600\n",
      "\n",
      "2022-03-25T10:32:58.049471: step 5601, loss 0.00302097, acc 1\n",
      "2022-03-25T10:32:58.191454: step 5602, loss 0.00227385, acc 1\n",
      "2022-03-25T10:32:58.330785: step 5603, loss 0.00231446, acc 1\n",
      "2022-03-25T10:32:58.478413: step 5604, loss 0.000227091, acc 1\n",
      "2022-03-25T10:32:58.613017: step 5605, loss 0.00117013, acc 1\n",
      "2022-03-25T10:32:58.771651: step 5606, loss 0.000624684, acc 1\n",
      "2022-03-25T10:32:58.912400: step 5607, loss 0.0134633, acc 0.984375\n",
      "2022-03-25T10:32:59.057096: step 5608, loss 0.00223821, acc 1\n",
      "2022-03-25T10:32:59.198955: step 5609, loss 0.000180493, acc 1\n",
      "2022-03-25T10:32:59.357725: step 5610, loss 0.000375008, acc 1\n",
      "2022-03-25T10:32:59.500214: step 5611, loss 0.000763261, acc 1\n",
      "2022-03-25T10:32:59.649209: step 5612, loss 0.000526836, acc 1\n",
      "2022-03-25T10:32:59.794399: step 5613, loss 0.000459423, acc 1\n",
      "2022-03-25T10:32:59.938728: step 5614, loss 0.00042908, acc 1\n",
      "2022-03-25T10:33:00.071481: step 5615, loss 0.000157365, acc 1\n",
      "2022-03-25T10:33:00.222346: step 5616, loss 0.000659611, acc 1\n",
      "2022-03-25T10:33:00.376224: step 5617, loss 0.0040035, acc 1\n",
      "2022-03-25T10:33:00.516501: step 5618, loss 0.000857069, acc 1\n",
      "2022-03-25T10:33:00.661733: step 5619, loss 0.000675067, acc 1\n",
      "2022-03-25T10:33:00.808450: step 5620, loss 0.0013046, acc 1\n",
      "2022-03-25T10:33:00.952397: step 5621, loss 0.00113028, acc 1\n",
      "2022-03-25T10:33:01.088424: step 5622, loss 0.000156662, acc 1\n",
      "2022-03-25T10:33:01.232682: step 5623, loss 0.000383477, acc 1\n",
      "2022-03-25T10:33:01.387891: step 5624, loss 0.00489298, acc 1\n",
      "2022-03-25T10:33:01.537281: step 5625, loss 0.000391665, acc 1\n",
      "2022-03-25T10:33:01.660549: step 5626, loss 0.00617318, acc 1\n",
      "2022-03-25T10:33:01.805866: step 5627, loss 0.00265068, acc 1\n",
      "2022-03-25T10:33:01.958990: step 5628, loss 0.0165229, acc 0.984375\n",
      "2022-03-25T10:33:02.096918: step 5629, loss 0.000569265, acc 1\n",
      "2022-03-25T10:33:02.245807: step 5630, loss 0.00105987, acc 1\n",
      "2022-03-25T10:33:02.396051: step 5631, loss 0.00017996, acc 1\n",
      "2022-03-25T10:33:02.534457: step 5632, loss 0.00106035, acc 1\n",
      "2022-03-25T10:33:02.671126: step 5633, loss 0.000431296, acc 1\n",
      "2022-03-25T10:33:02.818362: step 5634, loss 0.00600318, acc 1\n",
      "2022-03-25T10:33:02.959274: step 5635, loss 0.000428726, acc 1\n",
      "2022-03-25T10:33:03.114000: step 5636, loss 0.00041374, acc 1\n",
      "2022-03-25T10:33:03.251616: step 5637, loss 0.00236301, acc 1\n",
      "2022-03-25T10:33:03.415321: step 5638, loss 0.000348255, acc 1\n",
      "2022-03-25T10:33:03.561180: step 5639, loss 0.000745447, acc 1\n",
      "2022-03-25T10:33:03.705690: step 5640, loss 0.0056236, acc 1\n",
      "2022-03-25T10:33:03.845013: step 5641, loss 0.000401216, acc 1\n",
      "2022-03-25T10:33:04.033530: step 5642, loss 0.00100114, acc 1\n",
      "2022-03-25T10:33:04.171824: step 5643, loss 0.000233618, acc 1\n",
      "2022-03-25T10:33:04.311874: step 5644, loss 0.000173767, acc 1\n",
      "2022-03-25T10:33:04.455719: step 5645, loss 0.00119369, acc 1\n",
      "2022-03-25T10:33:04.600732: step 5646, loss 0.00076419, acc 1\n",
      "2022-03-25T10:33:04.735985: step 5647, loss 0.000994623, acc 1\n",
      "2022-03-25T10:33:04.890185: step 5648, loss 0.0032438, acc 1\n",
      "2022-03-25T10:33:05.029537: step 5649, loss 0.0293223, acc 0.984375\n",
      "2022-03-25T10:33:05.177713: step 5650, loss 0.000652167, acc 1\n",
      "2022-03-25T10:33:05.319229: step 5651, loss 0.0010653, acc 1\n",
      "2022-03-25T10:33:05.481608: step 5652, loss 0.00024814, acc 1\n",
      "2022-03-25T10:33:05.619378: step 5653, loss 0.00295784, acc 1\n",
      "2022-03-25T10:33:05.765436: step 5654, loss 5.67482e-05, acc 1\n",
      "2022-03-25T10:33:05.904792: step 5655, loss 0.000479986, acc 1\n",
      "2022-03-25T10:33:06.044090: step 5656, loss 0.000450838, acc 1\n",
      "2022-03-25T10:33:06.184485: step 5657, loss 0.00363902, acc 1\n",
      "2022-03-25T10:33:06.332509: step 5658, loss 0.000592044, acc 1\n",
      "2022-03-25T10:33:06.484908: step 5659, loss 0.000151123, acc 1\n",
      "2022-03-25T10:33:06.628481: step 5660, loss 3.08604e-05, acc 1\n",
      "2022-03-25T10:33:06.763260: step 5661, loss 0.000492491, acc 1\n",
      "2022-03-25T10:33:06.922320: step 5662, loss 0.000244175, acc 1\n",
      "2022-03-25T10:33:07.062202: step 5663, loss 0.000843473, acc 1\n",
      "2022-03-25T10:33:07.204506: step 5664, loss 5.57028e-05, acc 1\n",
      "2022-03-25T10:33:07.350266: step 5665, loss 0.00017071, acc 1\n",
      "2022-03-25T10:33:07.509660: step 5666, loss 0.00207062, acc 1\n",
      "2022-03-25T10:33:07.654988: step 5667, loss 0.0020928, acc 1\n",
      "2022-03-25T10:33:07.806800: step 5668, loss 0.000697179, acc 1\n",
      "2022-03-25T10:33:07.951930: step 5669, loss 0.00122789, acc 1\n",
      "2022-03-25T10:33:08.097476: step 5670, loss 4.74322e-05, acc 1\n",
      "2022-03-25T10:33:08.239688: step 5671, loss 0.00046616, acc 1\n",
      "2022-03-25T10:33:08.391951: step 5672, loss 0.000177345, acc 1\n",
      "2022-03-25T10:33:08.539190: step 5673, loss 0.000330008, acc 1\n",
      "2022-03-25T10:33:08.688646: step 5674, loss 0.000859037, acc 1\n",
      "2022-03-25T10:33:08.839190: step 5675, loss 3.22162e-05, acc 1\n",
      "2022-03-25T10:33:08.989847: step 5676, loss 0.000566883, acc 1\n",
      "2022-03-25T10:33:09.154689: step 5677, loss 0.000413399, acc 1\n",
      "2022-03-25T10:33:09.303477: step 5678, loss 0.00110054, acc 1\n",
      "2022-03-25T10:33:09.452496: step 5679, loss 0.000509768, acc 1\n",
      "2022-03-25T10:33:09.611753: step 5680, loss 0.000214658, acc 1\n",
      "2022-03-25T10:33:09.755528: step 5681, loss 0.000654426, acc 1\n",
      "2022-03-25T10:33:09.914617: step 5682, loss 0.00906805, acc 1\n",
      "2022-03-25T10:33:10.063716: step 5683, loss 0.00747335, acc 1\n",
      "2022-03-25T10:33:10.216787: step 5684, loss 0.000822395, acc 1\n",
      "2022-03-25T10:33:10.369876: step 5685, loss 0.0023845, acc 1\n",
      "2022-03-25T10:33:10.520489: step 5686, loss 0.000549296, acc 1\n",
      "2022-03-25T10:33:10.685590: step 5687, loss 0.00153456, acc 1\n",
      "2022-03-25T10:33:10.832220: step 5688, loss 0.00340423, acc 1\n",
      "2022-03-25T10:33:10.982265: step 5689, loss 0.000369794, acc 1\n",
      "2022-03-25T10:33:11.133759: step 5690, loss 0.00257935, acc 1\n",
      "2022-03-25T10:33:11.282110: step 5691, loss 0.000307414, acc 1\n",
      "2022-03-25T10:33:11.430675: step 5692, loss 0.00265935, acc 1\n",
      "2022-03-25T10:33:11.598189: step 5693, loss 0.00357209, acc 1\n",
      "2022-03-25T10:33:11.741313: step 5694, loss 0.00186302, acc 1\n",
      "2022-03-25T10:33:11.896694: step 5695, loss 0.000379825, acc 1\n",
      "2022-03-25T10:33:12.047003: step 5696, loss 0.000697948, acc 1\n",
      "2022-03-25T10:33:12.202131: step 5697, loss 0.000294057, acc 1\n",
      "2022-03-25T10:33:12.346404: step 5698, loss 0.000737216, acc 1\n",
      "2022-03-25T10:33:12.499999: step 5699, loss 0.00583528, acc 1\n",
      "2022-03-25T10:33:12.662907: step 5700, loss 5.45009e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:33:12.815182: step 5700, loss 0.546653, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5700\n",
      "\n",
      "2022-03-25T10:33:13.086938: step 5701, loss 0.000418673, acc 1\n",
      "2022-03-25T10:33:13.238201: step 5702, loss 0.00157535, acc 1\n",
      "2022-03-25T10:33:13.393899: step 5703, loss 0.00925363, acc 1\n",
      "2022-03-25T10:33:13.556467: step 5704, loss 0.000150452, acc 1\n",
      "2022-03-25T10:33:13.714913: step 5705, loss 0.000336077, acc 1\n",
      "2022-03-25T10:33:13.863146: step 5706, loss 0.00550373, acc 1\n",
      "2022-03-25T10:33:14.016466: step 5707, loss 0.00200609, acc 1\n",
      "2022-03-25T10:33:14.172654: step 5708, loss 0.000555015, acc 1\n",
      "2022-03-25T10:33:14.338911: step 5709, loss 0.000177637, acc 1\n",
      "2022-03-25T10:33:14.490278: step 5710, loss 0.00244951, acc 1\n",
      "2022-03-25T10:33:14.657145: step 5711, loss 0.000415708, acc 1\n",
      "2022-03-25T10:33:14.810886: step 5712, loss 0.00458459, acc 1\n",
      "2022-03-25T10:33:14.975770: step 5713, loss 0.000130336, acc 1\n",
      "2022-03-25T10:33:15.132504: step 5714, loss 0.00137033, acc 1\n",
      "2022-03-25T10:33:15.285908: step 5715, loss 0.00250197, acc 1\n",
      "2022-03-25T10:33:15.445801: step 5716, loss 0.000307672, acc 1\n",
      "2022-03-25T10:33:15.600899: step 5717, loss 0.00874265, acc 1\n",
      "2022-03-25T10:33:15.751117: step 5718, loss 0.000997272, acc 1\n",
      "2022-03-25T10:33:15.904278: step 5719, loss 0.000505615, acc 1\n",
      "2022-03-25T10:33:16.055489: step 5720, loss 0.000592939, acc 1\n",
      "2022-03-25T10:33:16.206086: step 5721, loss 0.000126181, acc 1\n",
      "2022-03-25T10:33:16.352026: step 5722, loss 0.000958211, acc 1\n",
      "2022-03-25T10:33:16.512803: step 5723, loss 0.000130043, acc 1\n",
      "2022-03-25T10:33:16.662979: step 5724, loss 0.00257857, acc 1\n",
      "2022-03-25T10:33:16.822607: step 5725, loss 0.000629549, acc 1\n",
      "2022-03-25T10:33:16.975213: step 5726, loss 0.000538722, acc 1\n",
      "2022-03-25T10:33:17.139508: step 5727, loss 0.000581096, acc 1\n",
      "2022-03-25T10:33:17.293728: step 5728, loss 0.00064394, acc 1\n",
      "2022-03-25T10:33:17.443176: step 5729, loss 0.00476258, acc 1\n",
      "2022-03-25T10:33:17.600501: step 5730, loss 0.000989695, acc 1\n",
      "2022-03-25T10:33:17.768508: step 5731, loss 7.40537e-05, acc 1\n",
      "2022-03-25T10:33:17.921793: step 5732, loss 0.000446012, acc 1\n",
      "2022-03-25T10:33:18.069470: step 5733, loss 0.000576157, acc 1\n",
      "2022-03-25T10:33:18.217867: step 5734, loss 0.0202974, acc 0.984375\n",
      "2022-03-25T10:33:18.369647: step 5735, loss 0.00308831, acc 1\n",
      "2022-03-25T10:33:18.517314: step 5736, loss 0.011703, acc 0.984375\n",
      "2022-03-25T10:33:18.673175: step 5737, loss 0.000301876, acc 1\n",
      "2022-03-25T10:33:18.824692: step 5738, loss 0.000584606, acc 1\n",
      "2022-03-25T10:33:18.980817: step 5739, loss 0.000302618, acc 1\n",
      "2022-03-25T10:33:19.146082: step 5740, loss 0.000876365, acc 1\n",
      "2022-03-25T10:33:19.299236: step 5741, loss 0.000269462, acc 1\n",
      "2022-03-25T10:33:19.455701: step 5742, loss 0.00044104, acc 1\n",
      "2022-03-25T10:33:19.605172: step 5743, loss 0.000445355, acc 1\n",
      "2022-03-25T10:33:19.758462: step 5744, loss 0.00134922, acc 1\n",
      "2022-03-25T10:33:19.909528: step 5745, loss 0.00140408, acc 1\n",
      "2022-03-25T10:33:20.070531: step 5746, loss 8.08604e-05, acc 1\n",
      "2022-03-25T10:33:20.229783: step 5747, loss 8.15044e-05, acc 1\n",
      "2022-03-25T10:33:20.379083: step 5748, loss 0.000536342, acc 1\n",
      "2022-03-25T10:33:20.533319: step 5749, loss 0.00182612, acc 1\n",
      "2022-03-25T10:33:20.681041: step 5750, loss 0.000586783, acc 1\n",
      "2022-03-25T10:33:20.848585: step 5751, loss 0.000147896, acc 1\n",
      "2022-03-25T10:33:20.996485: step 5752, loss 0.000151644, acc 1\n",
      "2022-03-25T10:33:21.149997: step 5753, loss 0.000371018, acc 1\n",
      "2022-03-25T10:33:21.298763: step 5754, loss 0.000395899, acc 1\n",
      "2022-03-25T10:33:21.452918: step 5755, loss 0.000223109, acc 1\n",
      "2022-03-25T10:33:21.602366: step 5756, loss 0.00265137, acc 1\n",
      "2022-03-25T10:33:21.752995: step 5757, loss 0.0157162, acc 0.984375\n",
      "2022-03-25T10:33:21.911411: step 5758, loss 0.000992549, acc 1\n",
      "2022-03-25T10:33:22.063603: step 5759, loss 0.00265164, acc 1\n",
      "2022-03-25T10:33:22.215596: step 5760, loss 0.000564377, acc 1\n",
      "2022-03-25T10:33:22.371202: step 5761, loss 0.000168424, acc 1\n",
      "2022-03-25T10:33:22.516116: step 5762, loss 0.00455573, acc 1\n",
      "2022-03-25T10:33:22.664833: step 5763, loss 9.10652e-05, acc 1\n",
      "2022-03-25T10:33:22.815596: step 5764, loss 0.000645421, acc 1\n",
      "2022-03-25T10:33:22.972859: step 5765, loss 0.00219663, acc 1\n",
      "2022-03-25T10:33:23.134210: step 5766, loss 0.00099405, acc 1\n",
      "2022-03-25T10:33:23.294561: step 5767, loss 0.000303714, acc 1\n",
      "2022-03-25T10:33:23.443381: step 5768, loss 0.0194193, acc 0.984375\n",
      "2022-03-25T10:33:23.589915: step 5769, loss 0.00222335, acc 1\n",
      "2022-03-25T10:33:23.742132: step 5770, loss 0.000411239, acc 1\n",
      "2022-03-25T10:33:23.896980: step 5771, loss 6.79826e-05, acc 1\n",
      "2022-03-25T10:33:24.053693: step 5772, loss 0.000330898, acc 1\n",
      "2022-03-25T10:33:24.208073: step 5773, loss 0.000153385, acc 1\n",
      "2022-03-25T10:33:24.364903: step 5774, loss 0.000514362, acc 1\n",
      "2022-03-25T10:33:24.528552: step 5775, loss 0.000123156, acc 1\n",
      "2022-03-25T10:33:24.688789: step 5776, loss 0.000133343, acc 1\n",
      "2022-03-25T10:33:24.843314: step 5777, loss 0.000334087, acc 1\n",
      "2022-03-25T10:33:25.007761: step 5778, loss 0.000214379, acc 1\n",
      "2022-03-25T10:33:25.163134: step 5779, loss 0.000207755, acc 1\n",
      "2022-03-25T10:33:25.320917: step 5780, loss 0.00111844, acc 1\n",
      "2022-03-25T10:33:25.472564: step 5781, loss 0.000292898, acc 1\n",
      "2022-03-25T10:33:25.620729: step 5782, loss 0.00349002, acc 1\n",
      "2022-03-25T10:33:25.773207: step 5783, loss 0.00103558, acc 1\n",
      "2022-03-25T10:33:25.927507: step 5784, loss 0.000857904, acc 1\n",
      "2022-03-25T10:33:26.096176: step 5785, loss 0.0004595, acc 1\n",
      "2022-03-25T10:33:26.247005: step 5786, loss 0.0092304, acc 1\n",
      "2022-03-25T10:33:26.401610: step 5787, loss 0.000723818, acc 1\n",
      "2022-03-25T10:33:26.557178: step 5788, loss 0.000134178, acc 1\n",
      "2022-03-25T10:33:26.714741: step 5789, loss 0.000238085, acc 1\n",
      "2022-03-25T10:33:26.865494: step 5790, loss 0.00478969, acc 1\n",
      "2022-03-25T10:33:27.021799: step 5791, loss 0.000885207, acc 1\n",
      "2022-03-25T10:33:27.164512: step 5792, loss 0.00060696, acc 1\n",
      "2022-03-25T10:33:27.315948: step 5793, loss 0.000436083, acc 1\n",
      "2022-03-25T10:33:27.467081: step 5794, loss 0.000397431, acc 1\n",
      "2022-03-25T10:33:27.616204: step 5795, loss 0.0199424, acc 0.984375\n",
      "2022-03-25T10:33:27.771284: step 5796, loss 0.000395986, acc 1\n",
      "2022-03-25T10:33:27.929444: step 5797, loss 8.98236e-05, acc 1\n",
      "2022-03-25T10:33:28.081770: step 5798, loss 0.000592357, acc 1\n",
      "2022-03-25T10:33:28.233433: step 5799, loss 4.88868e-05, acc 1\n",
      "2022-03-25T10:33:28.380112: step 5800, loss 0.000959835, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:33:28.530186: step 5800, loss 0.590198, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5800\n",
      "\n",
      "2022-03-25T10:33:28.801178: step 5801, loss 0.00561337, acc 1\n",
      "2022-03-25T10:33:28.950442: step 5802, loss 0.00025011, acc 1\n",
      "2022-03-25T10:33:29.103464: step 5803, loss 7.04421e-05, acc 1\n",
      "2022-03-25T10:33:29.256602: step 5804, loss 3.73973e-05, acc 1\n",
      "2022-03-25T10:33:29.409634: step 5805, loss 0.00755877, acc 1\n",
      "2022-03-25T10:33:29.564735: step 5806, loss 0.000356498, acc 1\n",
      "2022-03-25T10:33:29.725493: step 5807, loss 8.87612e-05, acc 1\n",
      "2022-03-25T10:33:29.878040: step 5808, loss 0.0015288, acc 1\n",
      "2022-03-25T10:33:30.046564: step 5809, loss 0.00217445, acc 1\n",
      "2022-03-25T10:33:30.198749: step 5810, loss 0.000273535, acc 1\n",
      "2022-03-25T10:33:30.355081: step 5811, loss 0.00494721, acc 1\n",
      "2022-03-25T10:33:30.509532: step 5812, loss 0.00286793, acc 1\n",
      "2022-03-25T10:33:30.663835: step 5813, loss 0.000450864, acc 1\n",
      "2022-03-25T10:33:30.818431: step 5814, loss 0.00201392, acc 1\n",
      "2022-03-25T10:33:30.971154: step 5815, loss 0.000158514, acc 1\n",
      "2022-03-25T10:33:31.117055: step 5816, loss 0.00232497, acc 1\n",
      "2022-03-25T10:33:31.267033: step 5817, loss 0.000970072, acc 1\n",
      "2022-03-25T10:33:31.415392: step 5818, loss 0.00471401, acc 1\n",
      "2022-03-25T10:33:31.577443: step 5819, loss 0.000330853, acc 1\n",
      "2022-03-25T10:33:31.694596: step 5820, loss 0.000743103, acc 1\n",
      "2022-03-25T10:33:31.850992: step 5821, loss 0.000855291, acc 1\n",
      "2022-03-25T10:33:31.992725: step 5822, loss 0.000623109, acc 1\n",
      "2022-03-25T10:33:32.146344: step 5823, loss 0.00707067, acc 1\n",
      "2022-03-25T10:33:32.294483: step 5824, loss 0.00395625, acc 1\n",
      "2022-03-25T10:33:32.445440: step 5825, loss 0.000200373, acc 1\n",
      "2022-03-25T10:33:32.597175: step 5826, loss 0.00100346, acc 1\n",
      "2022-03-25T10:33:32.751228: step 5827, loss 0.000865749, acc 1\n",
      "2022-03-25T10:33:32.904883: step 5828, loss 0.000159108, acc 1\n",
      "2022-03-25T10:33:33.062039: step 5829, loss 0.000306351, acc 1\n",
      "2022-03-25T10:33:33.217714: step 5830, loss 0.00019615, acc 1\n",
      "2022-03-25T10:33:33.375702: step 5831, loss 0.000437786, acc 1\n",
      "2022-03-25T10:33:33.529144: step 5832, loss 6.45174e-05, acc 1\n",
      "2022-03-25T10:33:33.685924: step 5833, loss 0.00016491, acc 1\n",
      "2022-03-25T10:33:33.831462: step 5834, loss 0.0038589, acc 1\n",
      "2022-03-25T10:33:33.982697: step 5835, loss 0.00138373, acc 1\n",
      "2022-03-25T10:33:34.139951: step 5836, loss 0.00322116, acc 1\n",
      "2022-03-25T10:33:34.281952: step 5837, loss 0.000667466, acc 1\n",
      "2022-03-25T10:33:34.438296: step 5838, loss 0.00256317, acc 1\n",
      "2022-03-25T10:33:34.586578: step 5839, loss 8.18189e-05, acc 1\n",
      "2022-03-25T10:33:34.752597: step 5840, loss 0.000253623, acc 1\n",
      "2022-03-25T10:33:34.910194: step 5841, loss 0.000439938, acc 1\n",
      "2022-03-25T10:33:35.053848: step 5842, loss 0.000744761, acc 1\n",
      "2022-03-25T10:33:35.217582: step 5843, loss 0.00267118, acc 1\n",
      "2022-03-25T10:33:35.374585: step 5844, loss 0.000668529, acc 1\n",
      "2022-03-25T10:33:35.527245: step 5845, loss 0.000394062, acc 1\n",
      "2022-03-25T10:33:35.668761: step 5846, loss 0.0115965, acc 0.984375\n",
      "2022-03-25T10:33:35.815040: step 5847, loss 0.000361287, acc 1\n",
      "2022-03-25T10:33:35.966257: step 5848, loss 0.000351495, acc 1\n",
      "2022-03-25T10:33:36.121852: step 5849, loss 0.000298051, acc 1\n",
      "2022-03-25T10:33:36.276757: step 5850, loss 0.000104446, acc 1\n",
      "2022-03-25T10:33:36.431740: step 5851, loss 0.000504643, acc 1\n",
      "2022-03-25T10:33:36.585196: step 5852, loss 0.00160595, acc 1\n",
      "2022-03-25T10:33:36.732462: step 5853, loss 0.00028758, acc 1\n",
      "2022-03-25T10:33:36.878425: step 5854, loss 0.000446768, acc 1\n",
      "2022-03-25T10:33:37.025201: step 5855, loss 0.000232513, acc 1\n",
      "2022-03-25T10:33:37.184662: step 5856, loss 0.000346877, acc 1\n",
      "2022-03-25T10:33:37.341863: step 5857, loss 0.00943602, acc 1\n",
      "2022-03-25T10:33:37.495505: step 5858, loss 0.000290083, acc 1\n",
      "2022-03-25T10:33:37.656560: step 5859, loss 6.54118e-05, acc 1\n",
      "2022-03-25T10:33:37.806160: step 5860, loss 0.000235791, acc 1\n",
      "2022-03-25T10:33:37.955214: step 5861, loss 0.000511029, acc 1\n",
      "2022-03-25T10:33:38.107874: step 5862, loss 0.000377174, acc 1\n",
      "2022-03-25T10:33:38.267807: step 5863, loss 5.78609e-05, acc 1\n",
      "2022-03-25T10:33:38.416640: step 5864, loss 0.000101727, acc 1\n",
      "2022-03-25T10:33:38.570383: step 5865, loss 0.000264722, acc 1\n",
      "2022-03-25T10:33:38.714412: step 5866, loss 7.28671e-05, acc 1\n",
      "2022-03-25T10:33:38.875689: step 5867, loss 0.000929045, acc 1\n",
      "2022-03-25T10:33:39.030168: step 5868, loss 9.15149e-05, acc 1\n",
      "2022-03-25T10:33:39.174153: step 5869, loss 0.00031144, acc 1\n",
      "2022-03-25T10:33:39.327293: step 5870, loss 0.000110592, acc 1\n",
      "2022-03-25T10:33:39.476833: step 5871, loss 0.000862849, acc 1\n",
      "2022-03-25T10:33:39.624745: step 5872, loss 0.000602544, acc 1\n",
      "2022-03-25T10:33:39.770865: step 5873, loss 0.00243623, acc 1\n",
      "2022-03-25T10:33:39.916894: step 5874, loss 0.00030986, acc 1\n",
      "2022-03-25T10:33:40.066820: step 5875, loss 0.000728179, acc 1\n",
      "2022-03-25T10:33:40.223698: step 5876, loss 0.000249972, acc 1\n",
      "2022-03-25T10:33:40.371768: step 5877, loss 0.000190675, acc 1\n",
      "2022-03-25T10:33:40.527420: step 5878, loss 0.000977081, acc 1\n",
      "2022-03-25T10:33:40.680039: step 5879, loss 5.37981e-05, acc 1\n",
      "2022-03-25T10:33:40.833914: step 5880, loss 0.000307369, acc 1\n",
      "2022-03-25T10:33:40.992535: step 5881, loss 0.000333345, acc 1\n",
      "2022-03-25T10:33:41.139339: step 5882, loss 8.37154e-05, acc 1\n",
      "2022-03-25T10:33:41.300242: step 5883, loss 0.000304674, acc 1\n",
      "2022-03-25T10:33:41.446022: step 5884, loss 0.000135983, acc 1\n",
      "2022-03-25T10:33:41.591760: step 5885, loss 1.203e-05, acc 1\n",
      "2022-03-25T10:33:41.739224: step 5886, loss 8.79641e-05, acc 1\n",
      "2022-03-25T10:33:41.889533: step 5887, loss 0.00138279, acc 1\n",
      "2022-03-25T10:33:42.042267: step 5888, loss 0.000145631, acc 1\n",
      "2022-03-25T10:33:42.187513: step 5889, loss 0.00133858, acc 1\n",
      "2022-03-25T10:33:42.346710: step 5890, loss 0.000860633, acc 1\n",
      "2022-03-25T10:33:42.502234: step 5891, loss 0.000240551, acc 1\n",
      "2022-03-25T10:33:42.646588: step 5892, loss 0.00170566, acc 1\n",
      "2022-03-25T10:33:42.794234: step 5893, loss 0.000313323, acc 1\n",
      "2022-03-25T10:33:42.943030: step 5894, loss 0.000797006, acc 1\n",
      "2022-03-25T10:33:43.086479: step 5895, loss 0.00488389, acc 1\n",
      "2022-03-25T10:33:43.242695: step 5896, loss 0.000301422, acc 1\n",
      "2022-03-25T10:33:43.394945: step 5897, loss 0.000118075, acc 1\n",
      "2022-03-25T10:33:43.548658: step 5898, loss 0.00033155, acc 1\n",
      "2022-03-25T10:33:43.707294: step 5899, loss 0.000471256, acc 1\n",
      "2022-03-25T10:33:43.855456: step 5900, loss 0.00210613, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:33:44.002508: step 5900, loss 0.569514, acc 0.922965\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-5900\n",
      "\n",
      "2022-03-25T10:33:44.262031: step 5901, loss 0.00153534, acc 1\n",
      "2022-03-25T10:33:44.416623: step 5902, loss 0.000104461, acc 1\n",
      "2022-03-25T10:33:44.580255: step 5903, loss 0.000747299, acc 1\n",
      "2022-03-25T10:33:44.731833: step 5904, loss 3.74983e-05, acc 1\n",
      "2022-03-25T10:33:44.888830: step 5905, loss 0.000599081, acc 1\n",
      "2022-03-25T10:33:45.044236: step 5906, loss 0.000344865, acc 1\n",
      "2022-03-25T10:33:45.187797: step 5907, loss 0.000161748, acc 1\n",
      "2022-03-25T10:33:45.350117: step 5908, loss 0.00011346, acc 1\n",
      "2022-03-25T10:33:45.502386: step 5909, loss 0.00588386, acc 1\n",
      "2022-03-25T10:33:45.650788: step 5910, loss 0.000706282, acc 1\n",
      "2022-03-25T10:33:45.800029: step 5911, loss 0.00487431, acc 1\n",
      "2022-03-25T10:33:45.954573: step 5912, loss 0.00010468, acc 1\n",
      "2022-03-25T10:33:46.111668: step 5913, loss 0.00058198, acc 1\n",
      "2022-03-25T10:33:46.263135: step 5914, loss 0.000751136, acc 1\n",
      "2022-03-25T10:33:46.418350: step 5915, loss 0.00743718, acc 1\n",
      "2022-03-25T10:33:46.571065: step 5916, loss 0.000214387, acc 1\n",
      "2022-03-25T10:33:46.716743: step 5917, loss 0.000554606, acc 1\n",
      "2022-03-25T10:33:46.868803: step 5918, loss 8.33264e-05, acc 1\n",
      "2022-03-25T10:33:47.025299: step 5919, loss 0.000249441, acc 1\n",
      "2022-03-25T10:33:47.175933: step 5920, loss 0.000239719, acc 1\n",
      "2022-03-25T10:33:47.318826: step 5921, loss 0.00100139, acc 1\n",
      "2022-03-25T10:33:47.480224: step 5922, loss 6.34982e-05, acc 1\n",
      "2022-03-25T10:33:47.628459: step 5923, loss 0.00136322, acc 1\n",
      "2022-03-25T10:33:47.780524: step 5924, loss 0.000135426, acc 1\n",
      "2022-03-25T10:33:47.935759: step 5925, loss 0.00865903, acc 1\n",
      "2022-03-25T10:33:48.081767: step 5926, loss 0.00126485, acc 1\n",
      "2022-03-25T10:33:48.228469: step 5927, loss 0.000114389, acc 1\n",
      "2022-03-25T10:33:48.381199: step 5928, loss 0.000147174, acc 1\n",
      "2022-03-25T10:33:48.532183: step 5929, loss 0.000582911, acc 1\n",
      "2022-03-25T10:33:48.678592: step 5930, loss 0.00101977, acc 1\n",
      "2022-03-25T10:33:48.829080: step 5931, loss 0.00133407, acc 1\n",
      "2022-03-25T10:33:48.977637: step 5932, loss 6.41753e-05, acc 1\n",
      "2022-03-25T10:33:49.127413: step 5933, loss 0.00131005, acc 1\n",
      "2022-03-25T10:33:49.281716: step 5934, loss 0.000284626, acc 1\n",
      "2022-03-25T10:33:49.432851: step 5935, loss 0.000145278, acc 1\n",
      "2022-03-25T10:33:49.586568: step 5936, loss 0.000368924, acc 1\n",
      "2022-03-25T10:33:49.740698: step 5937, loss 0.000217912, acc 1\n",
      "2022-03-25T10:33:49.879671: step 5938, loss 0.00035475, acc 1\n",
      "2022-03-25T10:33:50.029923: step 5939, loss 8.92759e-05, acc 1\n",
      "2022-03-25T10:33:50.178446: step 5940, loss 0.000249207, acc 1\n",
      "2022-03-25T10:33:50.333421: step 5941, loss 0.0036801, acc 1\n",
      "2022-03-25T10:33:50.490762: step 5942, loss 0.0084627, acc 1\n",
      "2022-03-25T10:33:50.638280: step 5943, loss 0.000333445, acc 1\n",
      "2022-03-25T10:33:50.791617: step 5944, loss 0.000670181, acc 1\n",
      "2022-03-25T10:33:50.945138: step 5945, loss 0.00116999, acc 1\n",
      "2022-03-25T10:33:51.090622: step 5946, loss 0.000254175, acc 1\n",
      "2022-03-25T10:33:51.241862: step 5947, loss 0.00343959, acc 1\n",
      "2022-03-25T10:33:51.396175: step 5948, loss 0.00141379, acc 1\n",
      "2022-03-25T10:33:51.555815: step 5949, loss 0.000233962, acc 1\n",
      "2022-03-25T10:33:51.705206: step 5950, loss 0.00257357, acc 1\n",
      "2022-03-25T10:33:51.851135: step 5951, loss 0.000221223, acc 1\n",
      "2022-03-25T10:33:51.998621: step 5952, loss 0.000735755, acc 1\n",
      "2022-03-25T10:33:52.151368: step 5953, loss 0.000536773, acc 1\n",
      "2022-03-25T10:33:52.297695: step 5954, loss 0.000200617, acc 1\n",
      "2022-03-25T10:33:52.449778: step 5955, loss 0.000372609, acc 1\n",
      "2022-03-25T10:33:52.594286: step 5956, loss 0.000341548, acc 1\n",
      "2022-03-25T10:33:52.752532: step 5957, loss 0.000349042, acc 1\n",
      "2022-03-25T10:33:52.906296: step 5958, loss 0.000454411, acc 1\n",
      "2022-03-25T10:33:53.060848: step 5959, loss 0.000195287, acc 1\n",
      "2022-03-25T10:33:53.206707: step 5960, loss 3.32337e-05, acc 1\n",
      "2022-03-25T10:33:53.357397: step 5961, loss 0.000607009, acc 1\n",
      "2022-03-25T10:33:53.513368: step 5962, loss 0.000291337, acc 1\n",
      "2022-03-25T10:33:53.656631: step 5963, loss 0.000114999, acc 1\n",
      "2022-03-25T10:33:53.807926: step 5964, loss 5.18699e-05, acc 1\n",
      "2022-03-25T10:33:53.965898: step 5965, loss 0.000111238, acc 1\n",
      "2022-03-25T10:33:54.113945: step 5966, loss 0.002836, acc 1\n",
      "2022-03-25T10:33:54.269420: step 5967, loss 0.00850187, acc 1\n",
      "2022-03-25T10:33:54.419663: step 5968, loss 0.000178229, acc 1\n",
      "2022-03-25T10:33:54.576712: step 5969, loss 0.000457057, acc 1\n",
      "2022-03-25T10:33:54.718746: step 5970, loss 0.00136389, acc 1\n",
      "2022-03-25T10:33:54.880188: step 5971, loss 0.000523911, acc 1\n",
      "2022-03-25T10:33:55.032136: step 5972, loss 0.00375631, acc 1\n",
      "2022-03-25T10:33:55.184360: step 5973, loss 0.00539852, acc 1\n",
      "2022-03-25T10:33:55.332519: step 5974, loss 0.000787874, acc 1\n",
      "2022-03-25T10:33:55.481155: step 5975, loss 0.000299341, acc 1\n",
      "2022-03-25T10:33:55.642467: step 5976, loss 0.00764643, acc 1\n",
      "2022-03-25T10:33:55.794299: step 5977, loss 0.000190808, acc 1\n",
      "2022-03-25T10:33:55.949217: step 5978, loss 0.000284657, acc 1\n",
      "2022-03-25T10:33:56.101467: step 5979, loss 5.58586e-05, acc 1\n",
      "2022-03-25T10:33:56.251780: step 5980, loss 8.21196e-05, acc 1\n",
      "2022-03-25T10:33:56.400499: step 5981, loss 0.00167905, acc 1\n",
      "2022-03-25T10:33:56.550178: step 5982, loss 4.06691e-05, acc 1\n",
      "2022-03-25T10:33:56.701224: step 5983, loss 0.000174983, acc 1\n",
      "2022-03-25T10:33:56.863456: step 5984, loss 0.00412387, acc 1\n",
      "2022-03-25T10:33:57.015790: step 5985, loss 0.00101072, acc 1\n",
      "2022-03-25T10:33:57.167194: step 5986, loss 0.00103271, acc 1\n",
      "2022-03-25T10:33:57.321583: step 5987, loss 5.22654e-05, acc 1\n",
      "2022-03-25T10:33:57.466744: step 5988, loss 2.89255e-05, acc 1\n",
      "2022-03-25T10:33:57.621474: step 5989, loss 0.00271763, acc 1\n",
      "2022-03-25T10:33:57.784928: step 5990, loss 0.000450131, acc 1\n",
      "2022-03-25T10:33:57.937114: step 5991, loss 0.000137146, acc 1\n",
      "2022-03-25T10:33:58.085941: step 5992, loss 0.000948034, acc 1\n",
      "2022-03-25T10:33:58.236616: step 5993, loss 0.00175919, acc 1\n",
      "2022-03-25T10:33:58.389633: step 5994, loss 0.000222526, acc 1\n",
      "2022-03-25T10:33:58.548289: step 5995, loss 0.00584439, acc 1\n",
      "2022-03-25T10:33:58.717992: step 5996, loss 0.00013315, acc 1\n",
      "2022-03-25T10:33:58.869669: step 5997, loss 0.000277287, acc 1\n",
      "2022-03-25T10:33:59.019860: step 5998, loss 0.000135122, acc 1\n",
      "2022-03-25T10:33:59.172133: step 5999, loss 0.000950349, acc 1\n",
      "2022-03-25T10:33:59.334710: step 6000, loss 0.000482753, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:33:59.485825: step 6000, loss 0.57934, acc 0.922965\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6000\n",
      "\n",
      "2022-03-25T10:33:59.752242: step 6001, loss 0.000183633, acc 1\n",
      "2022-03-25T10:33:59.894553: step 6002, loss 0.00019707, acc 1\n",
      "2022-03-25T10:34:00.036382: step 6003, loss 5.46134e-05, acc 1\n",
      "2022-03-25T10:34:00.186395: step 6004, loss 0.122169, acc 0.984375\n",
      "2022-03-25T10:34:00.364534: step 6005, loss 0.00091593, acc 1\n",
      "2022-03-25T10:34:00.509715: step 6006, loss 0.00023284, acc 1\n",
      "2022-03-25T10:34:00.666508: step 6007, loss 0.000148764, acc 1\n",
      "2022-03-25T10:34:00.810914: step 6008, loss 0.000164436, acc 1\n",
      "2022-03-25T10:34:00.956971: step 6009, loss 4.51164e-05, acc 1\n",
      "2022-03-25T10:34:01.114755: step 6010, loss 8.47635e-05, acc 1\n",
      "2022-03-25T10:34:01.251461: step 6011, loss 0.00183997, acc 1\n",
      "2022-03-25T10:34:01.409762: step 6012, loss 0.00618471, acc 1\n",
      "2022-03-25T10:34:01.566859: step 6013, loss 0.00272859, acc 1\n",
      "2022-03-25T10:34:01.688765: step 6014, loss 0.00011828, acc 1\n",
      "2022-03-25T10:34:01.835315: step 6015, loss 0.000709687, acc 1\n",
      "2022-03-25T10:34:01.983454: step 6016, loss 9.56916e-05, acc 1\n",
      "2022-03-25T10:34:02.130792: step 6017, loss 0.000140253, acc 1\n",
      "2022-03-25T10:34:02.276480: step 6018, loss 0.000139955, acc 1\n",
      "2022-03-25T10:34:02.425638: step 6019, loss 8.60376e-05, acc 1\n",
      "2022-03-25T10:34:02.570224: step 6020, loss 0.00139807, acc 1\n",
      "2022-03-25T10:34:02.709316: step 6021, loss 0.00137169, acc 1\n",
      "2022-03-25T10:34:02.863736: step 6022, loss 8.49653e-05, acc 1\n",
      "2022-03-25T10:34:03.011823: step 6023, loss 0.000181747, acc 1\n",
      "2022-03-25T10:34:03.158734: step 6024, loss 9.98305e-05, acc 1\n",
      "2022-03-25T10:34:03.310220: step 6025, loss 0.000495027, acc 1\n",
      "2022-03-25T10:34:03.454509: step 6026, loss 0.00014593, acc 1\n",
      "2022-03-25T10:34:03.610284: step 6027, loss 0.000310602, acc 1\n",
      "2022-03-25T10:34:03.767098: step 6028, loss 0.0154834, acc 0.984375\n",
      "2022-03-25T10:34:03.912909: step 6029, loss 0.000152021, acc 1\n",
      "2022-03-25T10:34:04.054784: step 6030, loss 3.47601e-05, acc 1\n",
      "2022-03-25T10:34:04.196994: step 6031, loss 0.00318523, acc 1\n",
      "2022-03-25T10:34:04.348840: step 6032, loss 0.000369951, acc 1\n",
      "2022-03-25T10:34:04.491003: step 6033, loss 0.000259886, acc 1\n",
      "2022-03-25T10:34:04.638121: step 6034, loss 0.0294869, acc 0.96875\n",
      "2022-03-25T10:34:04.793455: step 6035, loss 2.73374e-05, acc 1\n",
      "2022-03-25T10:34:04.948332: step 6036, loss 0.000174265, acc 1\n",
      "2022-03-25T10:34:05.103328: step 6037, loss 0.000283714, acc 1\n",
      "2022-03-25T10:34:05.255515: step 6038, loss 0.000211171, acc 1\n",
      "2022-03-25T10:34:05.410791: step 6039, loss 7.43608e-05, acc 1\n",
      "2022-03-25T10:34:05.550194: step 6040, loss 0.00145529, acc 1\n",
      "2022-03-25T10:34:05.707599: step 6041, loss 0.000185652, acc 1\n",
      "2022-03-25T10:34:05.858209: step 6042, loss 0.000225687, acc 1\n",
      "2022-03-25T10:34:06.003111: step 6043, loss 2.31445e-05, acc 1\n",
      "2022-03-25T10:34:06.154455: step 6044, loss 0.000447642, acc 1\n",
      "2022-03-25T10:34:06.299309: step 6045, loss 0.000253091, acc 1\n",
      "2022-03-25T10:34:06.448915: step 6046, loss 0.00172881, acc 1\n",
      "2022-03-25T10:34:06.594615: step 6047, loss 0.000159888, acc 1\n",
      "2022-03-25T10:34:06.742147: step 6048, loss 0.00119334, acc 1\n",
      "2022-03-25T10:34:06.895847: step 6049, loss 0.000233583, acc 1\n",
      "2022-03-25T10:34:07.042159: step 6050, loss 6.6425e-05, acc 1\n",
      "2022-03-25T10:34:07.194553: step 6051, loss 0.000723146, acc 1\n",
      "2022-03-25T10:34:07.339347: step 6052, loss 0.000459046, acc 1\n",
      "2022-03-25T10:34:07.481315: step 6053, loss 0.00028351, acc 1\n",
      "2022-03-25T10:34:07.635947: step 6054, loss 0.00135399, acc 1\n",
      "2022-03-25T10:34:07.779555: step 6055, loss 0.000370955, acc 1\n",
      "2022-03-25T10:34:07.940160: step 6056, loss 0.00134616, acc 1\n",
      "2022-03-25T10:34:08.080301: step 6057, loss 0.000237226, acc 1\n",
      "2022-03-25T10:34:08.240234: step 6058, loss 8.04786e-05, acc 1\n",
      "2022-03-25T10:34:08.390690: step 6059, loss 9.23203e-05, acc 1\n",
      "2022-03-25T10:34:08.535160: step 6060, loss 2.63438e-05, acc 1\n",
      "2022-03-25T10:34:08.690697: step 6061, loss 0.000902118, acc 1\n",
      "2022-03-25T10:34:08.835938: step 6062, loss 0.00010324, acc 1\n",
      "2022-03-25T10:34:08.985160: step 6063, loss 0.000143629, acc 1\n",
      "2022-03-25T10:34:09.133488: step 6064, loss 0.000184401, acc 1\n",
      "2022-03-25T10:34:09.270490: step 6065, loss 0.000258255, acc 1\n",
      "2022-03-25T10:34:09.424164: step 6066, loss 0.000161769, acc 1\n",
      "2022-03-25T10:34:09.571501: step 6067, loss 0.000153676, acc 1\n",
      "2022-03-25T10:34:09.709140: step 6068, loss 0.000169066, acc 1\n",
      "2022-03-25T10:34:09.860131: step 6069, loss 0.000560795, acc 1\n",
      "2022-03-25T10:34:10.007772: step 6070, loss 0.00037451, acc 1\n",
      "2022-03-25T10:34:10.160039: step 6071, loss 0.00299325, acc 1\n",
      "2022-03-25T10:34:10.304976: step 6072, loss 0.00504458, acc 1\n",
      "2022-03-25T10:34:10.454753: step 6073, loss 0.000183892, acc 1\n",
      "2022-03-25T10:34:10.604512: step 6074, loss 0.000934691, acc 1\n",
      "2022-03-25T10:34:10.758160: step 6075, loss 0.000397534, acc 1\n",
      "2022-03-25T10:34:10.921827: step 6076, loss 0.000297242, acc 1\n",
      "2022-03-25T10:34:11.062174: step 6077, loss 0.00028954, acc 1\n",
      "2022-03-25T10:34:11.213897: step 6078, loss 0.000628094, acc 1\n",
      "2022-03-25T10:34:11.354629: step 6079, loss 0.000158361, acc 1\n",
      "2022-03-25T10:34:11.500176: step 6080, loss 8.1975e-05, acc 1\n",
      "2022-03-25T10:34:11.643171: step 6081, loss 0.000194478, acc 1\n",
      "2022-03-25T10:34:11.787909: step 6082, loss 0.000160784, acc 1\n",
      "2022-03-25T10:34:11.929568: step 6083, loss 0.000503308, acc 1\n",
      "2022-03-25T10:34:12.074838: step 6084, loss 0.000204643, acc 1\n",
      "2022-03-25T10:34:12.230482: step 6085, loss 0.000609904, acc 1\n",
      "2022-03-25T10:34:12.379273: step 6086, loss 3.93451e-05, acc 1\n",
      "2022-03-25T10:34:12.523542: step 6087, loss 0.00534027, acc 1\n",
      "2022-03-25T10:34:12.670364: step 6088, loss 0.000314648, acc 1\n",
      "2022-03-25T10:34:12.815080: step 6089, loss 0.000927202, acc 1\n",
      "2022-03-25T10:34:12.967135: step 6090, loss 0.00134132, acc 1\n",
      "2022-03-25T10:34:13.112267: step 6091, loss 0.000130661, acc 1\n",
      "2022-03-25T10:34:13.262973: step 6092, loss 0.00036239, acc 1\n",
      "2022-03-25T10:34:13.397485: step 6093, loss 0.000393613, acc 1\n",
      "2022-03-25T10:34:13.549133: step 6094, loss 0.000167887, acc 1\n",
      "2022-03-25T10:34:13.698470: step 6095, loss 0.00020912, acc 1\n",
      "2022-03-25T10:34:13.839392: step 6096, loss 0.0435517, acc 0.984375\n",
      "2022-03-25T10:34:13.991365: step 6097, loss 0.000169027, acc 1\n",
      "2022-03-25T10:34:14.139355: step 6098, loss 0.00020522, acc 1\n",
      "2022-03-25T10:34:14.279585: step 6099, loss 0.000607288, acc 1\n",
      "2022-03-25T10:34:14.426507: step 6100, loss 0.00123313, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:34:14.568018: step 6100, loss 0.600967, acc 0.921512\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6100\n",
      "\n",
      "2022-03-25T10:34:14.819510: step 6101, loss 0.00467696, acc 1\n",
      "2022-03-25T10:34:14.964487: step 6102, loss 9.3156e-05, acc 1\n",
      "2022-03-25T10:34:15.102850: step 6103, loss 0.000189022, acc 1\n",
      "2022-03-25T10:34:15.257425: step 6104, loss 8.26228e-05, acc 1\n",
      "2022-03-25T10:34:15.395731: step 6105, loss 0.000131308, acc 1\n",
      "2022-03-25T10:34:15.546659: step 6106, loss 3.66861e-05, acc 1\n",
      "2022-03-25T10:34:15.683465: step 6107, loss 0.00864582, acc 1\n",
      "2022-03-25T10:34:15.835075: step 6108, loss 0.00031714, acc 1\n",
      "2022-03-25T10:34:15.973226: step 6109, loss 9.97852e-05, acc 1\n",
      "2022-03-25T10:34:16.122673: step 6110, loss 6.2497e-05, acc 1\n",
      "2022-03-25T10:34:16.253512: step 6111, loss 0.0135677, acc 0.984375\n",
      "2022-03-25T10:34:16.401216: step 6112, loss 0.000693458, acc 1\n",
      "2022-03-25T10:34:16.540928: step 6113, loss 0.0010277, acc 1\n",
      "2022-03-25T10:34:16.697822: step 6114, loss 9.99186e-05, acc 1\n",
      "2022-03-25T10:34:16.830350: step 6115, loss 0.000136444, acc 1\n",
      "2022-03-25T10:34:16.972113: step 6116, loss 0.000111987, acc 1\n",
      "2022-03-25T10:34:17.112176: step 6117, loss 0.000395747, acc 1\n",
      "2022-03-25T10:34:17.258421: step 6118, loss 0.00147837, acc 1\n",
      "2022-03-25T10:34:17.399434: step 6119, loss 0.000963088, acc 1\n",
      "2022-03-25T10:34:17.555213: step 6120, loss 0.00102149, acc 1\n",
      "2022-03-25T10:34:17.701802: step 6121, loss 0.00115715, acc 1\n",
      "2022-03-25T10:34:17.844403: step 6122, loss 0.000132525, acc 1\n",
      "2022-03-25T10:34:17.981099: step 6123, loss 0.00115549, acc 1\n",
      "2022-03-25T10:34:18.130447: step 6124, loss 0.00345278, acc 1\n",
      "2022-03-25T10:34:18.269737: step 6125, loss 0.00730801, acc 1\n",
      "2022-03-25T10:34:18.409600: step 6126, loss 0.00281459, acc 1\n",
      "2022-03-25T10:34:18.554611: step 6127, loss 0.000108896, acc 1\n",
      "2022-03-25T10:34:18.706613: step 6128, loss 0.000180092, acc 1\n",
      "2022-03-25T10:34:18.847538: step 6129, loss 8.47515e-05, acc 1\n",
      "2022-03-25T10:34:18.998942: step 6130, loss 0.00334737, acc 1\n",
      "2022-03-25T10:34:19.146184: step 6131, loss 0.000308851, acc 1\n",
      "2022-03-25T10:34:19.298593: step 6132, loss 0.000802922, acc 1\n",
      "2022-03-25T10:34:19.441916: step 6133, loss 0.000257287, acc 1\n",
      "2022-03-25T10:34:19.587093: step 6134, loss 0.00159402, acc 1\n",
      "2022-03-25T10:34:19.730752: step 6135, loss 0.000978719, acc 1\n",
      "2022-03-25T10:34:19.882473: step 6136, loss 0.000110574, acc 1\n",
      "2022-03-25T10:34:20.028271: step 6137, loss 0.000965477, acc 1\n",
      "2022-03-25T10:34:20.182538: step 6138, loss 0.000854406, acc 1\n",
      "2022-03-25T10:34:20.317265: step 6139, loss 0.00318662, acc 1\n",
      "2022-03-25T10:34:20.461978: step 6140, loss 0.00051211, acc 1\n",
      "2022-03-25T10:34:20.612104: step 6141, loss 0.0038998, acc 1\n",
      "2022-03-25T10:34:20.755225: step 6142, loss 0.000180396, acc 1\n",
      "2022-03-25T10:34:20.898414: step 6143, loss 0.000394565, acc 1\n",
      "2022-03-25T10:34:21.046119: step 6144, loss 0.00456414, acc 1\n",
      "2022-03-25T10:34:21.189835: step 6145, loss 0.000483535, acc 1\n",
      "2022-03-25T10:34:21.336913: step 6146, loss 0.000827883, acc 1\n",
      "2022-03-25T10:34:21.475152: step 6147, loss 0.000284833, acc 1\n",
      "2022-03-25T10:34:21.630665: step 6148, loss 0.000648867, acc 1\n",
      "2022-03-25T10:34:21.765759: step 6149, loss 0.000145136, acc 1\n",
      "2022-03-25T10:34:21.906359: step 6150, loss 0.000107898, acc 1\n",
      "2022-03-25T10:34:22.049208: step 6151, loss 0.0202172, acc 0.984375\n",
      "2022-03-25T10:34:22.204572: step 6152, loss 0.000289718, acc 1\n",
      "2022-03-25T10:34:22.344545: step 6153, loss 0.000649051, acc 1\n",
      "2022-03-25T10:34:22.491775: step 6154, loss 0.00079556, acc 1\n",
      "2022-03-25T10:34:22.625981: step 6155, loss 0.00147673, acc 1\n",
      "2022-03-25T10:34:22.774149: step 6156, loss 0.0004617, acc 1\n",
      "2022-03-25T10:34:22.918401: step 6157, loss 0.00197082, acc 1\n",
      "2022-03-25T10:34:23.060421: step 6158, loss 0.00253978, acc 1\n",
      "2022-03-25T10:34:23.207164: step 6159, loss 0.00128307, acc 1\n",
      "2022-03-25T10:34:23.350703: step 6160, loss 0.000459199, acc 1\n",
      "2022-03-25T10:34:23.487987: step 6161, loss 0.000615717, acc 1\n",
      "2022-03-25T10:34:23.635001: step 6162, loss 0.000204403, acc 1\n",
      "2022-03-25T10:34:23.770161: step 6163, loss 0.00409426, acc 1\n",
      "2022-03-25T10:34:23.922485: step 6164, loss 0.00147736, acc 1\n",
      "2022-03-25T10:34:24.056002: step 6165, loss 0.000100346, acc 1\n",
      "2022-03-25T10:34:24.206956: step 6166, loss 0.0010286, acc 1\n",
      "2022-03-25T10:34:24.346672: step 6167, loss 0.000662663, acc 1\n",
      "2022-03-25T10:34:24.496023: step 6168, loss 0.000478549, acc 1\n",
      "2022-03-25T10:34:24.644080: step 6169, loss 0.00576769, acc 1\n",
      "2022-03-25T10:34:24.783674: step 6170, loss 5.56509e-05, acc 1\n",
      "2022-03-25T10:34:24.925921: step 6171, loss 4.74112e-05, acc 1\n",
      "2022-03-25T10:34:25.077842: step 6172, loss 0.000874846, acc 1\n",
      "2022-03-25T10:34:25.216537: step 6173, loss 0.000218241, acc 1\n",
      "2022-03-25T10:34:25.368841: step 6174, loss 0.00191556, acc 1\n",
      "2022-03-25T10:34:25.508116: step 6175, loss 3.70616e-05, acc 1\n",
      "2022-03-25T10:34:25.659687: step 6176, loss 0.000389345, acc 1\n",
      "2022-03-25T10:34:25.800317: step 6177, loss 0.00085722, acc 1\n",
      "2022-03-25T10:34:25.953660: step 6178, loss 0.00202483, acc 1\n",
      "2022-03-25T10:34:26.089230: step 6179, loss 0.00394908, acc 1\n",
      "2022-03-25T10:34:26.240983: step 6180, loss 0.00339573, acc 1\n",
      "2022-03-25T10:34:26.380344: step 6181, loss 0.0176045, acc 0.984375\n",
      "2022-03-25T10:34:26.521969: step 6182, loss 0.000700922, acc 1\n",
      "2022-03-25T10:34:26.669487: step 6183, loss 0.00189373, acc 1\n",
      "2022-03-25T10:34:26.814646: step 6184, loss 0.000889709, acc 1\n",
      "2022-03-25T10:34:26.947967: step 6185, loss 0.000166848, acc 1\n",
      "2022-03-25T10:34:27.095223: step 6186, loss 8.93044e-05, acc 1\n",
      "2022-03-25T10:34:27.230177: step 6187, loss 1.44525e-05, acc 1\n",
      "2022-03-25T10:34:27.388719: step 6188, loss 0.000428609, acc 1\n",
      "2022-03-25T10:34:27.525670: step 6189, loss 0.000306635, acc 1\n",
      "2022-03-25T10:34:27.668358: step 6190, loss 6.61111e-05, acc 1\n",
      "2022-03-25T10:34:27.809036: step 6191, loss 0.000639338, acc 1\n",
      "2022-03-25T10:34:27.954212: step 6192, loss 0.000139479, acc 1\n",
      "2022-03-25T10:34:28.103934: step 6193, loss 0.00042659, acc 1\n",
      "2022-03-25T10:34:28.248431: step 6194, loss 0.000751639, acc 1\n",
      "2022-03-25T10:34:28.392969: step 6195, loss 0.0132767, acc 0.984375\n",
      "2022-03-25T10:34:28.546395: step 6196, loss 3.15255e-05, acc 1\n",
      "2022-03-25T10:34:28.690648: step 6197, loss 0.00020292, acc 1\n",
      "2022-03-25T10:34:28.841935: step 6198, loss 0.00337477, acc 1\n",
      "2022-03-25T10:34:28.980313: step 6199, loss 0.00216183, acc 1\n",
      "2022-03-25T10:34:29.125349: step 6200, loss 0.00592889, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:34:29.247855: step 6200, loss 0.60157, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6200\n",
      "\n",
      "2022-03-25T10:34:29.501892: step 6201, loss 0.000456905, acc 1\n",
      "2022-03-25T10:34:29.661895: step 6202, loss 8.39737e-05, acc 1\n",
      "2022-03-25T10:34:29.810121: step 6203, loss 0.000215076, acc 1\n",
      "2022-03-25T10:34:29.956001: step 6204, loss 4.98223e-05, acc 1\n",
      "2022-03-25T10:34:30.086338: step 6205, loss 0.00050219, acc 1\n",
      "2022-03-25T10:34:30.232568: step 6206, loss 0.00226457, acc 1\n",
      "2022-03-25T10:34:30.381768: step 6207, loss 0.00029503, acc 1\n",
      "2022-03-25T10:34:30.504614: step 6208, loss 0.00279162, acc 1\n",
      "2022-03-25T10:34:30.644766: step 6209, loss 9.82713e-05, acc 1\n",
      "2022-03-25T10:34:30.800152: step 6210, loss 0.000115768, acc 1\n",
      "2022-03-25T10:34:30.940503: step 6211, loss 0.00124701, acc 1\n",
      "2022-03-25T10:34:31.089227: step 6212, loss 0.00059384, acc 1\n",
      "2022-03-25T10:34:31.219757: step 6213, loss 0.00284731, acc 1\n",
      "2022-03-25T10:34:31.373744: step 6214, loss 2.54806e-05, acc 1\n",
      "2022-03-25T10:34:31.506240: step 6215, loss 0.000438529, acc 1\n",
      "2022-03-25T10:34:31.654301: step 6216, loss 0.00017687, acc 1\n",
      "2022-03-25T10:34:31.800315: step 6217, loss 0.000270664, acc 1\n",
      "2022-03-25T10:34:31.950484: step 6218, loss 0.000362145, acc 1\n",
      "2022-03-25T10:34:32.089757: step 6219, loss 8.41996e-05, acc 1\n",
      "2022-03-25T10:34:32.236946: step 6220, loss 0.00016872, acc 1\n",
      "2022-03-25T10:34:32.376990: step 6221, loss 0.00277349, acc 1\n",
      "2022-03-25T10:34:32.531083: step 6222, loss 0.00028511, acc 1\n",
      "2022-03-25T10:34:32.674954: step 6223, loss 0.0268745, acc 0.984375\n",
      "2022-03-25T10:34:32.827650: step 6224, loss 0.000540657, acc 1\n",
      "2022-03-25T10:34:32.967774: step 6225, loss 0.000169125, acc 1\n",
      "2022-03-25T10:34:33.109085: step 6226, loss 8.15323e-05, acc 1\n",
      "2022-03-25T10:34:33.253388: step 6227, loss 0.000394032, acc 1\n",
      "2022-03-25T10:34:33.399033: step 6228, loss 0.00330643, acc 1\n",
      "2022-03-25T10:34:33.545197: step 6229, loss 0.000174603, acc 1\n",
      "2022-03-25T10:34:33.695487: step 6230, loss 0.000188015, acc 1\n",
      "2022-03-25T10:34:33.842102: step 6231, loss 3.45086e-05, acc 1\n",
      "2022-03-25T10:34:33.987244: step 6232, loss 0.000220022, acc 1\n",
      "2022-03-25T10:34:34.132008: step 6233, loss 0.000659575, acc 1\n",
      "2022-03-25T10:34:34.276084: step 6234, loss 0.000538113, acc 1\n",
      "2022-03-25T10:34:34.418747: step 6235, loss 0.00427867, acc 1\n",
      "2022-03-25T10:34:34.564445: step 6236, loss 0.000532591, acc 1\n",
      "2022-03-25T10:34:34.704061: step 6237, loss 0.000206517, acc 1\n",
      "2022-03-25T10:34:34.863033: step 6238, loss 0.00902372, acc 1\n",
      "2022-03-25T10:34:34.997270: step 6239, loss 0.000222922, acc 1\n",
      "2022-03-25T10:34:35.145784: step 6240, loss 0.000693339, acc 1\n",
      "2022-03-25T10:34:35.286786: step 6241, loss 0.000129336, acc 1\n",
      "2022-03-25T10:34:35.439550: step 6242, loss 3.68045e-05, acc 1\n",
      "2022-03-25T10:34:35.580581: step 6243, loss 0.000252593, acc 1\n",
      "2022-03-25T10:34:35.731268: step 6244, loss 0.000253788, acc 1\n",
      "2022-03-25T10:34:35.871553: step 6245, loss 0.000279489, acc 1\n",
      "2022-03-25T10:34:36.020391: step 6246, loss 0.0141481, acc 1\n",
      "2022-03-25T10:34:36.176936: step 6247, loss 0.00268313, acc 1\n",
      "2022-03-25T10:34:36.316907: step 6248, loss 0.00234665, acc 1\n",
      "2022-03-25T10:34:36.450878: step 6249, loss 0.000301745, acc 1\n",
      "2022-03-25T10:34:36.592997: step 6250, loss 0.000167139, acc 1\n",
      "2022-03-25T10:34:36.732940: step 6251, loss 0.00403013, acc 1\n",
      "2022-03-25T10:34:36.876261: step 6252, loss 0.00102687, acc 1\n",
      "2022-03-25T10:34:37.013127: step 6253, loss 0.000392786, acc 1\n",
      "2022-03-25T10:34:37.156875: step 6254, loss 0.000294396, acc 1\n",
      "2022-03-25T10:34:37.296783: step 6255, loss 0.000637164, acc 1\n",
      "2022-03-25T10:34:37.454127: step 6256, loss 0.000442915, acc 1\n",
      "2022-03-25T10:34:37.598403: step 6257, loss 0.0449015, acc 0.984375\n",
      "2022-03-25T10:34:37.743287: step 6258, loss 0.0104799, acc 1\n",
      "2022-03-25T10:34:37.877246: step 6259, loss 0.000118563, acc 1\n",
      "2022-03-25T10:34:38.018003: step 6260, loss 0.000133885, acc 1\n",
      "2022-03-25T10:34:38.157664: step 6261, loss 0.00087656, acc 1\n",
      "2022-03-25T10:34:38.303514: step 6262, loss 0.000564956, acc 1\n",
      "2022-03-25T10:34:38.439841: step 6263, loss 0.0294973, acc 0.984375\n",
      "2022-03-25T10:34:38.602979: step 6264, loss 0.00117467, acc 1\n",
      "2022-03-25T10:34:38.749197: step 6265, loss 0.000531987, acc 1\n",
      "2022-03-25T10:34:38.894621: step 6266, loss 5.24363e-05, acc 1\n",
      "2022-03-25T10:34:39.033568: step 6267, loss 0.00338604, acc 1\n",
      "2022-03-25T10:34:39.172997: step 6268, loss 0.00196219, acc 1\n",
      "2022-03-25T10:34:39.309955: step 6269, loss 7.67241e-05, acc 1\n",
      "2022-03-25T10:34:39.457079: step 6270, loss 0.000485071, acc 1\n",
      "2022-03-25T10:34:39.602548: step 6271, loss 0.00155912, acc 1\n",
      "2022-03-25T10:34:39.753202: step 6272, loss 0.000201343, acc 1\n",
      "2022-03-25T10:34:39.901439: step 6273, loss 0.000340853, acc 1\n",
      "2022-03-25T10:34:40.043435: step 6274, loss 0.000959586, acc 1\n",
      "2022-03-25T10:34:40.186454: step 6275, loss 0.000764478, acc 1\n",
      "2022-03-25T10:34:40.332217: step 6276, loss 1.36387e-05, acc 1\n",
      "2022-03-25T10:34:40.474161: step 6277, loss 0.000106737, acc 1\n",
      "2022-03-25T10:34:40.632335: step 6278, loss 1.90981e-05, acc 1\n",
      "2022-03-25T10:34:40.766205: step 6279, loss 9.62629e-06, acc 1\n",
      "2022-03-25T10:34:40.917155: step 6280, loss 0.000481077, acc 1\n",
      "2022-03-25T10:34:41.064975: step 6281, loss 0.000724115, acc 1\n",
      "2022-03-25T10:34:41.222132: step 6282, loss 0.00135011, acc 1\n",
      "2022-03-25T10:34:41.380196: step 6283, loss 0.00013359, acc 1\n",
      "2022-03-25T10:34:41.518743: step 6284, loss 0.00385627, acc 1\n",
      "2022-03-25T10:34:41.671042: step 6285, loss 0.000440683, acc 1\n",
      "2022-03-25T10:34:41.823865: step 6286, loss 0.00096111, acc 1\n",
      "2022-03-25T10:34:41.961468: step 6287, loss 0.000567222, acc 1\n",
      "2022-03-25T10:34:42.117703: step 6288, loss 0.000912617, acc 1\n",
      "2022-03-25T10:34:42.258055: step 6289, loss 0.0019577, acc 1\n",
      "2022-03-25T10:34:42.402501: step 6290, loss 4.9171e-05, acc 1\n",
      "2022-03-25T10:34:42.541318: step 6291, loss 0.000877769, acc 1\n",
      "2022-03-25T10:34:42.701200: step 6292, loss 0.000388811, acc 1\n",
      "2022-03-25T10:34:42.845381: step 6293, loss 0.000984089, acc 1\n",
      "2022-03-25T10:34:42.997755: step 6294, loss 0.00084207, acc 1\n",
      "2022-03-25T10:34:43.128994: step 6295, loss 0.00620518, acc 1\n",
      "2022-03-25T10:34:43.282424: step 6296, loss 0.000215712, acc 1\n",
      "2022-03-25T10:34:43.423633: step 6297, loss 0.000219577, acc 1\n",
      "2022-03-25T10:34:43.578507: step 6298, loss 0.00213426, acc 1\n",
      "2022-03-25T10:34:43.729964: step 6299, loss 0.00339514, acc 1\n",
      "2022-03-25T10:34:43.878413: step 6300, loss 0.0196797, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:34:43.999160: step 6300, loss 0.598027, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6300\n",
      "\n",
      "2022-03-25T10:34:44.251325: step 6301, loss 0.000908094, acc 1\n",
      "2022-03-25T10:34:44.396137: step 6302, loss 9.16155e-05, acc 1\n",
      "2022-03-25T10:34:44.546949: step 6303, loss 0.00297088, acc 1\n",
      "2022-03-25T10:34:44.699622: step 6304, loss 0.000627909, acc 1\n",
      "2022-03-25T10:34:44.850404: step 6305, loss 0.000241377, acc 1\n",
      "2022-03-25T10:34:44.999319: step 6306, loss 5.43981e-05, acc 1\n",
      "2022-03-25T10:34:45.148879: step 6307, loss 0.000492028, acc 1\n",
      "2022-03-25T10:34:45.299398: step 6308, loss 0.000248251, acc 1\n",
      "2022-03-25T10:34:45.458955: step 6309, loss 0.000552811, acc 1\n",
      "2022-03-25T10:34:45.614309: step 6310, loss 0.00118517, acc 1\n",
      "2022-03-25T10:34:45.774579: step 6311, loss 0.000102708, acc 1\n",
      "2022-03-25T10:34:45.921297: step 6312, loss 0.00013794, acc 1\n",
      "2022-03-25T10:34:46.070975: step 6313, loss 0.000763645, acc 1\n",
      "2022-03-25T10:34:46.218947: step 6314, loss 0.000324776, acc 1\n",
      "2022-03-25T10:34:46.379996: step 6315, loss 0.000715426, acc 1\n",
      "2022-03-25T10:34:46.521555: step 6316, loss 0.000817642, acc 1\n",
      "2022-03-25T10:34:46.672037: step 6317, loss 0.000673592, acc 1\n",
      "2022-03-25T10:34:46.816935: step 6318, loss 0.00234005, acc 1\n",
      "2022-03-25T10:34:46.968365: step 6319, loss 0.00125628, acc 1\n",
      "2022-03-25T10:34:47.122514: step 6320, loss 0.0024841, acc 1\n",
      "2022-03-25T10:34:47.277250: step 6321, loss 0.000330694, acc 1\n",
      "2022-03-25T10:34:47.433759: step 6322, loss 0.00387248, acc 1\n",
      "2022-03-25T10:34:47.592288: step 6323, loss 0.00200998, acc 1\n",
      "2022-03-25T10:34:47.745451: step 6324, loss 0.000165114, acc 1\n",
      "2022-03-25T10:34:47.912637: step 6325, loss 0.000268135, acc 1\n",
      "2022-03-25T10:34:48.062774: step 6326, loss 0.000709463, acc 1\n",
      "2022-03-25T10:34:48.215651: step 6327, loss 0.000104223, acc 1\n",
      "2022-03-25T10:34:48.361824: step 6328, loss 0.000715017, acc 1\n",
      "2022-03-25T10:34:48.514077: step 6329, loss 0.000361149, acc 1\n",
      "2022-03-25T10:34:48.668944: step 6330, loss 0.000179921, acc 1\n",
      "2022-03-25T10:34:48.838296: step 6331, loss 0.00298465, acc 1\n",
      "2022-03-25T10:34:48.987509: step 6332, loss 0.000797554, acc 1\n",
      "2022-03-25T10:34:49.136642: step 6333, loss 0.000354064, acc 1\n",
      "2022-03-25T10:34:49.286994: step 6334, loss 3.26006e-05, acc 1\n",
      "2022-03-25T10:34:49.437984: step 6335, loss 0.00244053, acc 1\n",
      "2022-03-25T10:34:49.597183: step 6336, loss 0.000227527, acc 1\n",
      "2022-03-25T10:34:49.748035: step 6337, loss 0.00145833, acc 1\n",
      "2022-03-25T10:34:49.901432: step 6338, loss 5.60273e-06, acc 1\n",
      "2022-03-25T10:34:50.048170: step 6339, loss 0.000167973, acc 1\n",
      "2022-03-25T10:34:50.197912: step 6340, loss 0.000662195, acc 1\n",
      "2022-03-25T10:34:50.352522: step 6341, loss 0.000233336, acc 1\n",
      "2022-03-25T10:34:50.506404: step 6342, loss 4.46305e-05, acc 1\n",
      "2022-03-25T10:34:50.659684: step 6343, loss 0.00198875, acc 1\n",
      "2022-03-25T10:34:50.814150: step 6344, loss 0.00084815, acc 1\n",
      "2022-03-25T10:34:50.975004: step 6345, loss 0.0395445, acc 0.984375\n",
      "2022-03-25T10:34:51.126017: step 6346, loss 0.000133014, acc 1\n",
      "2022-03-25T10:34:51.279439: step 6347, loss 0.000302833, acc 1\n",
      "2022-03-25T10:34:51.427580: step 6348, loss 0.0380398, acc 0.984375\n",
      "2022-03-25T10:34:51.577170: step 6349, loss 0.000152193, acc 1\n",
      "2022-03-25T10:34:51.731771: step 6350, loss 0.00132051, acc 1\n",
      "2022-03-25T10:34:51.893125: step 6351, loss 0.00203992, acc 1\n",
      "2022-03-25T10:34:52.041437: step 6352, loss 0.0188864, acc 0.984375\n",
      "2022-03-25T10:34:52.188272: step 6353, loss 0.00107558, acc 1\n",
      "2022-03-25T10:34:52.340077: step 6354, loss 0.000178591, acc 1\n",
      "2022-03-25T10:34:52.490547: step 6355, loss 0.000451759, acc 1\n",
      "2022-03-25T10:34:52.650202: step 6356, loss 0.000411599, acc 1\n",
      "2022-03-25T10:34:52.797359: step 6357, loss 0.000148665, acc 1\n",
      "2022-03-25T10:34:52.959160: step 6358, loss 0.000277815, acc 1\n",
      "2022-03-25T10:34:53.113493: step 6359, loss 0.0102509, acc 1\n",
      "2022-03-25T10:34:53.271879: step 6360, loss 0.00190805, acc 1\n",
      "2022-03-25T10:34:53.431507: step 6361, loss 0.00448203, acc 1\n",
      "2022-03-25T10:34:53.588326: step 6362, loss 5.88133e-05, acc 1\n",
      "2022-03-25T10:34:53.738666: step 6363, loss 0.000507283, acc 1\n",
      "2022-03-25T10:34:53.893804: step 6364, loss 0.00180042, acc 1\n",
      "2022-03-25T10:34:54.058782: step 6365, loss 0.000446062, acc 1\n",
      "2022-03-25T10:34:54.207006: step 6366, loss 0.000151267, acc 1\n",
      "2022-03-25T10:34:54.353992: step 6367, loss 0.000121961, acc 1\n",
      "2022-03-25T10:34:54.508699: step 6368, loss 0.0373495, acc 0.984375\n",
      "2022-03-25T10:34:54.675298: step 6369, loss 0.0039861, acc 1\n",
      "2022-03-25T10:34:54.823975: step 6370, loss 4.95861e-05, acc 1\n",
      "2022-03-25T10:34:54.989819: step 6371, loss 0.000422588, acc 1\n",
      "2022-03-25T10:34:55.133326: step 6372, loss 9.24666e-05, acc 1\n",
      "2022-03-25T10:34:55.285443: step 6373, loss 0.00018866, acc 1\n",
      "2022-03-25T10:34:55.440465: step 6374, loss 0.000981139, acc 1\n",
      "2022-03-25T10:34:55.594791: step 6375, loss 0.000659965, acc 1\n",
      "2022-03-25T10:34:55.748013: step 6376, loss 0.000255563, acc 1\n",
      "2022-03-25T10:34:55.905703: step 6377, loss 7.18486e-05, acc 1\n",
      "2022-03-25T10:34:56.062207: step 6378, loss 0.000464933, acc 1\n",
      "2022-03-25T10:34:56.220321: step 6379, loss 1.66686e-05, acc 1\n",
      "2022-03-25T10:34:56.377751: step 6380, loss 6.68375e-05, acc 1\n",
      "2022-03-25T10:34:56.537402: step 6381, loss 1.48705e-05, acc 1\n",
      "2022-03-25T10:34:56.689448: step 6382, loss 0.000411401, acc 1\n",
      "2022-03-25T10:34:56.847287: step 6383, loss 0.0014521, acc 1\n",
      "2022-03-25T10:34:56.999904: step 6384, loss 0.00220611, acc 1\n",
      "2022-03-25T10:34:57.164113: step 6385, loss 1.20518e-05, acc 1\n",
      "2022-03-25T10:34:57.317174: step 6386, loss 0.0363499, acc 0.984375\n",
      "2022-03-25T10:34:57.474971: step 6387, loss 0.006391, acc 1\n",
      "2022-03-25T10:34:57.634116: step 6388, loss 0.00197634, acc 1\n",
      "2022-03-25T10:34:57.786593: step 6389, loss 0.00639922, acc 1\n",
      "2022-03-25T10:34:57.937262: step 6390, loss 0.00017848, acc 1\n",
      "2022-03-25T10:34:58.086543: step 6391, loss 0.000279961, acc 1\n",
      "2022-03-25T10:34:58.237405: step 6392, loss 0.00249398, acc 1\n",
      "2022-03-25T10:34:58.392270: step 6393, loss 0.000643362, acc 1\n",
      "2022-03-25T10:34:58.550227: step 6394, loss 0.000470502, acc 1\n",
      "2022-03-25T10:34:58.699904: step 6395, loss 0.000419268, acc 1\n",
      "2022-03-25T10:34:58.843479: step 6396, loss 0.000201073, acc 1\n",
      "2022-03-25T10:34:59.000952: step 6397, loss 0.000260486, acc 1\n",
      "2022-03-25T10:34:59.163664: step 6398, loss 0.00161348, acc 1\n",
      "2022-03-25T10:34:59.318577: step 6399, loss 9.67379e-05, acc 1\n",
      "2022-03-25T10:34:59.478811: step 6400, loss 0.000307516, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:34:59.628061: step 6400, loss 0.528634, acc 0.914244\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6400\n",
      "\n",
      "2022-03-25T10:34:59.878477: step 6401, loss 0.0100583, acc 1\n",
      "2022-03-25T10:34:59.990019: step 6402, loss 0.000497116, acc 1\n",
      "2022-03-25T10:35:00.148684: step 6403, loss 0.00263512, acc 1\n",
      "2022-03-25T10:35:00.297804: step 6404, loss 0.00088741, acc 1\n",
      "2022-03-25T10:35:00.458905: step 6405, loss 0.00507281, acc 1\n",
      "2022-03-25T10:35:00.609250: step 6406, loss 0.000733899, acc 1\n",
      "2022-03-25T10:35:00.766494: step 6407, loss 0.000711592, acc 1\n",
      "2022-03-25T10:35:00.918906: step 6408, loss 0.00371349, acc 1\n",
      "2022-03-25T10:35:01.075034: step 6409, loss 0.00497141, acc 1\n",
      "2022-03-25T10:35:01.225902: step 6410, loss 0.000608202, acc 1\n",
      "2022-03-25T10:35:01.380590: step 6411, loss 0.000852484, acc 1\n",
      "2022-03-25T10:35:01.526230: step 6412, loss 7.11469e-05, acc 1\n",
      "2022-03-25T10:35:01.678763: step 6413, loss 0.000539138, acc 1\n",
      "2022-03-25T10:35:01.832985: step 6414, loss 0.000417879, acc 1\n",
      "2022-03-25T10:35:01.997946: step 6415, loss 0.000754976, acc 1\n",
      "2022-03-25T10:35:02.159489: step 6416, loss 0.00309255, acc 1\n",
      "2022-03-25T10:35:02.305406: step 6417, loss 0.000709482, acc 1\n",
      "2022-03-25T10:35:02.458456: step 6418, loss 0.000530859, acc 1\n",
      "2022-03-25T10:35:02.611458: step 6419, loss 0.00177752, acc 1\n",
      "2022-03-25T10:35:02.766456: step 6420, loss 0.0139, acc 0.984375\n",
      "2022-03-25T10:35:02.925562: step 6421, loss 0.00171107, acc 1\n",
      "2022-03-25T10:35:03.082846: step 6422, loss 7.9234e-05, acc 1\n",
      "2022-03-25T10:35:03.242563: step 6423, loss 0.000557249, acc 1\n",
      "2022-03-25T10:35:03.407220: step 6424, loss 0.0236361, acc 0.984375\n",
      "2022-03-25T10:35:03.559673: step 6425, loss 0.000184887, acc 1\n",
      "2022-03-25T10:35:03.715834: step 6426, loss 0.00131119, acc 1\n",
      "2022-03-25T10:35:03.873548: step 6427, loss 3.04102e-05, acc 1\n",
      "2022-03-25T10:35:04.023442: step 6428, loss 0.00048845, acc 1\n",
      "2022-03-25T10:35:04.187620: step 6429, loss 0.000432922, acc 1\n",
      "2022-03-25T10:35:04.342539: step 6430, loss 0.000517303, acc 1\n",
      "2022-03-25T10:35:04.504164: step 6431, loss 0.000640338, acc 1\n",
      "2022-03-25T10:35:04.651395: step 6432, loss 7.7818e-05, acc 1\n",
      "2022-03-25T10:35:04.809326: step 6433, loss 0.00127502, acc 1\n",
      "2022-03-25T10:35:04.972498: step 6434, loss 0.000254585, acc 1\n",
      "2022-03-25T10:35:05.125775: step 6435, loss 9.57759e-05, acc 1\n",
      "2022-03-25T10:35:05.278380: step 6436, loss 0.000254412, acc 1\n",
      "2022-03-25T10:35:05.430723: step 6437, loss 0.00161459, acc 1\n",
      "2022-03-25T10:35:05.582702: step 6438, loss 5.91732e-05, acc 1\n",
      "2022-03-25T10:35:05.736755: step 6439, loss 0.00117853, acc 1\n",
      "2022-03-25T10:35:05.889851: step 6440, loss 0.00341852, acc 1\n",
      "2022-03-25T10:35:06.044778: step 6441, loss 0.000296372, acc 1\n",
      "2022-03-25T10:35:06.198717: step 6442, loss 0.000255519, acc 1\n",
      "2022-03-25T10:35:06.351781: step 6443, loss 0.00135905, acc 1\n",
      "2022-03-25T10:35:06.499041: step 6444, loss 0.00370305, acc 1\n",
      "2022-03-25T10:35:06.647826: step 6445, loss 0.000598162, acc 1\n",
      "2022-03-25T10:35:06.803663: step 6446, loss 0.0008764, acc 1\n",
      "2022-03-25T10:35:06.967418: step 6447, loss 0.000591495, acc 1\n",
      "2022-03-25T10:35:07.119522: step 6448, loss 0.000847636, acc 1\n",
      "2022-03-25T10:35:07.278565: step 6449, loss 0.000453229, acc 1\n",
      "2022-03-25T10:35:07.433963: step 6450, loss 0.00020336, acc 1\n",
      "2022-03-25T10:35:07.581745: step 6451, loss 0.001058, acc 1\n",
      "2022-03-25T10:35:07.734344: step 6452, loss 0.000544143, acc 1\n",
      "2022-03-25T10:35:07.885157: step 6453, loss 0.000107011, acc 1\n",
      "2022-03-25T10:35:08.029265: step 6454, loss 0.00149166, acc 1\n",
      "2022-03-25T10:35:08.183560: step 6455, loss 0.000322661, acc 1\n",
      "2022-03-25T10:35:08.335186: step 6456, loss 0.000108501, acc 1\n",
      "2022-03-25T10:35:08.479927: step 6457, loss 3.15147e-05, acc 1\n",
      "2022-03-25T10:35:08.633242: step 6458, loss 4.92248e-05, acc 1\n",
      "2022-03-25T10:35:08.777840: step 6459, loss 0.000311979, acc 1\n",
      "2022-03-25T10:35:08.933267: step 6460, loss 0.000224194, acc 1\n",
      "2022-03-25T10:35:09.091857: step 6461, loss 0.00448097, acc 1\n",
      "2022-03-25T10:35:09.244412: step 6462, loss 0.000969055, acc 1\n",
      "2022-03-25T10:35:09.397609: step 6463, loss 0.000224953, acc 1\n",
      "2022-03-25T10:35:09.546010: step 6464, loss 0.00052958, acc 1\n",
      "2022-03-25T10:35:09.694404: step 6465, loss 0.00069202, acc 1\n",
      "2022-03-25T10:35:09.849619: step 6466, loss 0.00268003, acc 1\n",
      "2022-03-25T10:35:10.003820: step 6467, loss 0.00194086, acc 1\n",
      "2022-03-25T10:35:10.160970: step 6468, loss 0.000628255, acc 1\n",
      "2022-03-25T10:35:10.323241: step 6469, loss 5.81039e-05, acc 1\n",
      "2022-03-25T10:35:10.475851: step 6470, loss 0.000515491, acc 1\n",
      "2022-03-25T10:35:10.624235: step 6471, loss 0.000172516, acc 1\n",
      "2022-03-25T10:35:10.766381: step 6472, loss 0.000410021, acc 1\n",
      "2022-03-25T10:35:10.914638: step 6473, loss 0.0030039, acc 1\n",
      "2022-03-25T10:35:11.070760: step 6474, loss 0.000327054, acc 1\n",
      "2022-03-25T10:35:11.218987: step 6475, loss 0.0773331, acc 0.984375\n",
      "2022-03-25T10:35:11.369158: step 6476, loss 8.99746e-05, acc 1\n",
      "2022-03-25T10:35:11.517318: step 6477, loss 0.000117317, acc 1\n",
      "2022-03-25T10:35:11.670464: step 6478, loss 0.0013103, acc 1\n",
      "2022-03-25T10:35:11.824935: step 6479, loss 0.00125951, acc 1\n",
      "2022-03-25T10:35:11.977015: step 6480, loss 0.0082025, acc 1\n",
      "2022-03-25T10:35:12.129956: step 6481, loss 0.000118431, acc 1\n",
      "2022-03-25T10:35:12.278035: step 6482, loss 0.0146258, acc 0.984375\n",
      "2022-03-25T10:35:12.433056: step 6483, loss 0.000138649, acc 1\n",
      "2022-03-25T10:35:12.589578: step 6484, loss 0.000272205, acc 1\n",
      "2022-03-25T10:35:12.739712: step 6485, loss 0.00275298, acc 1\n",
      "2022-03-25T10:35:12.896271: step 6486, loss 0.00176844, acc 1\n",
      "2022-03-25T10:35:13.040795: step 6487, loss 0.000404594, acc 1\n",
      "2022-03-25T10:35:13.191945: step 6488, loss 0.00169019, acc 1\n",
      "2022-03-25T10:35:13.348866: step 6489, loss 0.00196493, acc 1\n",
      "2022-03-25T10:35:13.498182: step 6490, loss 0.00158834, acc 1\n",
      "2022-03-25T10:35:13.654741: step 6491, loss 0.000456255, acc 1\n",
      "2022-03-25T10:35:13.801851: step 6492, loss 4.79971e-05, acc 1\n",
      "2022-03-25T10:35:13.951723: step 6493, loss 0.000409949, acc 1\n",
      "2022-03-25T10:35:14.103681: step 6494, loss 0.000540044, acc 1\n",
      "2022-03-25T10:35:14.253256: step 6495, loss 0.000200796, acc 1\n",
      "2022-03-25T10:35:14.408195: step 6496, loss 0.00062373, acc 1\n",
      "2022-03-25T10:35:14.560533: step 6497, loss 5.72183e-05, acc 1\n",
      "2022-03-25T10:35:14.705358: step 6498, loss 0.000457692, acc 1\n",
      "2022-03-25T10:35:14.861250: step 6499, loss 0.000923614, acc 1\n",
      "2022-03-25T10:35:15.016169: step 6500, loss 0.000907994, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:35:15.170397: step 6500, loss 0.574123, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6500\n",
      "\n",
      "2022-03-25T10:35:15.427595: step 6501, loss 6.39643e-05, acc 1\n",
      "2022-03-25T10:35:15.580706: step 6502, loss 0.000258155, acc 1\n",
      "2022-03-25T10:35:15.739689: step 6503, loss 0.000115916, acc 1\n",
      "2022-03-25T10:35:15.903987: step 6504, loss 0.000374388, acc 1\n",
      "2022-03-25T10:35:16.060236: step 6505, loss 0.000142626, acc 1\n",
      "2022-03-25T10:35:16.210055: step 6506, loss 0.000671901, acc 1\n",
      "2022-03-25T10:35:16.364265: step 6507, loss 6.3243e-05, acc 1\n",
      "2022-03-25T10:35:16.519280: step 6508, loss 0.000128795, acc 1\n",
      "2022-03-25T10:35:16.670663: step 6509, loss 0.000315417, acc 1\n",
      "2022-03-25T10:35:16.818148: step 6510, loss 0.00661308, acc 1\n",
      "2022-03-25T10:35:16.965174: step 6511, loss 0.000165933, acc 1\n",
      "2022-03-25T10:35:17.131090: step 6512, loss 0.000174435, acc 1\n",
      "2022-03-25T10:35:17.280252: step 6513, loss 0.000383067, acc 1\n",
      "2022-03-25T10:35:17.429975: step 6514, loss 9.69704e-05, acc 1\n",
      "2022-03-25T10:35:17.584051: step 6515, loss 0.00131311, acc 1\n",
      "2022-03-25T10:35:17.743520: step 6516, loss 9.60923e-05, acc 1\n",
      "2022-03-25T10:35:17.896144: step 6517, loss 0.000316396, acc 1\n",
      "2022-03-25T10:35:18.046497: step 6518, loss 0.000238432, acc 1\n",
      "2022-03-25T10:35:18.202136: step 6519, loss 0.000988284, acc 1\n",
      "2022-03-25T10:35:18.347237: step 6520, loss 0.00198781, acc 1\n",
      "2022-03-25T10:35:18.508208: step 6521, loss 0.000464357, acc 1\n",
      "2022-03-25T10:35:18.663368: step 6522, loss 0.00189239, acc 1\n",
      "2022-03-25T10:35:18.821157: step 6523, loss 0.000924602, acc 1\n",
      "2022-03-25T10:35:18.977534: step 6524, loss 0.00018233, acc 1\n",
      "2022-03-25T10:35:19.128012: step 6525, loss 0.000674772, acc 1\n",
      "2022-03-25T10:35:19.284675: step 6526, loss 0.000450788, acc 1\n",
      "2022-03-25T10:35:19.438640: step 6527, loss 0.00355532, acc 1\n",
      "2022-03-25T10:35:19.597714: step 6528, loss 0.000504408, acc 1\n",
      "2022-03-25T10:35:19.753220: step 6529, loss 0.000159179, acc 1\n",
      "2022-03-25T10:35:19.905267: step 6530, loss 0.000143964, acc 1\n",
      "2022-03-25T10:35:20.057864: step 6531, loss 0.00124362, acc 1\n",
      "2022-03-25T10:35:20.211273: step 6532, loss 0.00116469, acc 1\n",
      "2022-03-25T10:35:20.359886: step 6533, loss 8.05465e-05, acc 1\n",
      "2022-03-25T10:35:20.515221: step 6534, loss 0.00107976, acc 1\n",
      "2022-03-25T10:35:20.672787: step 6535, loss 0.000229855, acc 1\n",
      "2022-03-25T10:35:20.827208: step 6536, loss 0.0360505, acc 0.984375\n",
      "2022-03-25T10:35:20.979374: step 6537, loss 0.000122244, acc 1\n",
      "2022-03-25T10:35:21.137705: step 6538, loss 0.00486508, acc 1\n",
      "2022-03-25T10:35:21.294231: step 6539, loss 0.000396597, acc 1\n",
      "2022-03-25T10:35:21.439976: step 6540, loss 0.000214519, acc 1\n",
      "2022-03-25T10:35:21.589261: step 6541, loss 0.000315698, acc 1\n",
      "2022-03-25T10:35:21.742400: step 6542, loss 0.0119838, acc 0.984375\n",
      "2022-03-25T10:35:21.894926: step 6543, loss 0.00167873, acc 1\n",
      "2022-03-25T10:35:22.038795: step 6544, loss 0.000350742, acc 1\n",
      "2022-03-25T10:35:22.186066: step 6545, loss 0.000596807, acc 1\n",
      "2022-03-25T10:35:22.345265: step 6546, loss 0.000143365, acc 1\n",
      "2022-03-25T10:35:22.493779: step 6547, loss 0.000209505, acc 1\n",
      "2022-03-25T10:35:22.646206: step 6548, loss 0.000583003, acc 1\n",
      "2022-03-25T10:35:22.798412: step 6549, loss 0.000150969, acc 1\n",
      "2022-03-25T10:35:22.942610: step 6550, loss 0.00010621, acc 1\n",
      "2022-03-25T10:35:23.089742: step 6551, loss 0.00372529, acc 1\n",
      "2022-03-25T10:35:23.241839: step 6552, loss 0.00211811, acc 1\n",
      "2022-03-25T10:35:23.397456: step 6553, loss 0.000385261, acc 1\n",
      "2022-03-25T10:35:23.550038: step 6554, loss 0.00359309, acc 1\n",
      "2022-03-25T10:35:23.709625: step 6555, loss 4.49373e-05, acc 1\n",
      "2022-03-25T10:35:23.854637: step 6556, loss 0.000877791, acc 1\n",
      "2022-03-25T10:35:24.008474: step 6557, loss 0.000315469, acc 1\n",
      "2022-03-25T10:35:24.162569: step 6558, loss 0.000322304, acc 1\n",
      "2022-03-25T10:35:24.316432: step 6559, loss 0.00377724, acc 1\n",
      "2022-03-25T10:35:24.470536: step 6560, loss 0.000283096, acc 1\n",
      "2022-03-25T10:35:24.637064: step 6561, loss 0.000101396, acc 1\n",
      "2022-03-25T10:35:24.788293: step 6562, loss 0.00159795, acc 1\n",
      "2022-03-25T10:35:24.937042: step 6563, loss 0.000258559, acc 1\n",
      "2022-03-25T10:35:25.082297: step 6564, loss 8.11766e-05, acc 1\n",
      "2022-03-25T10:35:25.240236: step 6565, loss 9.15784e-05, acc 1\n",
      "2022-03-25T10:35:25.402793: step 6566, loss 0.000899343, acc 1\n",
      "2022-03-25T10:35:25.560732: step 6567, loss 0.0029575, acc 1\n",
      "2022-03-25T10:35:25.723980: step 6568, loss 0.000112373, acc 1\n",
      "2022-03-25T10:35:25.877165: step 6569, loss 5.25123e-05, acc 1\n",
      "2022-03-25T10:35:26.025741: step 6570, loss 0.00022804, acc 1\n",
      "2022-03-25T10:35:26.177162: step 6571, loss 0.00199936, acc 1\n",
      "2022-03-25T10:35:26.322870: step 6572, loss 3.04251e-05, acc 1\n",
      "2022-03-25T10:35:26.468338: step 6573, loss 0.00379542, acc 1\n",
      "2022-03-25T10:35:26.626250: step 6574, loss 0.000628867, acc 1\n",
      "2022-03-25T10:35:26.783474: step 6575, loss 0.000110814, acc 1\n",
      "2022-03-25T10:35:26.942220: step 6576, loss 5.64108e-05, acc 1\n",
      "2022-03-25T10:35:27.085696: step 6577, loss 0.00087955, acc 1\n",
      "2022-03-25T10:35:27.243892: step 6578, loss 0.00102618, acc 1\n",
      "2022-03-25T10:35:27.404747: step 6579, loss 0.000119917, acc 1\n",
      "2022-03-25T10:35:27.559479: step 6580, loss 9.33447e-05, acc 1\n",
      "2022-03-25T10:35:27.716101: step 6581, loss 0.004932, acc 1\n",
      "2022-03-25T10:35:27.871461: step 6582, loss 0.000870154, acc 1\n",
      "2022-03-25T10:35:28.020671: step 6583, loss 6.57447e-05, acc 1\n",
      "2022-03-25T10:35:28.166863: step 6584, loss 0.000110512, acc 1\n",
      "2022-03-25T10:35:28.309314: step 6585, loss 0.0007306, acc 1\n",
      "2022-03-25T10:35:28.461153: step 6586, loss 0.000731916, acc 1\n",
      "2022-03-25T10:35:28.609691: step 6587, loss 0.00047999, acc 1\n",
      "2022-03-25T10:35:28.763693: step 6588, loss 0.000735075, acc 1\n",
      "2022-03-25T10:35:28.923714: step 6589, loss 0.00625535, acc 1\n",
      "2022-03-25T10:35:29.075962: step 6590, loss 0.03387, acc 0.984375\n",
      "2022-03-25T10:35:29.231709: step 6591, loss 0.000111278, acc 1\n",
      "2022-03-25T10:35:29.381439: step 6592, loss 9.6033e-05, acc 1\n",
      "2022-03-25T10:35:29.531549: step 6593, loss 0.00061179, acc 1\n",
      "2022-03-25T10:35:29.698124: step 6594, loss 8.46444e-05, acc 1\n",
      "2022-03-25T10:35:29.846784: step 6595, loss 0.000387614, acc 1\n",
      "2022-03-25T10:35:29.967991: step 6596, loss 0.00068384, acc 1\n",
      "2022-03-25T10:35:30.123106: step 6597, loss 0.000857957, acc 1\n",
      "2022-03-25T10:35:30.274457: step 6598, loss 0.000846162, acc 1\n",
      "2022-03-25T10:35:30.432178: step 6599, loss 0.000219641, acc 1\n",
      "2022-03-25T10:35:30.581689: step 6600, loss 0.000523923, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:35:30.730448: step 6600, loss 0.557448, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6600\n",
      "\n",
      "2022-03-25T10:35:30.991011: step 6601, loss 0.000203247, acc 1\n",
      "2022-03-25T10:35:31.135493: step 6602, loss 0.000487242, acc 1\n",
      "2022-03-25T10:35:31.286692: step 6603, loss 0.000606874, acc 1\n",
      "2022-03-25T10:35:31.432677: step 6604, loss 0.000951242, acc 1\n",
      "2022-03-25T10:35:31.582031: step 6605, loss 9.16905e-05, acc 1\n",
      "2022-03-25T10:35:31.741334: step 6606, loss 0.00101471, acc 1\n",
      "2022-03-25T10:35:31.891504: step 6607, loss 0.000572301, acc 1\n",
      "2022-03-25T10:35:32.030670: step 6608, loss 0.00184289, acc 1\n",
      "2022-03-25T10:35:32.174010: step 6609, loss 0.000488726, acc 1\n",
      "2022-03-25T10:35:32.319136: step 6610, loss 0.00227705, acc 1\n",
      "2022-03-25T10:35:32.479366: step 6611, loss 0.000876713, acc 1\n",
      "2022-03-25T10:35:32.622257: step 6612, loss 0.0094361, acc 1\n",
      "2022-03-25T10:35:32.777519: step 6613, loss 0.00260618, acc 1\n",
      "2022-03-25T10:35:32.921915: step 6614, loss 0.000476341, acc 1\n",
      "2022-03-25T10:35:33.072455: step 6615, loss 0.00145148, acc 1\n",
      "2022-03-25T10:35:33.217086: step 6616, loss 0.00025188, acc 1\n",
      "2022-03-25T10:35:33.359861: step 6617, loss 7.41293e-05, acc 1\n",
      "2022-03-25T10:35:33.505822: step 6618, loss 0.00152262, acc 1\n",
      "2022-03-25T10:35:33.649218: step 6619, loss 7.45612e-05, acc 1\n",
      "2022-03-25T10:35:33.812913: step 6620, loss 0.000419452, acc 1\n",
      "2022-03-25T10:35:33.968680: step 6621, loss 0.00512084, acc 1\n",
      "2022-03-25T10:35:34.118589: step 6622, loss 0.000200668, acc 1\n",
      "2022-03-25T10:35:34.270304: step 6623, loss 1.90459e-05, acc 1\n",
      "2022-03-25T10:35:34.414193: step 6624, loss 5.15742e-05, acc 1\n",
      "2022-03-25T10:35:34.561656: step 6625, loss 0.00149858, acc 1\n",
      "2022-03-25T10:35:34.714383: step 6626, loss 1.34379e-05, acc 1\n",
      "2022-03-25T10:35:34.872463: step 6627, loss 0.000280571, acc 1\n",
      "2022-03-25T10:35:35.019925: step 6628, loss 3.15947e-05, acc 1\n",
      "2022-03-25T10:35:35.172172: step 6629, loss 3.43594e-05, acc 1\n",
      "2022-03-25T10:35:35.314699: step 6630, loss 1.91851e-06, acc 1\n",
      "2022-03-25T10:35:35.461637: step 6631, loss 8.61586e-05, acc 1\n",
      "2022-03-25T10:35:35.612695: step 6632, loss 1.91194e-05, acc 1\n",
      "2022-03-25T10:35:35.770557: step 6633, loss 2.91053e-05, acc 1\n",
      "2022-03-25T10:35:35.922457: step 6634, loss 0.00267236, acc 1\n",
      "2022-03-25T10:35:36.082266: step 6635, loss 0.000649104, acc 1\n",
      "2022-03-25T10:35:36.241267: step 6636, loss 0.000231048, acc 1\n",
      "2022-03-25T10:35:36.396662: step 6637, loss 0.000379404, acc 1\n",
      "2022-03-25T10:35:36.550427: step 6638, loss 8.45808e-05, acc 1\n",
      "2022-03-25T10:35:36.706753: step 6639, loss 0.000222085, acc 1\n",
      "2022-03-25T10:35:36.870173: step 6640, loss 0.000246942, acc 1\n",
      "2022-03-25T10:35:37.016557: step 6641, loss 0.00489538, acc 1\n",
      "2022-03-25T10:35:37.169256: step 6642, loss 0.000167462, acc 1\n",
      "2022-03-25T10:35:37.328273: step 6643, loss 9.04266e-06, acc 1\n",
      "2022-03-25T10:35:37.477442: step 6644, loss 1.14648e-05, acc 1\n",
      "2022-03-25T10:35:37.635420: step 6645, loss 0.00102624, acc 1\n",
      "2022-03-25T10:35:37.796207: step 6646, loss 0.000123115, acc 1\n",
      "2022-03-25T10:35:37.953641: step 6647, loss 0.000151099, acc 1\n",
      "2022-03-25T10:35:38.105498: step 6648, loss 0.000109542, acc 1\n",
      "2022-03-25T10:35:38.254488: step 6649, loss 0.00177819, acc 1\n",
      "2022-03-25T10:35:38.409497: step 6650, loss 0.00115777, acc 1\n",
      "2022-03-25T10:35:38.557835: step 6651, loss 0.000238242, acc 1\n",
      "2022-03-25T10:35:38.706610: step 6652, loss 0.000148749, acc 1\n",
      "2022-03-25T10:35:38.855698: step 6653, loss 0.00163475, acc 1\n",
      "2022-03-25T10:35:38.999340: step 6654, loss 7.43258e-05, acc 1\n",
      "2022-03-25T10:35:39.148652: step 6655, loss 0.000569208, acc 1\n",
      "2022-03-25T10:35:39.296148: step 6656, loss 9.19362e-05, acc 1\n",
      "2022-03-25T10:35:39.454865: step 6657, loss 0.00138941, acc 1\n",
      "2022-03-25T10:35:39.613221: step 6658, loss 0.00376734, acc 1\n",
      "2022-03-25T10:35:39.767440: step 6659, loss 9.62034e-05, acc 1\n",
      "2022-03-25T10:35:39.916800: step 6660, loss 0.0142302, acc 0.984375\n",
      "2022-03-25T10:35:40.071525: step 6661, loss 6.47191e-05, acc 1\n",
      "2022-03-25T10:35:40.220891: step 6662, loss 0.000125771, acc 1\n",
      "2022-03-25T10:35:40.368511: step 6663, loss 0.000562113, acc 1\n",
      "2022-03-25T10:35:40.522871: step 6664, loss 0.000311232, acc 1\n",
      "2022-03-25T10:35:40.681445: step 6665, loss 0.000683152, acc 1\n",
      "2022-03-25T10:35:40.840015: step 6666, loss 0.000366474, acc 1\n",
      "2022-03-25T10:35:41.007154: step 6667, loss 0.0114709, acc 1\n",
      "2022-03-25T10:35:41.158459: step 6668, loss 0.00678724, acc 1\n",
      "2022-03-25T10:35:41.314134: step 6669, loss 0.000121498, acc 1\n",
      "2022-03-25T10:35:41.464312: step 6670, loss 0.000640231, acc 1\n",
      "2022-03-25T10:35:41.614776: step 6671, loss 0.000236982, acc 1\n",
      "2022-03-25T10:35:41.771700: step 6672, loss 0.000379792, acc 1\n",
      "2022-03-25T10:35:41.918842: step 6673, loss 0.00433191, acc 1\n",
      "2022-03-25T10:35:42.070624: step 6674, loss 0.000134932, acc 1\n",
      "2022-03-25T10:35:42.218078: step 6675, loss 0.000393291, acc 1\n",
      "2022-03-25T10:35:42.360878: step 6676, loss 0.0044945, acc 1\n",
      "2022-03-25T10:35:42.507903: step 6677, loss 0.00410823, acc 1\n",
      "2022-03-25T10:35:42.655616: step 6678, loss 0.000173789, acc 1\n",
      "2022-03-25T10:35:42.819166: step 6679, loss 4.96026e-05, acc 1\n",
      "2022-03-25T10:35:42.973065: step 6680, loss 0.00597542, acc 1\n",
      "2022-03-25T10:35:43.120487: step 6681, loss 0.000465792, acc 1\n",
      "2022-03-25T10:35:43.266789: step 6682, loss 0.000593862, acc 1\n",
      "2022-03-25T10:35:43.414928: step 6683, loss 0.000583691, acc 1\n",
      "2022-03-25T10:35:43.563910: step 6684, loss 0.000854806, acc 1\n",
      "2022-03-25T10:35:43.708766: step 6685, loss 0.000822022, acc 1\n",
      "2022-03-25T10:35:43.865334: step 6686, loss 0.000945214, acc 1\n",
      "2022-03-25T10:35:44.016497: step 6687, loss 0.000110705, acc 1\n",
      "2022-03-25T10:35:44.174546: step 6688, loss 0.000368831, acc 1\n",
      "2022-03-25T10:35:44.331119: step 6689, loss 0.00107137, acc 1\n",
      "2022-03-25T10:35:44.475781: step 6690, loss 0.000690236, acc 1\n",
      "2022-03-25T10:35:44.630250: step 6691, loss 0.000327013, acc 1\n",
      "2022-03-25T10:35:44.785017: step 6692, loss 0.00818288, acc 1\n",
      "2022-03-25T10:35:44.939234: step 6693, loss 0.000213378, acc 1\n",
      "2022-03-25T10:35:45.105702: step 6694, loss 0.000447109, acc 1\n",
      "2022-03-25T10:35:45.258688: step 6695, loss 0.00125003, acc 1\n",
      "2022-03-25T10:35:45.407754: step 6696, loss 0.000155645, acc 1\n",
      "2022-03-25T10:35:45.558222: step 6697, loss 6.77355e-05, acc 1\n",
      "2022-03-25T10:35:45.703206: step 6698, loss 0.000156236, acc 1\n",
      "2022-03-25T10:35:45.849695: step 6699, loss 6.8186e-05, acc 1\n",
      "2022-03-25T10:35:46.001545: step 6700, loss 0.000604987, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:35:46.152713: step 6700, loss 0.611134, acc 0.917151\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6700\n",
      "\n",
      "2022-03-25T10:35:46.392888: step 6701, loss 0.000100993, acc 1\n",
      "2022-03-25T10:35:46.542275: step 6702, loss 0.00299837, acc 1\n",
      "2022-03-25T10:35:46.693521: step 6703, loss 0.000886573, acc 1\n",
      "2022-03-25T10:35:46.850405: step 6704, loss 0.000134899, acc 1\n",
      "2022-03-25T10:35:46.998579: step 6705, loss 0.000177851, acc 1\n",
      "2022-03-25T10:35:47.153775: step 6706, loss 0.00221801, acc 1\n",
      "2022-03-25T10:35:47.300932: step 6707, loss 4.05767e-05, acc 1\n",
      "2022-03-25T10:35:47.450166: step 6708, loss 0.000601865, acc 1\n",
      "2022-03-25T10:35:47.594939: step 6709, loss 0.000204254, acc 1\n",
      "2022-03-25T10:35:47.741786: step 6710, loss 0.00502001, acc 1\n",
      "2022-03-25T10:35:47.898822: step 6711, loss 0.000691942, acc 1\n",
      "2022-03-25T10:35:48.043675: step 6712, loss 2.39178e-05, acc 1\n",
      "2022-03-25T10:35:48.194806: step 6713, loss 2.78465e-05, acc 1\n",
      "2022-03-25T10:35:48.333127: step 6714, loss 0.000179844, acc 1\n",
      "2022-03-25T10:35:48.473798: step 6715, loss 0.000149537, acc 1\n",
      "2022-03-25T10:35:48.632558: step 6716, loss 0.00178244, acc 1\n",
      "2022-03-25T10:35:48.771217: step 6717, loss 0.000121471, acc 1\n",
      "2022-03-25T10:35:48.926751: step 6718, loss 8.01812e-05, acc 1\n",
      "2022-03-25T10:35:49.066724: step 6719, loss 0.000207659, acc 1\n",
      "2022-03-25T10:35:49.228253: step 6720, loss 6.50836e-05, acc 1\n",
      "2022-03-25T10:35:49.380540: step 6721, loss 0.00203224, acc 1\n",
      "2022-03-25T10:35:49.520535: step 6722, loss 0.000588878, acc 1\n",
      "2022-03-25T10:35:49.660989: step 6723, loss 0.00117425, acc 1\n",
      "2022-03-25T10:35:49.809662: step 6724, loss 0.000167781, acc 1\n",
      "2022-03-25T10:35:49.946956: step 6725, loss 0.000255564, acc 1\n",
      "2022-03-25T10:35:50.091597: step 6726, loss 0.000538287, acc 1\n",
      "2022-03-25T10:35:50.239843: step 6727, loss 0.000199641, acc 1\n",
      "2022-03-25T10:35:50.384400: step 6728, loss 0.00437676, acc 1\n",
      "2022-03-25T10:35:50.523668: step 6729, loss 0.00015986, acc 1\n",
      "2022-03-25T10:35:50.680373: step 6730, loss 0.00131032, acc 1\n",
      "2022-03-25T10:35:50.837041: step 6731, loss 0.000436236, acc 1\n",
      "2022-03-25T10:35:50.985204: step 6732, loss 0.00939514, acc 1\n",
      "2022-03-25T10:35:51.125495: step 6733, loss 0.000468317, acc 1\n",
      "2022-03-25T10:35:51.287174: step 6734, loss 0.000207277, acc 1\n",
      "2022-03-25T10:35:51.443350: step 6735, loss 9.64706e-05, acc 1\n",
      "2022-03-25T10:35:51.587822: step 6736, loss 0.000308902, acc 1\n",
      "2022-03-25T10:35:51.733595: step 6737, loss 0.000250626, acc 1\n",
      "2022-03-25T10:35:51.877605: step 6738, loss 0.000756711, acc 1\n",
      "2022-03-25T10:35:52.023986: step 6739, loss 0.000524253, acc 1\n",
      "2022-03-25T10:35:52.171864: step 6740, loss 0.00149136, acc 1\n",
      "2022-03-25T10:35:52.320436: step 6741, loss 0.000215945, acc 1\n",
      "2022-03-25T10:35:52.463113: step 6742, loss 0.000655953, acc 1\n",
      "2022-03-25T10:35:52.604205: step 6743, loss 0.000662598, acc 1\n",
      "2022-03-25T10:35:52.749378: step 6744, loss 0.000276385, acc 1\n",
      "2022-03-25T10:35:52.899882: step 6745, loss 0.000850321, acc 1\n",
      "2022-03-25T10:35:53.048497: step 6746, loss 0.00239816, acc 1\n",
      "2022-03-25T10:35:53.194176: step 6747, loss 0.000276712, acc 1\n",
      "2022-03-25T10:35:53.348161: step 6748, loss 0.000299011, acc 1\n",
      "2022-03-25T10:35:53.489292: step 6749, loss 0.000266144, acc 1\n",
      "2022-03-25T10:35:53.645585: step 6750, loss 0.00025432, acc 1\n",
      "2022-03-25T10:35:53.793621: step 6751, loss 2.3962e-05, acc 1\n",
      "2022-03-25T10:35:53.941782: step 6752, loss 0.0458889, acc 0.984375\n",
      "2022-03-25T10:35:54.090471: step 6753, loss 3.58727e-05, acc 1\n",
      "2022-03-25T10:35:54.240724: step 6754, loss 0.000158085, acc 1\n",
      "2022-03-25T10:35:54.393155: step 6755, loss 0.000724747, acc 1\n",
      "2022-03-25T10:35:54.532224: step 6756, loss 8.85016e-05, acc 1\n",
      "2022-03-25T10:35:54.683974: step 6757, loss 7.46793e-05, acc 1\n",
      "2022-03-25T10:35:54.828183: step 6758, loss 2.38783e-05, acc 1\n",
      "2022-03-25T10:35:54.974639: step 6759, loss 0.00156109, acc 1\n",
      "2022-03-25T10:35:55.122156: step 6760, loss 0.00969811, acc 1\n",
      "2022-03-25T10:35:55.264164: step 6761, loss 0.000381197, acc 1\n",
      "2022-03-25T10:35:55.420755: step 6762, loss 0.000212688, acc 1\n",
      "2022-03-25T10:35:55.569602: step 6763, loss 0.000137091, acc 1\n",
      "2022-03-25T10:35:55.717650: step 6764, loss 0.00123552, acc 1\n",
      "2022-03-25T10:35:55.861245: step 6765, loss 0.000499556, acc 1\n",
      "2022-03-25T10:35:56.012774: step 6766, loss 5.87161e-05, acc 1\n",
      "2022-03-25T10:35:56.159472: step 6767, loss 0.00034646, acc 1\n",
      "2022-03-25T10:35:56.312383: step 6768, loss 9.52709e-06, acc 1\n",
      "2022-03-25T10:35:56.450897: step 6769, loss 0.000874904, acc 1\n",
      "2022-03-25T10:35:56.604648: step 6770, loss 0.0029624, acc 1\n",
      "2022-03-25T10:35:56.755770: step 6771, loss 0.000138788, acc 1\n",
      "2022-03-25T10:35:56.898435: step 6772, loss 0.000162712, acc 1\n",
      "2022-03-25T10:35:57.038571: step 6773, loss 0.0123023, acc 1\n",
      "2022-03-25T10:35:57.186719: step 6774, loss 0.000192043, acc 1\n",
      "2022-03-25T10:35:57.331345: step 6775, loss 0.0040431, acc 1\n",
      "2022-03-25T10:35:57.494302: step 6776, loss 0.000113457, acc 1\n",
      "2022-03-25T10:35:57.647301: step 6777, loss 5.33067e-05, acc 1\n",
      "2022-03-25T10:35:57.794861: step 6778, loss 9.60714e-06, acc 1\n",
      "2022-03-25T10:35:57.950266: step 6779, loss 0.00118954, acc 1\n",
      "2022-03-25T10:35:58.089191: step 6780, loss 0.000929964, acc 1\n",
      "2022-03-25T10:35:58.238993: step 6781, loss 0.0426843, acc 0.984375\n",
      "2022-03-25T10:35:58.382803: step 6782, loss 8.04448e-06, acc 1\n",
      "2022-03-25T10:35:58.524450: step 6783, loss 0.000175018, acc 1\n",
      "2022-03-25T10:35:58.671565: step 6784, loss 0.000324174, acc 1\n",
      "2022-03-25T10:35:58.813326: step 6785, loss 7.0778e-05, acc 1\n",
      "2022-03-25T10:35:58.964103: step 6786, loss 0.00015749, acc 1\n",
      "2022-03-25T10:35:59.104070: step 6787, loss 0.000155193, acc 1\n",
      "2022-03-25T10:35:59.251128: step 6788, loss 4.18056e-05, acc 1\n",
      "2022-03-25T10:35:59.388923: step 6789, loss 0.00662035, acc 1\n",
      "2022-03-25T10:35:59.512708: step 6790, loss 0.00116234, acc 1\n",
      "2022-03-25T10:35:59.655455: step 6791, loss 4.71596e-05, acc 1\n",
      "2022-03-25T10:35:59.810428: step 6792, loss 0.00065799, acc 1\n",
      "2022-03-25T10:35:59.958643: step 6793, loss 0.000231611, acc 1\n",
      "2022-03-25T10:36:00.111102: step 6794, loss 0.00635651, acc 1\n",
      "2022-03-25T10:36:00.249186: step 6795, loss 6.95012e-05, acc 1\n",
      "2022-03-25T10:36:00.397364: step 6796, loss 5.29669e-05, acc 1\n",
      "2022-03-25T10:36:00.543256: step 6797, loss 0.00031876, acc 1\n",
      "2022-03-25T10:36:00.691405: step 6798, loss 0.00378684, acc 1\n",
      "2022-03-25T10:36:00.830562: step 6799, loss 0.000307812, acc 1\n",
      "2022-03-25T10:36:00.992112: step 6800, loss 0.000497483, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:36:01.129484: step 6800, loss 0.634898, acc 0.917151\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6800\n",
      "\n",
      "2022-03-25T10:36:01.380326: step 6801, loss 4.10128e-05, acc 1\n",
      "2022-03-25T10:36:01.530693: step 6802, loss 8.9768e-05, acc 1\n",
      "2022-03-25T10:36:01.674463: step 6803, loss 0.000142032, acc 1\n",
      "2022-03-25T10:36:01.827480: step 6804, loss 0.0258371, acc 0.984375\n",
      "2022-03-25T10:36:01.978292: step 6805, loss 5.41049e-05, acc 1\n",
      "2022-03-25T10:36:02.125161: step 6806, loss 0.000199794, acc 1\n",
      "2022-03-25T10:36:02.266605: step 6807, loss 0.00368813, acc 1\n",
      "2022-03-25T10:36:02.424575: step 6808, loss 0.00034038, acc 1\n",
      "2022-03-25T10:36:02.576331: step 6809, loss 5.19717e-05, acc 1\n",
      "2022-03-25T10:36:02.723637: step 6810, loss 8.64709e-05, acc 1\n",
      "2022-03-25T10:36:02.871657: step 6811, loss 0.00025419, acc 1\n",
      "2022-03-25T10:36:03.022175: step 6812, loss 0.000679986, acc 1\n",
      "2022-03-25T10:36:03.176747: step 6813, loss 0.000425992, acc 1\n",
      "2022-03-25T10:36:03.313849: step 6814, loss 0.000347125, acc 1\n",
      "2022-03-25T10:36:03.456652: step 6815, loss 0.000255441, acc 1\n",
      "2022-03-25T10:36:03.616607: step 6816, loss 0.00029652, acc 1\n",
      "2022-03-25T10:36:03.766792: step 6817, loss 0.00594471, acc 1\n",
      "2022-03-25T10:36:03.910753: step 6818, loss 0.000142633, acc 1\n",
      "2022-03-25T10:36:04.060751: step 6819, loss 1.47437e-05, acc 1\n",
      "2022-03-25T10:36:04.205433: step 6820, loss 0.000375727, acc 1\n",
      "2022-03-25T10:36:04.352767: step 6821, loss 0.000103045, acc 1\n",
      "2022-03-25T10:36:04.492814: step 6822, loss 0.000302763, acc 1\n",
      "2022-03-25T10:36:04.646640: step 6823, loss 0.000256928, acc 1\n",
      "2022-03-25T10:36:04.789476: step 6824, loss 7.88019e-05, acc 1\n",
      "2022-03-25T10:36:04.945157: step 6825, loss 9.25878e-05, acc 1\n",
      "2022-03-25T10:36:05.083940: step 6826, loss 0.0013559, acc 1\n",
      "2022-03-25T10:36:05.220532: step 6827, loss 0.00467998, acc 1\n",
      "2022-03-25T10:36:05.363658: step 6828, loss 3.41824e-05, acc 1\n",
      "2022-03-25T10:36:05.509204: step 6829, loss 0.000212895, acc 1\n",
      "2022-03-25T10:36:05.665020: step 6830, loss 0.000355001, acc 1\n",
      "2022-03-25T10:36:05.815109: step 6831, loss 0.00168552, acc 1\n",
      "2022-03-25T10:36:05.950455: step 6832, loss 0.000519223, acc 1\n",
      "2022-03-25T10:36:06.093423: step 6833, loss 0.000367015, acc 1\n",
      "2022-03-25T10:36:06.235376: step 6834, loss 0.000762287, acc 1\n",
      "2022-03-25T10:36:06.376445: step 6835, loss 4.46001e-05, acc 1\n",
      "2022-03-25T10:36:06.534672: step 6836, loss 0.0146575, acc 0.984375\n",
      "2022-03-25T10:36:06.692454: step 6837, loss 0.00204888, acc 1\n",
      "2022-03-25T10:36:06.842406: step 6838, loss 0.000218344, acc 1\n",
      "2022-03-25T10:36:06.992223: step 6839, loss 0.000158201, acc 1\n",
      "2022-03-25T10:36:07.148912: step 6840, loss 0.000127483, acc 1\n",
      "2022-03-25T10:36:07.297103: step 6841, loss 0.000625457, acc 1\n",
      "2022-03-25T10:36:07.449523: step 6842, loss 0.000384064, acc 1\n",
      "2022-03-25T10:36:07.605098: step 6843, loss 0.000658873, acc 1\n",
      "2022-03-25T10:36:07.758619: step 6844, loss 0.000377474, acc 1\n",
      "2022-03-25T10:36:07.909280: step 6845, loss 6.89394e-05, acc 1\n",
      "2022-03-25T10:36:08.056137: step 6846, loss 0.000723051, acc 1\n",
      "2022-03-25T10:36:08.203269: step 6847, loss 7.77846e-05, acc 1\n",
      "2022-03-25T10:36:08.351416: step 6848, loss 0.000140457, acc 1\n",
      "2022-03-25T10:36:08.502275: step 6849, loss 0.00192752, acc 1\n",
      "2022-03-25T10:36:08.647018: step 6850, loss 0.000108624, acc 1\n",
      "2022-03-25T10:36:08.795258: step 6851, loss 0.00135339, acc 1\n",
      "2022-03-25T10:36:08.939699: step 6852, loss 0.00400134, acc 1\n",
      "2022-03-25T10:36:09.084352: step 6853, loss 0.000281735, acc 1\n",
      "2022-03-25T10:36:09.231806: step 6854, loss 0.000153332, acc 1\n",
      "2022-03-25T10:36:09.375798: step 6855, loss 0.000288365, acc 1\n",
      "2022-03-25T10:36:09.525822: step 6856, loss 0.000771931, acc 1\n",
      "2022-03-25T10:36:09.669597: step 6857, loss 0.000921012, acc 1\n",
      "2022-03-25T10:36:09.816291: step 6858, loss 6.82011e-05, acc 1\n",
      "2022-03-25T10:36:09.962539: step 6859, loss 0.000110385, acc 1\n",
      "2022-03-25T10:36:10.121854: step 6860, loss 0.000388515, acc 1\n",
      "2022-03-25T10:36:10.273421: step 6861, loss 0.000393124, acc 1\n",
      "2022-03-25T10:36:10.425616: step 6862, loss 0.00140481, acc 1\n",
      "2022-03-25T10:36:10.572862: step 6863, loss 0.000527737, acc 1\n",
      "2022-03-25T10:36:10.726336: step 6864, loss 0.00152528, acc 1\n",
      "2022-03-25T10:36:10.873196: step 6865, loss 0.000121434, acc 1\n",
      "2022-03-25T10:36:11.021348: step 6866, loss 0.00130596, acc 1\n",
      "2022-03-25T10:36:11.182665: step 6867, loss 0.000475403, acc 1\n",
      "2022-03-25T10:36:11.330010: step 6868, loss 0.000311997, acc 1\n",
      "2022-03-25T10:36:11.470484: step 6869, loss 0.00118225, acc 1\n",
      "2022-03-25T10:36:11.621680: step 6870, loss 0.000932253, acc 1\n",
      "2022-03-25T10:36:11.776309: step 6871, loss 0.00278455, acc 1\n",
      "2022-03-25T10:36:11.918628: step 6872, loss 0.000395405, acc 1\n",
      "2022-03-25T10:36:12.058621: step 6873, loss 0.000317323, acc 1\n",
      "2022-03-25T10:36:12.213449: step 6874, loss 3.38124e-05, acc 1\n",
      "2022-03-25T10:36:12.354108: step 6875, loss 0.000317566, acc 1\n",
      "2022-03-25T10:36:12.500257: step 6876, loss 0.000725243, acc 1\n",
      "2022-03-25T10:36:12.651876: step 6877, loss 0.0017178, acc 1\n",
      "2022-03-25T10:36:12.806729: step 6878, loss 1.01992e-05, acc 1\n",
      "2022-03-25T10:36:12.951639: step 6879, loss 0.000213975, acc 1\n",
      "2022-03-25T10:36:13.096665: step 6880, loss 0.000302743, acc 1\n",
      "2022-03-25T10:36:13.235469: step 6881, loss 0.000282259, acc 1\n",
      "2022-03-25T10:36:13.386587: step 6882, loss 0.00173778, acc 1\n",
      "2022-03-25T10:36:13.533263: step 6883, loss 5.82253e-05, acc 1\n",
      "2022-03-25T10:36:13.678311: step 6884, loss 3.21454e-05, acc 1\n",
      "2022-03-25T10:36:13.831082: step 6885, loss 0.00307814, acc 1\n",
      "2022-03-25T10:36:13.975114: step 6886, loss 0.000189951, acc 1\n",
      "2022-03-25T10:36:14.118290: step 6887, loss 3.57104e-05, acc 1\n",
      "2022-03-25T10:36:14.266701: step 6888, loss 1.57279e-05, acc 1\n",
      "2022-03-25T10:36:14.406963: step 6889, loss 7.60011e-05, acc 1\n",
      "2022-03-25T10:36:14.554117: step 6890, loss 0.000511567, acc 1\n",
      "2022-03-25T10:36:14.701166: step 6891, loss 0.000261726, acc 1\n",
      "2022-03-25T10:36:14.856772: step 6892, loss 0.000161521, acc 1\n",
      "2022-03-25T10:36:14.999631: step 6893, loss 0.0156616, acc 0.984375\n",
      "2022-03-25T10:36:15.154582: step 6894, loss 0.000299955, acc 1\n",
      "2022-03-25T10:36:15.297836: step 6895, loss 0.00165975, acc 1\n",
      "2022-03-25T10:36:15.442998: step 6896, loss 8.76835e-05, acc 1\n",
      "2022-03-25T10:36:15.590176: step 6897, loss 0.000410962, acc 1\n",
      "2022-03-25T10:36:15.740239: step 6898, loss 3.07951e-05, acc 1\n",
      "2022-03-25T10:36:15.897564: step 6899, loss 0.000344471, acc 1\n",
      "2022-03-25T10:36:16.040438: step 6900, loss 0.000543983, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:36:16.174559: step 6900, loss 0.622081, acc 0.917151\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-6900\n",
      "\n",
      "2022-03-25T10:36:16.413155: step 6901, loss 0.000477699, acc 1\n",
      "2022-03-25T10:36:16.563397: step 6902, loss 7.00721e-05, acc 1\n",
      "2022-03-25T10:36:16.713236: step 6903, loss 0.00257567, acc 1\n",
      "2022-03-25T10:36:16.870771: step 6904, loss 0.000156946, acc 1\n",
      "2022-03-25T10:36:17.022320: step 6905, loss 0.000238388, acc 1\n",
      "2022-03-25T10:36:17.177985: step 6906, loss 0.00148034, acc 1\n",
      "2022-03-25T10:36:17.331019: step 6907, loss 0.000167443, acc 1\n",
      "2022-03-25T10:36:17.479248: step 6908, loss 0.000162536, acc 1\n",
      "2022-03-25T10:36:17.622010: step 6909, loss 0.000439122, acc 1\n",
      "2022-03-25T10:36:17.769326: step 6910, loss 5.4319e-05, acc 1\n",
      "2022-03-25T10:36:17.922507: step 6911, loss 9.71652e-05, acc 1\n",
      "2022-03-25T10:36:18.067350: step 6912, loss 0.000491135, acc 1\n",
      "2022-03-25T10:36:18.210982: step 6913, loss 0.000472489, acc 1\n",
      "2022-03-25T10:36:18.363533: step 6914, loss 0.000409566, acc 1\n",
      "2022-03-25T10:36:18.503571: step 6915, loss 6.12848e-05, acc 1\n",
      "2022-03-25T10:36:18.664994: step 6916, loss 0.000574072, acc 1\n",
      "2022-03-25T10:36:18.816670: step 6917, loss 0.000162278, acc 1\n",
      "2022-03-25T10:36:18.973617: step 6918, loss 0.000170053, acc 1\n",
      "2022-03-25T10:36:19.129277: step 6919, loss 5.6153e-05, acc 1\n",
      "2022-03-25T10:36:19.276914: step 6920, loss 3.41805e-05, acc 1\n",
      "2022-03-25T10:36:19.429226: step 6921, loss 0.00188761, acc 1\n",
      "2022-03-25T10:36:19.583851: step 6922, loss 9.39531e-05, acc 1\n",
      "2022-03-25T10:36:19.738742: step 6923, loss 0.019173, acc 0.984375\n",
      "2022-03-25T10:36:19.892363: step 6924, loss 0.00564159, acc 1\n",
      "2022-03-25T10:36:20.044888: step 6925, loss 4.48734e-05, acc 1\n",
      "2022-03-25T10:36:20.189104: step 6926, loss 0.000322062, acc 1\n",
      "2022-03-25T10:36:20.334456: step 6927, loss 0.00138094, acc 1\n",
      "2022-03-25T10:36:20.478797: step 6928, loss 0.000349359, acc 1\n",
      "2022-03-25T10:36:20.630815: step 6929, loss 0.00281108, acc 1\n",
      "2022-03-25T10:36:20.783993: step 6930, loss 9.78214e-06, acc 1\n",
      "2022-03-25T10:36:20.933750: step 6931, loss 9.54564e-05, acc 1\n",
      "2022-03-25T10:36:21.080731: step 6932, loss 0.000421904, acc 1\n",
      "2022-03-25T10:36:21.237421: step 6933, loss 0.0394719, acc 0.984375\n",
      "2022-03-25T10:36:21.387875: step 6934, loss 0.00055262, acc 1\n",
      "2022-03-25T10:36:21.544007: step 6935, loss 5.67712e-05, acc 1\n",
      "2022-03-25T10:36:21.695960: step 6936, loss 1.47212e-05, acc 1\n",
      "2022-03-25T10:36:21.854793: step 6937, loss 8.96942e-05, acc 1\n",
      "2022-03-25T10:36:22.014787: step 6938, loss 0.00113021, acc 1\n",
      "2022-03-25T10:36:22.166240: step 6939, loss 0.000504091, acc 1\n",
      "2022-03-25T10:36:22.307788: step 6940, loss 0.00143868, acc 1\n",
      "2022-03-25T10:36:22.476894: step 6941, loss 0.000713053, acc 1\n",
      "2022-03-25T10:36:22.638338: step 6942, loss 0.000402044, acc 1\n",
      "2022-03-25T10:36:22.786014: step 6943, loss 0.000302483, acc 1\n",
      "2022-03-25T10:36:22.947824: step 6944, loss 0.000125089, acc 1\n",
      "2022-03-25T10:36:23.096775: step 6945, loss 0.00203014, acc 1\n",
      "2022-03-25T10:36:23.240845: step 6946, loss 0.000370998, acc 1\n",
      "2022-03-25T10:36:23.387258: step 6947, loss 0.00011622, acc 1\n",
      "2022-03-25T10:36:23.537597: step 6948, loss 0.000203314, acc 1\n",
      "2022-03-25T10:36:23.696342: step 6949, loss 0.000554572, acc 1\n",
      "2022-03-25T10:36:23.841623: step 6950, loss 0.000711524, acc 1\n",
      "2022-03-25T10:36:23.993365: step 6951, loss 0.000446935, acc 1\n",
      "2022-03-25T10:36:24.146426: step 6952, loss 0.000156484, acc 1\n",
      "2022-03-25T10:36:24.305863: step 6953, loss 0.00127363, acc 1\n",
      "2022-03-25T10:36:24.462611: step 6954, loss 0.000229387, acc 1\n",
      "2022-03-25T10:36:24.613865: step 6955, loss 0.00235278, acc 1\n",
      "2022-03-25T10:36:24.764244: step 6956, loss 0.000652162, acc 1\n",
      "2022-03-25T10:36:24.906984: step 6957, loss 9.50165e-05, acc 1\n",
      "2022-03-25T10:36:25.063461: step 6958, loss 0.000363604, acc 1\n",
      "2022-03-25T10:36:25.214883: step 6959, loss 0.00161216, acc 1\n",
      "2022-03-25T10:36:25.363499: step 6960, loss 0.00014935, acc 1\n",
      "2022-03-25T10:36:25.512928: step 6961, loss 0.00344812, acc 1\n",
      "2022-03-25T10:36:25.663851: step 6962, loss 0.000232478, acc 1\n",
      "2022-03-25T10:36:25.812722: step 6963, loss 9.4159e-05, acc 1\n",
      "2022-03-25T10:36:25.965691: step 6964, loss 4.59361e-05, acc 1\n",
      "2022-03-25T10:36:26.129337: step 6965, loss 0.000228818, acc 1\n",
      "2022-03-25T10:36:26.278666: step 6966, loss 0.00023405, acc 1\n",
      "2022-03-25T10:36:26.428481: step 6967, loss 9.07609e-05, acc 1\n",
      "2022-03-25T10:36:26.583904: step 6968, loss 0.000706589, acc 1\n",
      "2022-03-25T10:36:26.730499: step 6969, loss 0.00125791, acc 1\n",
      "2022-03-25T10:36:26.877654: step 6970, loss 6.75957e-05, acc 1\n",
      "2022-03-25T10:36:27.024274: step 6971, loss 0.0305819, acc 0.984375\n",
      "2022-03-25T10:36:27.180188: step 6972, loss 6.25943e-05, acc 1\n",
      "2022-03-25T10:36:27.335264: step 6973, loss 0.000462795, acc 1\n",
      "2022-03-25T10:36:27.485419: step 6974, loss 0.000322387, acc 1\n",
      "2022-03-25T10:36:27.632522: step 6975, loss 0.0022541, acc 1\n",
      "2022-03-25T10:36:27.783559: step 6976, loss 2.46968e-05, acc 1\n",
      "2022-03-25T10:36:27.938891: step 6977, loss 1.7897e-05, acc 1\n",
      "2022-03-25T10:36:28.087247: step 6978, loss 0.000612233, acc 1\n",
      "2022-03-25T10:36:28.228907: step 6979, loss 0.00290079, acc 1\n",
      "2022-03-25T10:36:28.380230: step 6980, loss 2.96732e-05, acc 1\n",
      "2022-03-25T10:36:28.533241: step 6981, loss 9.91722e-05, acc 1\n",
      "2022-03-25T10:36:28.689308: step 6982, loss 6.46011e-05, acc 1\n",
      "2022-03-25T10:36:28.841465: step 6983, loss 3.05056e-05, acc 1\n",
      "2022-03-25T10:36:28.967955: step 6984, loss 0.000137601, acc 1\n",
      "2022-03-25T10:36:29.121443: step 6985, loss 0.000192543, acc 1\n",
      "2022-03-25T10:36:29.278832: step 6986, loss 8.34464e-05, acc 1\n",
      "2022-03-25T10:36:29.431915: step 6987, loss 0.000157445, acc 1\n",
      "2022-03-25T10:36:29.579338: step 6988, loss 0.000221941, acc 1\n",
      "2022-03-25T10:36:29.733631: step 6989, loss 0.000684815, acc 1\n",
      "2022-03-25T10:36:29.887721: step 6990, loss 0.00011272, acc 1\n",
      "2022-03-25T10:36:30.036627: step 6991, loss 7.87113e-05, acc 1\n",
      "2022-03-25T10:36:30.194225: step 6992, loss 4.05507e-05, acc 1\n",
      "2022-03-25T10:36:30.345278: step 6993, loss 0.000792232, acc 1\n",
      "2022-03-25T10:36:30.496374: step 6994, loss 3.81093e-05, acc 1\n",
      "2022-03-25T10:36:30.644569: step 6995, loss 0.00912865, acc 1\n",
      "2022-03-25T10:36:30.801330: step 6996, loss 0.000271468, acc 1\n",
      "2022-03-25T10:36:30.953627: step 6997, loss 0.000298903, acc 1\n",
      "2022-03-25T10:36:31.097708: step 6998, loss 2.82684e-05, acc 1\n",
      "2022-03-25T10:36:31.262815: step 6999, loss 0.00219157, acc 1\n",
      "2022-03-25T10:36:31.415583: step 7000, loss 0.000775519, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:36:31.564781: step 7000, loss 0.662452, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7000\n",
      "\n",
      "2022-03-25T10:36:31.825538: step 7001, loss 4.57297e-05, acc 1\n",
      "2022-03-25T10:36:31.960545: step 7002, loss 0.000152541, acc 1\n",
      "2022-03-25T10:36:32.108529: step 7003, loss 0.000869789, acc 1\n",
      "2022-03-25T10:36:32.268064: step 7004, loss 0.00375846, acc 1\n",
      "2022-03-25T10:36:32.413021: step 7005, loss 3.86787e-05, acc 1\n",
      "2022-03-25T10:36:32.558177: step 7006, loss 5.13311e-05, acc 1\n",
      "2022-03-25T10:36:32.708390: step 7007, loss 0.000174347, acc 1\n",
      "2022-03-25T10:36:32.863376: step 7008, loss 0.000284793, acc 1\n",
      "2022-03-25T10:36:33.017803: step 7009, loss 0.000372471, acc 1\n",
      "2022-03-25T10:36:33.162658: step 7010, loss 0.000163138, acc 1\n",
      "2022-03-25T10:36:33.315800: step 7011, loss 3.9279e-05, acc 1\n",
      "2022-03-25T10:36:33.455077: step 7012, loss 0.0001069, acc 1\n",
      "2022-03-25T10:36:33.596138: step 7013, loss 0.000166364, acc 1\n",
      "2022-03-25T10:36:33.749910: step 7014, loss 5.60946e-05, acc 1\n",
      "2022-03-25T10:36:33.905227: step 7015, loss 8.76175e-05, acc 1\n",
      "2022-03-25T10:36:34.046754: step 7016, loss 4.97875e-06, acc 1\n",
      "2022-03-25T10:36:34.188649: step 7017, loss 0.00201458, acc 1\n",
      "2022-03-25T10:36:34.346596: step 7018, loss 0.00488631, acc 1\n",
      "2022-03-25T10:36:34.494694: step 7019, loss 5.56208e-05, acc 1\n",
      "2022-03-25T10:36:34.636409: step 7020, loss 0.000429076, acc 1\n",
      "2022-03-25T10:36:34.774135: step 7021, loss 0.000108743, acc 1\n",
      "2022-03-25T10:36:34.922950: step 7022, loss 0.000443199, acc 1\n",
      "2022-03-25T10:36:35.068037: step 7023, loss 0.000176928, acc 1\n",
      "2022-03-25T10:36:35.214747: step 7024, loss 0.000340912, acc 1\n",
      "2022-03-25T10:36:35.370503: step 7025, loss 0.000118758, acc 1\n",
      "2022-03-25T10:36:35.516824: step 7026, loss 0.00147002, acc 1\n",
      "2022-03-25T10:36:35.662922: step 7027, loss 0.00483294, acc 1\n",
      "2022-03-25T10:36:35.811752: step 7028, loss 0.00103052, acc 1\n",
      "2022-03-25T10:36:35.961282: step 7029, loss 5.6784e-05, acc 1\n",
      "2022-03-25T10:36:36.107786: step 7030, loss 0.00319881, acc 1\n",
      "2022-03-25T10:36:36.253302: step 7031, loss 4.10841e-05, acc 1\n",
      "2022-03-25T10:36:36.410285: step 7032, loss 0.000112006, acc 1\n",
      "2022-03-25T10:36:36.556416: step 7033, loss 9.13653e-05, acc 1\n",
      "2022-03-25T10:36:36.704373: step 7034, loss 4.25335e-05, acc 1\n",
      "2022-03-25T10:36:36.859251: step 7035, loss 7.83366e-05, acc 1\n",
      "2022-03-25T10:36:37.003822: step 7036, loss 2.83615e-05, acc 1\n",
      "2022-03-25T10:36:37.146601: step 7037, loss 0.0100071, acc 1\n",
      "2022-03-25T10:36:37.299901: step 7038, loss 0.00391729, acc 1\n",
      "2022-03-25T10:36:37.450295: step 7039, loss 6.00331e-05, acc 1\n",
      "2022-03-25T10:36:37.607541: step 7040, loss 8.00216e-05, acc 1\n",
      "2022-03-25T10:36:37.756496: step 7041, loss 2.49981e-05, acc 1\n",
      "2022-03-25T10:36:37.907891: step 7042, loss 4.31664e-05, acc 1\n",
      "2022-03-25T10:36:38.059174: step 7043, loss 4.07288e-05, acc 1\n",
      "2022-03-25T10:36:38.211511: step 7044, loss 0.00013529, acc 1\n",
      "2022-03-25T10:36:38.360917: step 7045, loss 0.000218038, acc 1\n",
      "2022-03-25T10:36:38.510316: step 7046, loss 0.000241808, acc 1\n",
      "2022-03-25T10:36:38.661079: step 7047, loss 0.00417449, acc 1\n",
      "2022-03-25T10:36:38.812701: step 7048, loss 0.00727511, acc 1\n",
      "2022-03-25T10:36:38.962119: step 7049, loss 0.000136593, acc 1\n",
      "2022-03-25T10:36:39.118258: step 7050, loss 0.00414029, acc 1\n",
      "2022-03-25T10:36:39.267982: step 7051, loss 0.000446009, acc 1\n",
      "2022-03-25T10:36:39.419715: step 7052, loss 0.000633606, acc 1\n",
      "2022-03-25T10:36:39.559316: step 7053, loss 0.000299057, acc 1\n",
      "2022-03-25T10:36:39.709378: step 7054, loss 0.00012244, acc 1\n",
      "2022-03-25T10:36:39.856160: step 7055, loss 0.000312087, acc 1\n",
      "2022-03-25T10:36:40.002427: step 7056, loss 0.000253328, acc 1\n",
      "2022-03-25T10:36:40.147580: step 7057, loss 5.75659e-05, acc 1\n",
      "2022-03-25T10:36:40.292158: step 7058, loss 4.65932e-05, acc 1\n",
      "2022-03-25T10:36:40.445758: step 7059, loss 0.000100755, acc 1\n",
      "2022-03-25T10:36:40.586534: step 7060, loss 0.000850866, acc 1\n",
      "2022-03-25T10:36:40.729248: step 7061, loss 0.000406266, acc 1\n",
      "2022-03-25T10:36:40.882233: step 7062, loss 9.36311e-05, acc 1\n",
      "2022-03-25T10:36:41.031160: step 7063, loss 6.7519e-06, acc 1\n",
      "2022-03-25T10:36:41.176653: step 7064, loss 0.000589299, acc 1\n",
      "2022-03-25T10:36:41.321266: step 7065, loss 1.4157e-05, acc 1\n",
      "2022-03-25T10:36:41.470837: step 7066, loss 0.000163052, acc 1\n",
      "2022-03-25T10:36:41.621108: step 7067, loss 2.31475e-05, acc 1\n",
      "2022-03-25T10:36:41.764614: step 7068, loss 0.000385906, acc 1\n",
      "2022-03-25T10:36:41.906350: step 7069, loss 7.61873e-05, acc 1\n",
      "2022-03-25T10:36:42.055674: step 7070, loss 8.03462e-05, acc 1\n",
      "2022-03-25T10:36:42.196268: step 7071, loss 7.31307e-05, acc 1\n",
      "2022-03-25T10:36:42.343428: step 7072, loss 0.000239297, acc 1\n",
      "2022-03-25T10:36:42.502335: step 7073, loss 0.00016397, acc 1\n",
      "2022-03-25T10:36:42.640327: step 7074, loss 0.000159125, acc 1\n",
      "2022-03-25T10:36:42.776142: step 7075, loss 5.61172e-05, acc 1\n",
      "2022-03-25T10:36:42.931769: step 7076, loss 6.98088e-05, acc 1\n",
      "2022-03-25T10:36:43.080997: step 7077, loss 0.000279359, acc 1\n",
      "2022-03-25T10:36:43.223582: step 7078, loss 0.00528016, acc 1\n",
      "2022-03-25T10:36:43.376876: step 7079, loss 7.1829e-05, acc 1\n",
      "2022-03-25T10:36:43.527686: step 7080, loss 0.000113156, acc 1\n",
      "2022-03-25T10:36:43.668465: step 7081, loss 2.04766e-05, acc 1\n",
      "2022-03-25T10:36:43.816628: step 7082, loss 0.00141498, acc 1\n",
      "2022-03-25T10:36:43.954263: step 7083, loss 0.000197456, acc 1\n",
      "2022-03-25T10:36:44.105231: step 7084, loss 0.0200401, acc 0.984375\n",
      "2022-03-25T10:36:44.246788: step 7085, loss 5.35074e-05, acc 1\n",
      "2022-03-25T10:36:44.390485: step 7086, loss 0.00294804, acc 1\n",
      "2022-03-25T10:36:44.539493: step 7087, loss 0.0299952, acc 0.984375\n",
      "2022-03-25T10:36:44.687180: step 7088, loss 9.28488e-05, acc 1\n",
      "2022-03-25T10:36:44.835467: step 7089, loss 0.000111093, acc 1\n",
      "2022-03-25T10:36:44.978823: step 7090, loss 0.000446979, acc 1\n",
      "2022-03-25T10:36:45.117650: step 7091, loss 0.000151585, acc 1\n",
      "2022-03-25T10:36:45.261436: step 7092, loss 0.000623174, acc 1\n",
      "2022-03-25T10:36:45.401699: step 7093, loss 0.0011704, acc 1\n",
      "2022-03-25T10:36:45.550478: step 7094, loss 5.61131e-05, acc 1\n",
      "2022-03-25T10:36:45.689986: step 7095, loss 4.0929e-05, acc 1\n",
      "2022-03-25T10:36:45.840530: step 7096, loss 5.44318e-05, acc 1\n",
      "2022-03-25T10:36:45.987076: step 7097, loss 0.00313446, acc 1\n",
      "2022-03-25T10:36:46.136294: step 7098, loss 4.42782e-05, acc 1\n",
      "2022-03-25T10:36:46.276290: step 7099, loss 4.2026e-05, acc 1\n",
      "2022-03-25T10:36:46.432534: step 7100, loss 0.000467039, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:36:46.567439: step 7100, loss 0.641756, acc 0.921512\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7100\n",
      "\n",
      "2022-03-25T10:36:46.819943: step 7101, loss 0.00153895, acc 1\n",
      "2022-03-25T10:36:46.976089: step 7102, loss 0.00123974, acc 1\n",
      "2022-03-25T10:36:47.122620: step 7103, loss 0.00920781, acc 1\n",
      "2022-03-25T10:36:47.266603: step 7104, loss 3.86548e-05, acc 1\n",
      "2022-03-25T10:36:47.412198: step 7105, loss 0.000461644, acc 1\n",
      "2022-03-25T10:36:47.566483: step 7106, loss 9.27017e-05, acc 1\n",
      "2022-03-25T10:36:47.714896: step 7107, loss 8.21254e-05, acc 1\n",
      "2022-03-25T10:36:47.854861: step 7108, loss 0.000195382, acc 1\n",
      "2022-03-25T10:36:48.002733: step 7109, loss 0.0002605, acc 1\n",
      "2022-03-25T10:36:48.150263: step 7110, loss 0.00142218, acc 1\n",
      "2022-03-25T10:36:48.300407: step 7111, loss 0.00887744, acc 1\n",
      "2022-03-25T10:36:48.443855: step 7112, loss 0.00012235, acc 1\n",
      "2022-03-25T10:36:48.594984: step 7113, loss 7.51245e-05, acc 1\n",
      "2022-03-25T10:36:48.732250: step 7114, loss 0.000799434, acc 1\n",
      "2022-03-25T10:36:48.876043: step 7115, loss 9.01825e-05, acc 1\n",
      "2022-03-25T10:36:49.022219: step 7116, loss 0.0538504, acc 0.984375\n",
      "2022-03-25T10:36:49.165431: step 7117, loss 0.00364698, acc 1\n",
      "2022-03-25T10:36:49.322186: step 7118, loss 0.000497915, acc 1\n",
      "2022-03-25T10:36:49.478267: step 7119, loss 9.70408e-05, acc 1\n",
      "2022-03-25T10:36:49.624204: step 7120, loss 0.00272228, acc 1\n",
      "2022-03-25T10:36:49.767916: step 7121, loss 0.000370777, acc 1\n",
      "2022-03-25T10:36:49.908774: step 7122, loss 0.00116488, acc 1\n",
      "2022-03-25T10:36:50.054131: step 7123, loss 0.00144359, acc 1\n",
      "2022-03-25T10:36:50.196793: step 7124, loss 0.00108875, acc 1\n",
      "2022-03-25T10:36:50.328500: step 7125, loss 0.000785375, acc 1\n",
      "2022-03-25T10:36:50.476361: step 7126, loss 0.000901904, acc 1\n",
      "2022-03-25T10:36:50.634498: step 7127, loss 0.000279339, acc 1\n",
      "2022-03-25T10:36:50.776110: step 7128, loss 0.000748318, acc 1\n",
      "2022-03-25T10:36:50.921906: step 7129, loss 0.000290071, acc 1\n",
      "2022-03-25T10:36:51.067572: step 7130, loss 0.00021501, acc 1\n",
      "2022-03-25T10:36:51.211441: step 7131, loss 7.13987e-05, acc 1\n",
      "2022-03-25T10:36:51.356833: step 7132, loss 0.00187167, acc 1\n",
      "2022-03-25T10:36:51.497211: step 7133, loss 0.00181753, acc 1\n",
      "2022-03-25T10:36:51.657434: step 7134, loss 0.00033355, acc 1\n",
      "2022-03-25T10:36:51.808789: step 7135, loss 0.00624413, acc 1\n",
      "2022-03-25T10:36:51.947754: step 7136, loss 0.000154617, acc 1\n",
      "2022-03-25T10:36:52.087805: step 7137, loss 9.62519e-05, acc 1\n",
      "2022-03-25T10:36:52.238322: step 7138, loss 0.0018928, acc 1\n",
      "2022-03-25T10:36:52.380094: step 7139, loss 0.000934135, acc 1\n",
      "2022-03-25T10:36:52.531770: step 7140, loss 0.000154832, acc 1\n",
      "2022-03-25T10:36:52.686361: step 7141, loss 0.000111615, acc 1\n",
      "2022-03-25T10:36:52.836694: step 7142, loss 0.000114038, acc 1\n",
      "2022-03-25T10:36:52.986323: step 7143, loss 0.000936444, acc 1\n",
      "2022-03-25T10:36:53.128115: step 7144, loss 0.000594586, acc 1\n",
      "2022-03-25T10:36:53.266980: step 7145, loss 5.38646e-05, acc 1\n",
      "2022-03-25T10:36:53.416050: step 7146, loss 0.00427323, acc 1\n",
      "2022-03-25T10:36:53.557465: step 7147, loss 0.000113389, acc 1\n",
      "2022-03-25T10:36:53.719485: step 7148, loss 0.00388825, acc 1\n",
      "2022-03-25T10:36:53.863087: step 7149, loss 0.000102177, acc 1\n",
      "2022-03-25T10:36:54.009360: step 7150, loss 0.000137634, acc 1\n",
      "2022-03-25T10:36:54.151627: step 7151, loss 0.00271453, acc 1\n",
      "2022-03-25T10:36:54.303303: step 7152, loss 8.236e-05, acc 1\n",
      "2022-03-25T10:36:54.460955: step 7153, loss 5.39121e-05, acc 1\n",
      "2022-03-25T10:36:54.600128: step 7154, loss 0.00244742, acc 1\n",
      "2022-03-25T10:36:54.761785: step 7155, loss 0.00124437, acc 1\n",
      "2022-03-25T10:36:54.908138: step 7156, loss 6.10412e-05, acc 1\n",
      "2022-03-25T10:36:55.062666: step 7157, loss 0.000361983, acc 1\n",
      "2022-03-25T10:36:55.197983: step 7158, loss 0.000142926, acc 1\n",
      "2022-03-25T10:36:55.341471: step 7159, loss 2.7557e-05, acc 1\n",
      "2022-03-25T10:36:55.485689: step 7160, loss 0.000329436, acc 1\n",
      "2022-03-25T10:36:55.629002: step 7161, loss 0.00017437, acc 1\n",
      "2022-03-25T10:36:55.794554: step 7162, loss 0.00037753, acc 1\n",
      "2022-03-25T10:36:55.949205: step 7163, loss 0.000657686, acc 1\n",
      "2022-03-25T10:36:56.091189: step 7164, loss 0.00490785, acc 1\n",
      "2022-03-25T10:36:56.237498: step 7165, loss 0.000244918, acc 1\n",
      "2022-03-25T10:36:56.383945: step 7166, loss 0.000736981, acc 1\n",
      "2022-03-25T10:36:56.526174: step 7167, loss 0.000578703, acc 1\n",
      "2022-03-25T10:36:56.678090: step 7168, loss 0.00177155, acc 1\n",
      "2022-03-25T10:36:56.828225: step 7169, loss 0.0306612, acc 0.984375\n",
      "2022-03-25T10:36:56.973897: step 7170, loss 0.000356529, acc 1\n",
      "2022-03-25T10:36:57.112292: step 7171, loss 0.000105648, acc 1\n",
      "2022-03-25T10:36:57.259265: step 7172, loss 0.000146894, acc 1\n",
      "2022-03-25T10:36:57.404339: step 7173, loss 0.000726777, acc 1\n",
      "2022-03-25T10:36:57.548928: step 7174, loss 0.00456351, acc 1\n",
      "2022-03-25T10:36:57.684829: step 7175, loss 0.000162069, acc 1\n",
      "2022-03-25T10:36:57.853231: step 7176, loss 0.000197844, acc 1\n",
      "2022-03-25T10:36:58.000618: step 7177, loss 0.000127001, acc 1\n",
      "2022-03-25T10:36:58.122704: step 7178, loss 0.000316779, acc 1\n",
      "2022-03-25T10:36:58.273821: step 7179, loss 0.000689665, acc 1\n",
      "2022-03-25T10:36:58.421208: step 7180, loss 0.00020754, acc 1\n",
      "2022-03-25T10:36:58.564798: step 7181, loss 0.00178939, acc 1\n",
      "2022-03-25T10:36:58.713620: step 7182, loss 0.000270204, acc 1\n",
      "2022-03-25T10:36:58.862473: step 7183, loss 0.000150375, acc 1\n",
      "2022-03-25T10:36:59.014386: step 7184, loss 0.000317138, acc 1\n",
      "2022-03-25T10:36:59.159783: step 7185, loss 0.0147905, acc 0.984375\n",
      "2022-03-25T10:36:59.302583: step 7186, loss 0.000763825, acc 1\n",
      "2022-03-25T10:36:59.449318: step 7187, loss 0.000213539, acc 1\n",
      "2022-03-25T10:36:59.596471: step 7188, loss 0.00031136, acc 1\n",
      "2022-03-25T10:36:59.746157: step 7189, loss 0.00944108, acc 1\n",
      "2022-03-25T10:36:59.895432: step 7190, loss 0.00345263, acc 1\n",
      "2022-03-25T10:37:00.038139: step 7191, loss 0.000925438, acc 1\n",
      "2022-03-25T10:37:00.189704: step 7192, loss 0.000461966, acc 1\n",
      "2022-03-25T10:37:00.335972: step 7193, loss 0.0107778, acc 1\n",
      "2022-03-25T10:37:00.487881: step 7194, loss 0.000291309, acc 1\n",
      "2022-03-25T10:37:00.635880: step 7195, loss 0.000367412, acc 1\n",
      "2022-03-25T10:37:00.786657: step 7196, loss 0.000174544, acc 1\n",
      "2022-03-25T10:37:00.949864: step 7197, loss 0.000502984, acc 1\n",
      "2022-03-25T10:37:01.104476: step 7198, loss 0.000725006, acc 1\n",
      "2022-03-25T10:37:01.253856: step 7199, loss 0.000117947, acc 1\n",
      "2022-03-25T10:37:01.392171: step 7200, loss 0.000631007, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:37:01.527986: step 7200, loss 0.648265, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7200\n",
      "\n",
      "2022-03-25T10:37:01.766751: step 7201, loss 5.7316e-05, acc 1\n",
      "2022-03-25T10:37:01.925670: step 7202, loss 8.91733e-05, acc 1\n",
      "2022-03-25T10:37:02.060852: step 7203, loss 3.72939e-05, acc 1\n",
      "2022-03-25T10:37:02.203300: step 7204, loss 0.000138993, acc 1\n",
      "2022-03-25T10:37:02.346427: step 7205, loss 0.00050335, acc 1\n",
      "2022-03-25T10:37:02.495713: step 7206, loss 0.000197596, acc 1\n",
      "2022-03-25T10:37:02.634564: step 7207, loss 1.73795e-05, acc 1\n",
      "2022-03-25T10:37:02.776396: step 7208, loss 0.00126386, acc 1\n",
      "2022-03-25T10:37:02.920768: step 7209, loss 0.000561799, acc 1\n",
      "2022-03-25T10:37:03.069267: step 7210, loss 0.000220474, acc 1\n",
      "2022-03-25T10:37:03.209252: step 7211, loss 0.000263391, acc 1\n",
      "2022-03-25T10:37:03.355080: step 7212, loss 0.000270796, acc 1\n",
      "2022-03-25T10:37:03.497712: step 7213, loss 9.47489e-05, acc 1\n",
      "2022-03-25T10:37:03.647035: step 7214, loss 0.000183942, acc 1\n",
      "2022-03-25T10:37:03.789912: step 7215, loss 8.76692e-05, acc 1\n",
      "2022-03-25T10:37:03.944497: step 7216, loss 0.00183119, acc 1\n",
      "2022-03-25T10:37:04.075797: step 7217, loss 0.000349674, acc 1\n",
      "2022-03-25T10:37:04.216783: step 7218, loss 0.00325138, acc 1\n",
      "2022-03-25T10:37:04.357267: step 7219, loss 0.00552437, acc 1\n",
      "2022-03-25T10:37:04.504889: step 7220, loss 0.000444535, acc 1\n",
      "2022-03-25T10:37:04.663362: step 7221, loss 0.000247846, acc 1\n",
      "2022-03-25T10:37:04.804893: step 7222, loss 0.00116683, acc 1\n",
      "2022-03-25T10:37:04.952291: step 7223, loss 7.64962e-05, acc 1\n",
      "2022-03-25T10:37:05.111117: step 7224, loss 0.000174691, acc 1\n",
      "2022-03-25T10:37:05.259666: step 7225, loss 2.52299e-05, acc 1\n",
      "2022-03-25T10:37:05.410935: step 7226, loss 0.000180271, acc 1\n",
      "2022-03-25T10:37:05.553990: step 7227, loss 0.00349991, acc 1\n",
      "2022-03-25T10:37:05.707110: step 7228, loss 0.000207613, acc 1\n",
      "2022-03-25T10:37:05.856158: step 7229, loss 0.00622283, acc 1\n",
      "2022-03-25T10:37:06.018455: step 7230, loss 0.000442482, acc 1\n",
      "2022-03-25T10:37:06.164197: step 7231, loss 0.000468852, acc 1\n",
      "2022-03-25T10:37:06.311859: step 7232, loss 0.000447226, acc 1\n",
      "2022-03-25T10:37:06.468427: step 7233, loss 0.000539707, acc 1\n",
      "2022-03-25T10:37:06.627040: step 7234, loss 0.00311818, acc 1\n",
      "2022-03-25T10:37:06.780917: step 7235, loss 1.73901e-05, acc 1\n",
      "2022-03-25T10:37:06.929978: step 7236, loss 0.000207782, acc 1\n",
      "2022-03-25T10:37:07.075793: step 7237, loss 0.000112011, acc 1\n",
      "2022-03-25T10:37:07.227544: step 7238, loss 0.000690761, acc 1\n",
      "2022-03-25T10:37:07.379157: step 7239, loss 0.00014841, acc 1\n",
      "2022-03-25T10:37:07.535486: step 7240, loss 0.00631985, acc 1\n",
      "2022-03-25T10:37:07.687086: step 7241, loss 6.12873e-05, acc 1\n",
      "2022-03-25T10:37:07.833980: step 7242, loss 0.00116535, acc 1\n",
      "2022-03-25T10:37:07.980726: step 7243, loss 4.20045e-05, acc 1\n",
      "2022-03-25T10:37:08.150290: step 7244, loss 8.92231e-05, acc 1\n",
      "2022-03-25T10:37:08.306153: step 7245, loss 0.000780266, acc 1\n",
      "2022-03-25T10:37:08.454783: step 7246, loss 2.87413e-05, acc 1\n",
      "2022-03-25T10:37:08.613571: step 7247, loss 0.00029388, acc 1\n",
      "2022-03-25T10:37:08.770958: step 7248, loss 3.6072e-05, acc 1\n",
      "2022-03-25T10:37:08.932467: step 7249, loss 0.000845632, acc 1\n",
      "2022-03-25T10:37:09.090188: step 7250, loss 6.18852e-05, acc 1\n",
      "2022-03-25T10:37:09.246715: step 7251, loss 7.76098e-05, acc 1\n",
      "2022-03-25T10:37:09.390535: step 7252, loss 0.00917949, acc 1\n",
      "2022-03-25T10:37:09.535402: step 7253, loss 6.50674e-05, acc 1\n",
      "2022-03-25T10:37:09.702408: step 7254, loss 0.000467769, acc 1\n",
      "2022-03-25T10:37:09.870409: step 7255, loss 0.00100358, acc 1\n",
      "2022-03-25T10:37:10.021565: step 7256, loss 8.48761e-05, acc 1\n",
      "2022-03-25T10:37:10.190071: step 7257, loss 0.000579824, acc 1\n",
      "2022-03-25T10:37:10.333461: step 7258, loss 2.83931e-05, acc 1\n",
      "2022-03-25T10:37:10.487592: step 7259, loss 1.50302e-05, acc 1\n",
      "2022-03-25T10:37:10.636029: step 7260, loss 0.000214154, acc 1\n",
      "2022-03-25T10:37:10.790652: step 7261, loss 8.64016e-05, acc 1\n",
      "2022-03-25T10:37:10.944819: step 7262, loss 2.02885e-05, acc 1\n",
      "2022-03-25T10:37:11.124802: step 7263, loss 0.0295804, acc 0.984375\n",
      "2022-03-25T10:37:11.284391: step 7264, loss 0.000615876, acc 1\n",
      "2022-03-25T10:37:11.437570: step 7265, loss 0.000599728, acc 1\n",
      "2022-03-25T10:37:11.592565: step 7266, loss 0.00014633, acc 1\n",
      "2022-03-25T10:37:11.747748: step 7267, loss 0.000129261, acc 1\n",
      "2022-03-25T10:37:11.903415: step 7268, loss 0.000782922, acc 1\n",
      "2022-03-25T10:37:12.054429: step 7269, loss 0.0163306, acc 0.984375\n",
      "2022-03-25T10:37:12.203324: step 7270, loss 0.00017013, acc 1\n",
      "2022-03-25T10:37:12.355410: step 7271, loss 0.000995644, acc 1\n",
      "2022-03-25T10:37:12.507512: step 7272, loss 0.00963856, acc 1\n",
      "2022-03-25T10:37:12.654190: step 7273, loss 0.000391979, acc 1\n",
      "2022-03-25T10:37:12.799098: step 7274, loss 0.000149766, acc 1\n",
      "2022-03-25T10:37:12.954706: step 7275, loss 0.0325294, acc 0.984375\n",
      "2022-03-25T10:37:13.106455: step 7276, loss 1.24507e-05, acc 1\n",
      "2022-03-25T10:37:13.263136: step 7277, loss 0.000127114, acc 1\n",
      "2022-03-25T10:37:13.410473: step 7278, loss 5.12961e-05, acc 1\n",
      "2022-03-25T10:37:13.563004: step 7279, loss 6.4575e-06, acc 1\n",
      "2022-03-25T10:37:13.714810: step 7280, loss 0.000731193, acc 1\n",
      "2022-03-25T10:37:13.862086: step 7281, loss 0.00145336, acc 1\n",
      "2022-03-25T10:37:14.010314: step 7282, loss 0.000208574, acc 1\n",
      "2022-03-25T10:37:14.174663: step 7283, loss 0.000314069, acc 1\n",
      "2022-03-25T10:37:14.324455: step 7284, loss 0.000258956, acc 1\n",
      "2022-03-25T10:37:14.479675: step 7285, loss 0.0010769, acc 1\n",
      "2022-03-25T10:37:14.618627: step 7286, loss 0.0301215, acc 0.984375\n",
      "2022-03-25T10:37:14.772278: step 7287, loss 2.63986e-05, acc 1\n",
      "2022-03-25T10:37:14.918806: step 7288, loss 2.15211e-05, acc 1\n",
      "2022-03-25T10:37:15.067040: step 7289, loss 0.0577212, acc 0.984375\n",
      "2022-03-25T10:37:15.229298: step 7290, loss 3.84068e-06, acc 1\n",
      "2022-03-25T10:37:15.378462: step 7291, loss 1.81197e-05, acc 1\n",
      "2022-03-25T10:37:15.532264: step 7292, loss 4.75413e-05, acc 1\n",
      "2022-03-25T10:37:15.691789: step 7293, loss 0.000199435, acc 1\n",
      "2022-03-25T10:37:15.848558: step 7294, loss 0.000430247, acc 1\n",
      "2022-03-25T10:37:16.015750: step 7295, loss 0.00232545, acc 1\n",
      "2022-03-25T10:37:16.162928: step 7296, loss 0.000172194, acc 1\n",
      "2022-03-25T10:37:16.320323: step 7297, loss 4.66818e-05, acc 1\n",
      "2022-03-25T10:37:16.470633: step 7298, loss 0.00211403, acc 1\n",
      "2022-03-25T10:37:16.625251: step 7299, loss 0.000104706, acc 1\n",
      "2022-03-25T10:37:16.783218: step 7300, loss 0.000195119, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:37:16.933823: step 7300, loss 0.608477, acc 0.912791\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7300\n",
      "\n",
      "2022-03-25T10:37:17.195173: step 7301, loss 0.000849467, acc 1\n",
      "2022-03-25T10:37:17.355649: step 7302, loss 0.00976938, acc 1\n",
      "2022-03-25T10:37:17.509000: step 7303, loss 0.000142475, acc 1\n",
      "2022-03-25T10:37:17.670661: step 7304, loss 0.00105028, acc 1\n",
      "2022-03-25T10:37:17.830612: step 7305, loss 0.00193854, acc 1\n",
      "2022-03-25T10:37:17.978768: step 7306, loss 0.020485, acc 0.984375\n",
      "2022-03-25T10:37:18.134567: step 7307, loss 0.000551805, acc 1\n",
      "2022-03-25T10:37:18.293374: step 7308, loss 0.0010591, acc 1\n",
      "2022-03-25T10:37:18.443375: step 7309, loss 0.000482467, acc 1\n",
      "2022-03-25T10:37:18.591940: step 7310, loss 0.000441601, acc 1\n",
      "2022-03-25T10:37:18.740189: step 7311, loss 0.0065294, acc 1\n",
      "2022-03-25T10:37:18.896844: step 7312, loss 0.00123766, acc 1\n",
      "2022-03-25T10:37:19.054576: step 7313, loss 9.94166e-05, acc 1\n",
      "2022-03-25T10:37:19.198337: step 7314, loss 5.50494e-05, acc 1\n",
      "2022-03-25T10:37:19.352571: step 7315, loss 0.0329348, acc 0.984375\n",
      "2022-03-25T10:37:19.503690: step 7316, loss 0.00221646, acc 1\n",
      "2022-03-25T10:37:19.662633: step 7317, loss 0.000218196, acc 1\n",
      "2022-03-25T10:37:19.806802: step 7318, loss 0.00175934, acc 1\n",
      "2022-03-25T10:37:19.961561: step 7319, loss 3.80347e-06, acc 1\n",
      "2022-03-25T10:37:20.117774: step 7320, loss 0.00118941, acc 1\n",
      "2022-03-25T10:37:20.273637: step 7321, loss 0.000165769, acc 1\n",
      "2022-03-25T10:37:20.420448: step 7322, loss 0.00318049, acc 1\n",
      "2022-03-25T10:37:20.574047: step 7323, loss 0.00377977, acc 1\n",
      "2022-03-25T10:37:20.723901: step 7324, loss 0.0646277, acc 0.984375\n",
      "2022-03-25T10:37:20.881750: step 7325, loss 0.0846525, acc 0.984375\n",
      "2022-03-25T10:37:21.037628: step 7326, loss 0.00426928, acc 1\n",
      "2022-03-25T10:37:21.180439: step 7327, loss 0.00018278, acc 1\n",
      "2022-03-25T10:37:21.338733: step 7328, loss 0.000350769, acc 1\n",
      "2022-03-25T10:37:21.486284: step 7329, loss 0.000282509, acc 1\n",
      "2022-03-25T10:37:21.634538: step 7330, loss 3.69975e-05, acc 1\n",
      "2022-03-25T10:37:21.781305: step 7331, loss 0.00803071, acc 1\n",
      "2022-03-25T10:37:21.931975: step 7332, loss 4.3226e-05, acc 1\n",
      "2022-03-25T10:37:22.080986: step 7333, loss 0.00802937, acc 1\n",
      "2022-03-25T10:37:22.231430: step 7334, loss 0.000111081, acc 1\n",
      "2022-03-25T10:37:22.388493: step 7335, loss 0.00508776, acc 1\n",
      "2022-03-25T10:37:22.538807: step 7336, loss 0.00277429, acc 1\n",
      "2022-03-25T10:37:22.695929: step 7337, loss 0.000281757, acc 1\n",
      "2022-03-25T10:37:22.846346: step 7338, loss 0.00363838, acc 1\n",
      "2022-03-25T10:37:23.002798: step 7339, loss 0.000589377, acc 1\n",
      "2022-03-25T10:37:23.151347: step 7340, loss 0.0302162, acc 0.984375\n",
      "2022-03-25T10:37:23.312262: step 7341, loss 0.00932169, acc 1\n",
      "2022-03-25T10:37:23.468380: step 7342, loss 0.00122024, acc 1\n",
      "2022-03-25T10:37:23.621138: step 7343, loss 4.73706e-05, acc 1\n",
      "2022-03-25T10:37:23.777961: step 7344, loss 0.000178121, acc 1\n",
      "2022-03-25T10:37:23.923010: step 7345, loss 0.00024078, acc 1\n",
      "2022-03-25T10:37:24.082414: step 7346, loss 7.27566e-05, acc 1\n",
      "2022-03-25T10:37:24.231011: step 7347, loss 0.000186338, acc 1\n",
      "2022-03-25T10:37:24.391905: step 7348, loss 0.0068641, acc 1\n",
      "2022-03-25T10:37:24.548244: step 7349, loss 4.66214e-05, acc 1\n",
      "2022-03-25T10:37:24.695847: step 7350, loss 0.000103722, acc 1\n",
      "2022-03-25T10:37:24.845629: step 7351, loss 0.000355024, acc 1\n",
      "2022-03-25T10:37:24.992142: step 7352, loss 3.40474e-05, acc 1\n",
      "2022-03-25T10:37:25.144198: step 7353, loss 0.000200618, acc 1\n",
      "2022-03-25T10:37:25.287647: step 7354, loss 8.83853e-05, acc 1\n",
      "2022-03-25T10:37:25.436577: step 7355, loss 0.0217554, acc 0.984375\n",
      "2022-03-25T10:37:25.593002: step 7356, loss 0.00107325, acc 1\n",
      "2022-03-25T10:37:25.739392: step 7357, loss 0.000345883, acc 1\n",
      "2022-03-25T10:37:25.895090: step 7358, loss 1.31759e-05, acc 1\n",
      "2022-03-25T10:37:26.042019: step 7359, loss 0.000248477, acc 1\n",
      "2022-03-25T10:37:26.194673: step 7360, loss 0.0319954, acc 0.984375\n",
      "2022-03-25T10:37:26.339751: step 7361, loss 0.00086834, acc 1\n",
      "2022-03-25T10:37:26.496129: step 7362, loss 5.38812e-05, acc 1\n",
      "2022-03-25T10:37:26.645442: step 7363, loss 0.0206021, acc 0.984375\n",
      "2022-03-25T10:37:26.800947: step 7364, loss 0.0390837, acc 0.984375\n",
      "2022-03-25T10:37:26.951888: step 7365, loss 0.000648883, acc 1\n",
      "2022-03-25T10:37:27.103035: step 7366, loss 8.76778e-05, acc 1\n",
      "2022-03-25T10:37:27.259160: step 7367, loss 0.00014747, acc 1\n",
      "2022-03-25T10:37:27.409702: step 7368, loss 0.000224163, acc 1\n",
      "2022-03-25T10:37:27.557808: step 7369, loss 0.000855459, acc 1\n",
      "2022-03-25T10:37:27.710291: step 7370, loss 0.000270878, acc 1\n",
      "2022-03-25T10:37:27.862221: step 7371, loss 0.00642666, acc 1\n",
      "2022-03-25T10:37:27.986402: step 7372, loss 0.000477928, acc 1\n",
      "2022-03-25T10:37:28.135934: step 7373, loss 0.00252764, acc 1\n",
      "2022-03-25T10:37:28.290147: step 7374, loss 0.000331246, acc 1\n",
      "2022-03-25T10:37:28.446604: step 7375, loss 0.0111482, acc 1\n",
      "2022-03-25T10:37:28.592705: step 7376, loss 0.0110127, acc 1\n",
      "2022-03-25T10:37:28.749143: step 7377, loss 0.0162775, acc 0.984375\n",
      "2022-03-25T10:37:28.902824: step 7378, loss 0.000261342, acc 1\n",
      "2022-03-25T10:37:29.058619: step 7379, loss 0.00042166, acc 1\n",
      "2022-03-25T10:37:29.204784: step 7380, loss 0.000138983, acc 1\n",
      "2022-03-25T10:37:29.354886: step 7381, loss 8.70919e-05, acc 1\n",
      "2022-03-25T10:37:29.507770: step 7382, loss 0.000667781, acc 1\n",
      "2022-03-25T10:37:29.659570: step 7383, loss 9.16334e-05, acc 1\n",
      "2022-03-25T10:37:29.810760: step 7384, loss 0.000152234, acc 1\n",
      "2022-03-25T10:37:29.958156: step 7385, loss 0.000908792, acc 1\n",
      "2022-03-25T10:37:30.108665: step 7386, loss 0.000108289, acc 1\n",
      "2022-03-25T10:37:30.259875: step 7387, loss 9.60276e-05, acc 1\n",
      "2022-03-25T10:37:30.412966: step 7388, loss 0.00905478, acc 1\n",
      "2022-03-25T10:37:30.566026: step 7389, loss 0.00272492, acc 1\n",
      "2022-03-25T10:37:30.720586: step 7390, loss 5.03214e-05, acc 1\n",
      "2022-03-25T10:37:30.882601: step 7391, loss 5.0045e-05, acc 1\n",
      "2022-03-25T10:37:31.029902: step 7392, loss 1.07665e-05, acc 1\n",
      "2022-03-25T10:37:31.176650: step 7393, loss 0.00102356, acc 1\n",
      "2022-03-25T10:37:31.328285: step 7394, loss 0.0171639, acc 0.984375\n",
      "2022-03-25T10:37:31.476874: step 7395, loss 5.15838e-05, acc 1\n",
      "2022-03-25T10:37:31.630455: step 7396, loss 0.00121323, acc 1\n",
      "2022-03-25T10:37:31.788483: step 7397, loss 6.51048e-05, acc 1\n",
      "2022-03-25T10:37:31.944774: step 7398, loss 0.000100267, acc 1\n",
      "2022-03-25T10:37:32.101171: step 7399, loss 0.00230716, acc 1\n",
      "2022-03-25T10:37:32.248724: step 7400, loss 0.000112859, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:37:32.396568: step 7400, loss 0.743465, acc 0.917151\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7400\n",
      "\n",
      "2022-03-25T10:37:32.653564: step 7401, loss 0.000249438, acc 1\n",
      "2022-03-25T10:37:32.801608: step 7402, loss 0.000182271, acc 1\n",
      "2022-03-25T10:37:32.945864: step 7403, loss 6.20849e-05, acc 1\n",
      "2022-03-25T10:37:33.090988: step 7404, loss 0.0001932, acc 1\n",
      "2022-03-25T10:37:33.232347: step 7405, loss 2.14942e-05, acc 1\n",
      "2022-03-25T10:37:33.378413: step 7406, loss 0.00258559, acc 1\n",
      "2022-03-25T10:37:33.527142: step 7407, loss 0.0302326, acc 0.984375\n",
      "2022-03-25T10:37:33.677692: step 7408, loss 0.000692321, acc 1\n",
      "2022-03-25T10:37:33.828971: step 7409, loss 0.0153335, acc 0.984375\n",
      "2022-03-25T10:37:33.968639: step 7410, loss 0.000126881, acc 1\n",
      "2022-03-25T10:37:34.112179: step 7411, loss 5.80039e-05, acc 1\n",
      "2022-03-25T10:37:34.260782: step 7412, loss 0.00136359, acc 1\n",
      "2022-03-25T10:37:34.408619: step 7413, loss 3.81303e-05, acc 1\n",
      "2022-03-25T10:37:34.552852: step 7414, loss 0.000125966, acc 1\n",
      "2022-03-25T10:37:34.714356: step 7415, loss 0.000431787, acc 1\n",
      "2022-03-25T10:37:34.857958: step 7416, loss 0.00373262, acc 1\n",
      "2022-03-25T10:37:34.993287: step 7417, loss 0.0123113, acc 0.984375\n",
      "2022-03-25T10:37:35.150304: step 7418, loss 0.000360782, acc 1\n",
      "2022-03-25T10:37:35.288548: step 7419, loss 0.000189018, acc 1\n",
      "2022-03-25T10:37:35.448373: step 7420, loss 0.00019087, acc 1\n",
      "2022-03-25T10:37:35.594227: step 7421, loss 0.000689194, acc 1\n",
      "2022-03-25T10:37:35.746614: step 7422, loss 0.00132138, acc 1\n",
      "2022-03-25T10:37:35.881120: step 7423, loss 0.0010128, acc 1\n",
      "2022-03-25T10:37:36.037927: step 7424, loss 0.000158434, acc 1\n",
      "2022-03-25T10:37:36.180158: step 7425, loss 0.00387083, acc 1\n",
      "2022-03-25T10:37:36.324781: step 7426, loss 0.000381294, acc 1\n",
      "2022-03-25T10:37:36.466188: step 7427, loss 0.000190496, acc 1\n",
      "2022-03-25T10:37:36.616559: step 7428, loss 0.000846215, acc 1\n",
      "2022-03-25T10:37:36.780060: step 7429, loss 0.000371359, acc 1\n",
      "2022-03-25T10:37:36.930261: step 7430, loss 0.000138935, acc 1\n",
      "2022-03-25T10:37:37.067541: step 7431, loss 0.00144849, acc 1\n",
      "2022-03-25T10:37:37.216248: step 7432, loss 6.00665e-05, acc 1\n",
      "2022-03-25T10:37:37.354180: step 7433, loss 0.00114809, acc 1\n",
      "2022-03-25T10:37:37.508447: step 7434, loss 7.13906e-05, acc 1\n",
      "2022-03-25T10:37:37.659324: step 7435, loss 4.07791e-05, acc 1\n",
      "2022-03-25T10:37:37.809435: step 7436, loss 0.000187439, acc 1\n",
      "2022-03-25T10:37:37.949949: step 7437, loss 2.35432e-05, acc 1\n",
      "2022-03-25T10:37:38.095421: step 7438, loss 0.000638281, acc 1\n",
      "2022-03-25T10:37:38.239179: step 7439, loss 0.000110773, acc 1\n",
      "2022-03-25T10:37:38.390811: step 7440, loss 0.00054734, acc 1\n",
      "2022-03-25T10:37:38.543444: step 7441, loss 0.000148505, acc 1\n",
      "2022-03-25T10:37:38.683276: step 7442, loss 0.00186177, acc 1\n",
      "2022-03-25T10:37:38.844216: step 7443, loss 0.0136018, acc 0.984375\n",
      "2022-03-25T10:37:38.988427: step 7444, loss 0.000196947, acc 1\n",
      "2022-03-25T10:37:39.139265: step 7445, loss 0.00108811, acc 1\n",
      "2022-03-25T10:37:39.273679: step 7446, loss 9.5136e-05, acc 1\n",
      "2022-03-25T10:37:39.419287: step 7447, loss 0.0172265, acc 0.984375\n",
      "2022-03-25T10:37:39.566575: step 7448, loss 0.000136503, acc 1\n",
      "2022-03-25T10:37:39.705807: step 7449, loss 0.000228078, acc 1\n",
      "2022-03-25T10:37:39.860650: step 7450, loss 0.000494645, acc 1\n",
      "2022-03-25T10:37:40.010300: step 7451, loss 0.00100653, acc 1\n",
      "2022-03-25T10:37:40.160164: step 7452, loss 0.00197631, acc 1\n",
      "2022-03-25T10:37:40.316168: step 7453, loss 3.60032e-05, acc 1\n",
      "2022-03-25T10:37:40.473401: step 7454, loss 0.00299909, acc 1\n",
      "2022-03-25T10:37:40.628596: step 7455, loss 3.67007e-05, acc 1\n",
      "2022-03-25T10:37:40.780063: step 7456, loss 0.000595765, acc 1\n",
      "2022-03-25T10:37:40.939742: step 7457, loss 7.33878e-05, acc 1\n",
      "2022-03-25T10:37:41.085367: step 7458, loss 0.00025448, acc 1\n",
      "2022-03-25T10:37:41.238397: step 7459, loss 0.000517465, acc 1\n",
      "2022-03-25T10:37:41.385954: step 7460, loss 0.00165093, acc 1\n",
      "2022-03-25T10:37:41.534137: step 7461, loss 0.000961301, acc 1\n",
      "2022-03-25T10:37:41.680725: step 7462, loss 0.00016197, acc 1\n",
      "2022-03-25T10:37:41.831015: step 7463, loss 0.000284492, acc 1\n",
      "2022-03-25T10:37:41.983792: step 7464, loss 0.000210613, acc 1\n",
      "2022-03-25T10:37:42.131537: step 7465, loss 0.000130204, acc 1\n",
      "2022-03-25T10:37:42.277298: step 7466, loss 0.000250865, acc 1\n",
      "2022-03-25T10:37:42.417287: step 7467, loss 0.000214631, acc 1\n",
      "2022-03-25T10:37:42.562241: step 7468, loss 4.76372e-05, acc 1\n",
      "2022-03-25T10:37:42.700957: step 7469, loss 0.000106869, acc 1\n",
      "2022-03-25T10:37:42.854718: step 7470, loss 0.000103553, acc 1\n",
      "2022-03-25T10:37:43.005850: step 7471, loss 0.00205045, acc 1\n",
      "2022-03-25T10:37:43.149601: step 7472, loss 0.000493545, acc 1\n",
      "2022-03-25T10:37:43.295297: step 7473, loss 0.00139985, acc 1\n",
      "2022-03-25T10:37:43.438993: step 7474, loss 0.000172924, acc 1\n",
      "2022-03-25T10:37:43.585836: step 7475, loss 0.000429929, acc 1\n",
      "2022-03-25T10:37:43.728554: step 7476, loss 0.00332085, acc 1\n",
      "2022-03-25T10:37:43.876232: step 7477, loss 0.00432097, acc 1\n",
      "2022-03-25T10:37:44.024418: step 7478, loss 6.11706e-05, acc 1\n",
      "2022-03-25T10:37:44.169014: step 7479, loss 5.83782e-05, acc 1\n",
      "2022-03-25T10:37:44.318896: step 7480, loss 0.000150312, acc 1\n",
      "2022-03-25T10:37:44.461345: step 7481, loss 0.00082594, acc 1\n",
      "2022-03-25T10:37:44.609653: step 7482, loss 0.000207619, acc 1\n",
      "2022-03-25T10:37:44.746393: step 7483, loss 2.32512e-05, acc 1\n",
      "2022-03-25T10:37:44.907763: step 7484, loss 0.000222929, acc 1\n",
      "2022-03-25T10:37:45.056936: step 7485, loss 2.26292e-05, acc 1\n",
      "2022-03-25T10:37:45.197585: step 7486, loss 0.000537698, acc 1\n",
      "2022-03-25T10:37:45.341557: step 7487, loss 0.0100466, acc 1\n",
      "2022-03-25T10:37:45.488333: step 7488, loss 7.25586e-05, acc 1\n",
      "2022-03-25T10:37:45.627991: step 7489, loss 0.000628925, acc 1\n",
      "2022-03-25T10:37:45.777468: step 7490, loss 0.00182243, acc 1\n",
      "2022-03-25T10:37:45.922872: step 7491, loss 0.00169619, acc 1\n",
      "2022-03-25T10:37:46.067424: step 7492, loss 0.000281624, acc 1\n",
      "2022-03-25T10:37:46.206522: step 7493, loss 0.000110944, acc 1\n",
      "2022-03-25T10:37:46.354551: step 7494, loss 5.10854e-05, acc 1\n",
      "2022-03-25T10:37:46.498216: step 7495, loss 0.00202337, acc 1\n",
      "2022-03-25T10:37:46.645137: step 7496, loss 4.744e-05, acc 1\n",
      "2022-03-25T10:37:46.793559: step 7497, loss 0.000806421, acc 1\n",
      "2022-03-25T10:37:46.945789: step 7498, loss 0.000828101, acc 1\n",
      "2022-03-25T10:37:47.087889: step 7499, loss 0.000253349, acc 1\n",
      "2022-03-25T10:37:47.231071: step 7500, loss 5.49314e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:37:47.366536: step 7500, loss 0.668558, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7500\n",
      "\n",
      "2022-03-25T10:37:47.609390: step 7501, loss 0.000343938, acc 1\n",
      "2022-03-25T10:37:47.760652: step 7502, loss 8.3793e-06, acc 1\n",
      "2022-03-25T10:37:47.915302: step 7503, loss 0.000160843, acc 1\n",
      "2022-03-25T10:37:48.069304: step 7504, loss 0.000129508, acc 1\n",
      "2022-03-25T10:37:48.205648: step 7505, loss 0.000995264, acc 1\n",
      "2022-03-25T10:37:48.358151: step 7506, loss 0.0012712, acc 1\n",
      "2022-03-25T10:37:48.511589: step 7507, loss 0.0237675, acc 0.984375\n",
      "2022-03-25T10:37:48.664170: step 7508, loss 7.75921e-05, acc 1\n",
      "2022-03-25T10:37:48.812657: step 7509, loss 0.00409038, acc 1\n",
      "2022-03-25T10:37:48.963270: step 7510, loss 0.00147096, acc 1\n",
      "2022-03-25T10:37:49.109415: step 7511, loss 0.00165106, acc 1\n",
      "2022-03-25T10:37:49.256620: step 7512, loss 0.000236686, acc 1\n",
      "2022-03-25T10:37:49.400111: step 7513, loss 0.000255781, acc 1\n",
      "2022-03-25T10:37:49.556813: step 7514, loss 7.90224e-05, acc 1\n",
      "2022-03-25T10:37:49.705938: step 7515, loss 0.000155029, acc 1\n",
      "2022-03-25T10:37:49.851648: step 7516, loss 0.0194927, acc 0.984375\n",
      "2022-03-25T10:37:49.997631: step 7517, loss 0.000404258, acc 1\n",
      "2022-03-25T10:37:50.142384: step 7518, loss 0.00261995, acc 1\n",
      "2022-03-25T10:37:50.288817: step 7519, loss 0.00128252, acc 1\n",
      "2022-03-25T10:37:50.432244: step 7520, loss 0.00113321, acc 1\n",
      "2022-03-25T10:37:50.578245: step 7521, loss 0.000252435, acc 1\n",
      "2022-03-25T10:37:50.738951: step 7522, loss 0.00062825, acc 1\n",
      "2022-03-25T10:37:50.878686: step 7523, loss 0.000205826, acc 1\n",
      "2022-03-25T10:37:51.027571: step 7524, loss 0.00174089, acc 1\n",
      "2022-03-25T10:37:51.185227: step 7525, loss 0.000185967, acc 1\n",
      "2022-03-25T10:37:51.323026: step 7526, loss 0.000115866, acc 1\n",
      "2022-03-25T10:37:51.456987: step 7527, loss 5.58136e-05, acc 1\n",
      "2022-03-25T10:37:51.604428: step 7528, loss 0.000155654, acc 1\n",
      "2022-03-25T10:37:51.752008: step 7529, loss 0.00109737, acc 1\n",
      "2022-03-25T10:37:51.897961: step 7530, loss 0.00050651, acc 1\n",
      "2022-03-25T10:37:52.048435: step 7531, loss 8.25186e-05, acc 1\n",
      "2022-03-25T10:37:52.193252: step 7532, loss 0.00393933, acc 1\n",
      "2022-03-25T10:37:52.336966: step 7533, loss 7.34868e-05, acc 1\n",
      "2022-03-25T10:37:52.485667: step 7534, loss 0.00796235, acc 1\n",
      "2022-03-25T10:37:52.635595: step 7535, loss 0.000454874, acc 1\n",
      "2022-03-25T10:37:52.786704: step 7536, loss 0.00011888, acc 1\n",
      "2022-03-25T10:37:52.932490: step 7537, loss 2.11239e-05, acc 1\n",
      "2022-03-25T10:37:53.071699: step 7538, loss 2.77218e-05, acc 1\n",
      "2022-03-25T10:37:53.207165: step 7539, loss 0.000261892, acc 1\n",
      "2022-03-25T10:37:53.352810: step 7540, loss 0.000162168, acc 1\n",
      "2022-03-25T10:37:53.502724: step 7541, loss 0.000365232, acc 1\n",
      "2022-03-25T10:37:53.657801: step 7542, loss 3.02905e-05, acc 1\n",
      "2022-03-25T10:37:53.804528: step 7543, loss 3.84509e-05, acc 1\n",
      "2022-03-25T10:37:53.951620: step 7544, loss 0.000130901, acc 1\n",
      "2022-03-25T10:37:54.102968: step 7545, loss 5.09822e-05, acc 1\n",
      "2022-03-25T10:37:54.245787: step 7546, loss 0.00122779, acc 1\n",
      "2022-03-25T10:37:54.398464: step 7547, loss 4.13973e-05, acc 1\n",
      "2022-03-25T10:37:54.544827: step 7548, loss 0.00069589, acc 1\n",
      "2022-03-25T10:37:54.696031: step 7549, loss 4.61345e-05, acc 1\n",
      "2022-03-25T10:37:54.845235: step 7550, loss 0.00533969, acc 1\n",
      "2022-03-25T10:37:55.002211: step 7551, loss 0.000258669, acc 1\n",
      "2022-03-25T10:37:55.170278: step 7552, loss 0.00251748, acc 1\n",
      "2022-03-25T10:37:55.327837: step 7553, loss 0.000106704, acc 1\n",
      "2022-03-25T10:37:55.481391: step 7554, loss 0.0016056, acc 1\n",
      "2022-03-25T10:37:55.631285: step 7555, loss 4.78503e-05, acc 1\n",
      "2022-03-25T10:37:55.780893: step 7556, loss 0.000274258, acc 1\n",
      "2022-03-25T10:37:55.942385: step 7557, loss 0.00301583, acc 1\n",
      "2022-03-25T10:37:56.096285: step 7558, loss 4.03598e-05, acc 1\n",
      "2022-03-25T10:37:56.248389: step 7559, loss 0.00148243, acc 1\n",
      "2022-03-25T10:37:56.395691: step 7560, loss 0.000319117, acc 1\n",
      "2022-03-25T10:37:56.548377: step 7561, loss 2.92661e-05, acc 1\n",
      "2022-03-25T10:37:56.698827: step 7562, loss 0.000398071, acc 1\n",
      "2022-03-25T10:37:56.851465: step 7563, loss 2.00235e-05, acc 1\n",
      "2022-03-25T10:37:56.999734: step 7564, loss 0.00178616, acc 1\n",
      "2022-03-25T10:37:57.153442: step 7565, loss 0.000309621, acc 1\n",
      "2022-03-25T10:37:57.276469: step 7566, loss 0.000255654, acc 1\n",
      "2022-03-25T10:37:57.430708: step 7567, loss 0.000476997, acc 1\n",
      "2022-03-25T10:37:57.578135: step 7568, loss 0.000271147, acc 1\n",
      "2022-03-25T10:37:57.720565: step 7569, loss 0.000401907, acc 1\n",
      "2022-03-25T10:37:57.880408: step 7570, loss 0.000332747, acc 1\n",
      "2022-03-25T10:37:58.025982: step 7571, loss 0.000215528, acc 1\n",
      "2022-03-25T10:37:58.177747: step 7572, loss 0.000706774, acc 1\n",
      "2022-03-25T10:37:58.325292: step 7573, loss 0.000273104, acc 1\n",
      "2022-03-25T10:37:58.475507: step 7574, loss 9.79412e-05, acc 1\n",
      "2022-03-25T10:37:58.627431: step 7575, loss 0.000576476, acc 1\n",
      "2022-03-25T10:37:58.788867: step 7576, loss 0.000123184, acc 1\n",
      "2022-03-25T10:37:58.939701: step 7577, loss 0.00131965, acc 1\n",
      "2022-03-25T10:37:59.085373: step 7578, loss 0.00144075, acc 1\n",
      "2022-03-25T10:37:59.239269: step 7579, loss 0.000705995, acc 1\n",
      "2022-03-25T10:37:59.397765: step 7580, loss 0.00188955, acc 1\n",
      "2022-03-25T10:37:59.562311: step 7581, loss 0.000350742, acc 1\n",
      "2022-03-25T10:37:59.721440: step 7582, loss 5.33601e-05, acc 1\n",
      "2022-03-25T10:37:59.869961: step 7583, loss 5.21575e-05, acc 1\n",
      "2022-03-25T10:38:00.010710: step 7584, loss 0.000411167, acc 1\n",
      "2022-03-25T10:38:00.156722: step 7585, loss 0.000464113, acc 1\n",
      "2022-03-25T10:38:00.314280: step 7586, loss 7.97702e-05, acc 1\n",
      "2022-03-25T10:38:00.467971: step 7587, loss 0.000749385, acc 1\n",
      "2022-03-25T10:38:00.614984: step 7588, loss 3.01041e-05, acc 1\n",
      "2022-03-25T10:38:00.764758: step 7589, loss 0.000356923, acc 1\n",
      "2022-03-25T10:38:00.907951: step 7590, loss 0.000983276, acc 1\n",
      "2022-03-25T10:38:01.059705: step 7591, loss 0.000207051, acc 1\n",
      "2022-03-25T10:38:01.215574: step 7592, loss 0.000982145, acc 1\n",
      "2022-03-25T10:38:01.385854: step 7593, loss 0.000594176, acc 1\n",
      "2022-03-25T10:38:01.544206: step 7594, loss 7.34962e-05, acc 1\n",
      "2022-03-25T10:38:01.708549: step 7595, loss 0.000156189, acc 1\n",
      "2022-03-25T10:38:01.854841: step 7596, loss 0.00921503, acc 1\n",
      "2022-03-25T10:38:02.007576: step 7597, loss 5.1069e-05, acc 1\n",
      "2022-03-25T10:38:02.158558: step 7598, loss 3.82043e-05, acc 1\n",
      "2022-03-25T10:38:02.316291: step 7599, loss 0.000154525, acc 1\n",
      "2022-03-25T10:38:02.476402: step 7600, loss 0.000130343, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:38:02.623317: step 7600, loss 0.687512, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7600\n",
      "\n",
      "2022-03-25T10:38:02.865738: step 7601, loss 3.54732e-05, acc 1\n",
      "2022-03-25T10:38:03.020264: step 7602, loss 5.16773e-05, acc 1\n",
      "2022-03-25T10:38:03.169510: step 7603, loss 2.89742e-05, acc 1\n",
      "2022-03-25T10:38:03.308570: step 7604, loss 0.000474299, acc 1\n",
      "2022-03-25T10:38:03.450090: step 7605, loss 0.000113393, acc 1\n",
      "2022-03-25T10:38:03.602673: step 7606, loss 0.0036076, acc 1\n",
      "2022-03-25T10:38:03.756426: step 7607, loss 0.000227634, acc 1\n",
      "2022-03-25T10:38:03.896778: step 7608, loss 8.42613e-05, acc 1\n",
      "2022-03-25T10:38:04.042305: step 7609, loss 2.5722e-05, acc 1\n",
      "2022-03-25T10:38:04.192704: step 7610, loss 0.000101765, acc 1\n",
      "2022-03-25T10:38:04.331023: step 7611, loss 9.66715e-05, acc 1\n",
      "2022-03-25T10:38:04.474286: step 7612, loss 7.32033e-05, acc 1\n",
      "2022-03-25T10:38:04.626408: step 7613, loss 0.00920677, acc 1\n",
      "2022-03-25T10:38:04.782252: step 7614, loss 0.000183833, acc 1\n",
      "2022-03-25T10:38:04.938852: step 7615, loss 6.79455e-05, acc 1\n",
      "2022-03-25T10:38:05.087674: step 7616, loss 0.000546389, acc 1\n",
      "2022-03-25T10:38:05.231886: step 7617, loss 0.000353585, acc 1\n",
      "2022-03-25T10:38:05.382323: step 7618, loss 0.000238015, acc 1\n",
      "2022-03-25T10:38:05.528382: step 7619, loss 0.000317962, acc 1\n",
      "2022-03-25T10:38:05.682264: step 7620, loss 0.00174233, acc 1\n",
      "2022-03-25T10:38:05.834513: step 7621, loss 0.00012297, acc 1\n",
      "2022-03-25T10:38:05.981674: step 7622, loss 6.57808e-05, acc 1\n",
      "2022-03-25T10:38:06.131882: step 7623, loss 9.23646e-05, acc 1\n",
      "2022-03-25T10:38:06.276786: step 7624, loss 0.000226196, acc 1\n",
      "2022-03-25T10:38:06.420996: step 7625, loss 3.35566e-05, acc 1\n",
      "2022-03-25T10:38:06.569268: step 7626, loss 1.03825e-05, acc 1\n",
      "2022-03-25T10:38:06.712466: step 7627, loss 0.00114747, acc 1\n",
      "2022-03-25T10:38:06.873942: step 7628, loss 9.12589e-05, acc 1\n",
      "2022-03-25T10:38:07.029616: step 7629, loss 0.000307036, acc 1\n",
      "2022-03-25T10:38:07.183085: step 7630, loss 0.000618611, acc 1\n",
      "2022-03-25T10:38:07.321263: step 7631, loss 2.36151e-05, acc 1\n",
      "2022-03-25T10:38:07.483792: step 7632, loss 5.68764e-05, acc 1\n",
      "2022-03-25T10:38:07.630530: step 7633, loss 8.5914e-05, acc 1\n",
      "2022-03-25T10:38:07.771547: step 7634, loss 1.14527e-05, acc 1\n",
      "2022-03-25T10:38:07.909592: step 7635, loss 0.000213695, acc 1\n",
      "2022-03-25T10:38:08.059454: step 7636, loss 4.14247e-06, acc 1\n",
      "2022-03-25T10:38:08.203159: step 7637, loss 0.000706295, acc 1\n",
      "2022-03-25T10:38:08.349026: step 7638, loss 0.000214943, acc 1\n",
      "2022-03-25T10:38:08.498174: step 7639, loss 6.25818e-05, acc 1\n",
      "2022-03-25T10:38:08.652037: step 7640, loss 5.44634e-05, acc 1\n",
      "2022-03-25T10:38:08.803494: step 7641, loss 7.9071e-05, acc 1\n",
      "2022-03-25T10:38:08.963708: step 7642, loss 0.000263571, acc 1\n",
      "2022-03-25T10:38:09.112267: step 7643, loss 0.052324, acc 0.984375\n",
      "2022-03-25T10:38:09.252181: step 7644, loss 0.00100411, acc 1\n",
      "2022-03-25T10:38:09.396788: step 7645, loss 2.84485e-05, acc 1\n",
      "2022-03-25T10:38:09.553708: step 7646, loss 2.80282e-05, acc 1\n",
      "2022-03-25T10:38:09.700701: step 7647, loss 3.65484e-05, acc 1\n",
      "2022-03-25T10:38:09.855417: step 7648, loss 0.000707421, acc 1\n",
      "2022-03-25T10:38:09.997847: step 7649, loss 0.000681253, acc 1\n",
      "2022-03-25T10:38:10.147291: step 7650, loss 0.00112916, acc 1\n",
      "2022-03-25T10:38:10.293225: step 7651, loss 0.00116677, acc 1\n",
      "2022-03-25T10:38:10.432172: step 7652, loss 0.000579504, acc 1\n",
      "2022-03-25T10:38:10.588876: step 7653, loss 0.00033037, acc 1\n",
      "2022-03-25T10:38:10.733585: step 7654, loss 0.000309014, acc 1\n",
      "2022-03-25T10:38:10.881974: step 7655, loss 0.00225018, acc 1\n",
      "2022-03-25T10:38:11.034467: step 7656, loss 0.00208227, acc 1\n",
      "2022-03-25T10:38:11.197615: step 7657, loss 0.000175141, acc 1\n",
      "2022-03-25T10:38:11.348658: step 7658, loss 0.000693672, acc 1\n",
      "2022-03-25T10:38:11.499908: step 7659, loss 0.000418278, acc 1\n",
      "2022-03-25T10:38:11.651999: step 7660, loss 0.000816534, acc 1\n",
      "2022-03-25T10:38:11.808684: step 7661, loss 0.000142184, acc 1\n",
      "2022-03-25T10:38:11.958426: step 7662, loss 0.002277, acc 1\n",
      "2022-03-25T10:38:12.109850: step 7663, loss 0.000372707, acc 1\n",
      "2022-03-25T10:38:12.252523: step 7664, loss 5.90826e-05, acc 1\n",
      "2022-03-25T10:38:12.403633: step 7665, loss 0.000129836, acc 1\n",
      "2022-03-25T10:38:12.547158: step 7666, loss 0.000286036, acc 1\n",
      "2022-03-25T10:38:12.698450: step 7667, loss 2.85859e-05, acc 1\n",
      "2022-03-25T10:38:12.846511: step 7668, loss 0.000617592, acc 1\n",
      "2022-03-25T10:38:12.993562: step 7669, loss 7.47837e-05, acc 1\n",
      "2022-03-25T10:38:13.136503: step 7670, loss 0.000839829, acc 1\n",
      "2022-03-25T10:38:13.279045: step 7671, loss 0.000374293, acc 1\n",
      "2022-03-25T10:38:13.428889: step 7672, loss 0.0395782, acc 0.984375\n",
      "2022-03-25T10:38:13.582838: step 7673, loss 0.00188063, acc 1\n",
      "2022-03-25T10:38:13.738423: step 7674, loss 0.000151738, acc 1\n",
      "2022-03-25T10:38:13.878100: step 7675, loss 0.000292492, acc 1\n",
      "2022-03-25T10:38:14.020274: step 7676, loss 0.000793714, acc 1\n",
      "2022-03-25T10:38:14.168028: step 7677, loss 6.1404e-05, acc 1\n",
      "2022-03-25T10:38:14.312714: step 7678, loss 0.000116872, acc 1\n",
      "2022-03-25T10:38:14.455824: step 7679, loss 0.000108926, acc 1\n",
      "2022-03-25T10:38:14.600077: step 7680, loss 9.05395e-06, acc 1\n",
      "2022-03-25T10:38:14.755845: step 7681, loss 0.000240958, acc 1\n",
      "2022-03-25T10:38:14.908674: step 7682, loss 6.51066e-05, acc 1\n",
      "2022-03-25T10:38:15.056836: step 7683, loss 0.0310022, acc 0.984375\n",
      "2022-03-25T10:38:15.209020: step 7684, loss 0.00202, acc 1\n",
      "2022-03-25T10:38:15.366436: step 7685, loss 7.28707e-05, acc 1\n",
      "2022-03-25T10:38:15.512822: step 7686, loss 0.0383953, acc 0.984375\n",
      "2022-03-25T10:38:15.667261: step 7687, loss 7.75489e-05, acc 1\n",
      "2022-03-25T10:38:15.811384: step 7688, loss 0.000450611, acc 1\n",
      "2022-03-25T10:38:15.965231: step 7689, loss 0.00172567, acc 1\n",
      "2022-03-25T10:38:16.115098: step 7690, loss 0.000675554, acc 1\n",
      "2022-03-25T10:38:16.258191: step 7691, loss 0.000101477, acc 1\n",
      "2022-03-25T10:38:16.404063: step 7692, loss 0.00184089, acc 1\n",
      "2022-03-25T10:38:16.551353: step 7693, loss 0.00106825, acc 1\n",
      "2022-03-25T10:38:16.700818: step 7694, loss 0.000455165, acc 1\n",
      "2022-03-25T10:38:16.851173: step 7695, loss 0.00418832, acc 1\n",
      "2022-03-25T10:38:16.994695: step 7696, loss 0.000131574, acc 1\n",
      "2022-03-25T10:38:17.144482: step 7697, loss 0.00156397, acc 1\n",
      "2022-03-25T10:38:17.284479: step 7698, loss 0.00018713, acc 1\n",
      "2022-03-25T10:38:17.432302: step 7699, loss 0.000515728, acc 1\n",
      "2022-03-25T10:38:17.575530: step 7700, loss 0.000300905, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:38:17.710169: step 7700, loss 0.655355, acc 0.914244\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7700\n",
      "\n",
      "2022-03-25T10:38:17.963041: step 7701, loss 0.00217979, acc 1\n",
      "2022-03-25T10:38:18.106544: step 7702, loss 0.000487175, acc 1\n",
      "2022-03-25T10:38:18.252011: step 7703, loss 9.08326e-05, acc 1\n",
      "2022-03-25T10:38:18.398601: step 7704, loss 0.0036949, acc 1\n",
      "2022-03-25T10:38:18.542723: step 7705, loss 0.00074825, acc 1\n",
      "2022-03-25T10:38:18.697603: step 7706, loss 0.000158147, acc 1\n",
      "2022-03-25T10:38:18.854905: step 7707, loss 0.000467563, acc 1\n",
      "2022-03-25T10:38:18.998775: step 7708, loss 0.000325512, acc 1\n",
      "2022-03-25T10:38:19.160407: step 7709, loss 7.13017e-05, acc 1\n",
      "2022-03-25T10:38:19.311302: step 7710, loss 0.000891836, acc 1\n",
      "2022-03-25T10:38:19.453612: step 7711, loss 0.000792472, acc 1\n",
      "2022-03-25T10:38:19.601188: step 7712, loss 0.00167056, acc 1\n",
      "2022-03-25T10:38:19.751688: step 7713, loss 0.000112139, acc 1\n",
      "2022-03-25T10:38:19.908243: step 7714, loss 0.00198659, acc 1\n",
      "2022-03-25T10:38:20.050875: step 7715, loss 0.00023257, acc 1\n",
      "2022-03-25T10:38:20.196632: step 7716, loss 0.00035027, acc 1\n",
      "2022-03-25T10:38:20.333690: step 7717, loss 5.90565e-05, acc 1\n",
      "2022-03-25T10:38:20.486284: step 7718, loss 0.0016875, acc 1\n",
      "2022-03-25T10:38:20.629653: step 7719, loss 0.00030158, acc 1\n",
      "2022-03-25T10:38:20.784946: step 7720, loss 4.52482e-05, acc 1\n",
      "2022-03-25T10:38:20.927924: step 7721, loss 0.00829507, acc 1\n",
      "2022-03-25T10:38:21.069960: step 7722, loss 0.00414452, acc 1\n",
      "2022-03-25T10:38:21.218305: step 7723, loss 8.71953e-05, acc 1\n",
      "2022-03-25T10:38:21.375839: step 7724, loss 2.45931e-05, acc 1\n",
      "2022-03-25T10:38:21.542244: step 7725, loss 0.000137703, acc 1\n",
      "2022-03-25T10:38:21.690396: step 7726, loss 2.52031e-05, acc 1\n",
      "2022-03-25T10:38:21.854297: step 7727, loss 0.000221144, acc 1\n",
      "2022-03-25T10:38:22.002246: step 7728, loss 0.000466706, acc 1\n",
      "2022-03-25T10:38:22.145229: step 7729, loss 8.00192e-05, acc 1\n",
      "2022-03-25T10:38:22.288380: step 7730, loss 0.000236026, acc 1\n",
      "2022-03-25T10:38:22.430169: step 7731, loss 0.000767108, acc 1\n",
      "2022-03-25T10:38:22.578619: step 7732, loss 1.51469e-05, acc 1\n",
      "2022-03-25T10:38:22.734248: step 7733, loss 2.44306e-05, acc 1\n",
      "2022-03-25T10:38:22.897463: step 7734, loss 1.79507e-05, acc 1\n",
      "2022-03-25T10:38:23.042210: step 7735, loss 0.00100211, acc 1\n",
      "2022-03-25T10:38:23.194457: step 7736, loss 1.05923e-05, acc 1\n",
      "2022-03-25T10:38:23.337726: step 7737, loss 5.23675e-05, acc 1\n",
      "2022-03-25T10:38:23.482747: step 7738, loss 7.08344e-05, acc 1\n",
      "2022-03-25T10:38:23.624673: step 7739, loss 0.00021522, acc 1\n",
      "2022-03-25T10:38:23.777053: step 7740, loss 0.00303139, acc 1\n",
      "2022-03-25T10:38:23.935716: step 7741, loss 0.000203452, acc 1\n",
      "2022-03-25T10:38:24.077311: step 7742, loss 0.000627411, acc 1\n",
      "2022-03-25T10:38:24.233172: step 7743, loss 0.000101003, acc 1\n",
      "2022-03-25T10:38:24.381827: step 7744, loss 0.00390943, acc 1\n",
      "2022-03-25T10:38:24.542691: step 7745, loss 0.00961194, acc 1\n",
      "2022-03-25T10:38:24.682867: step 7746, loss 1.71537e-05, acc 1\n",
      "2022-03-25T10:38:24.828998: step 7747, loss 5.05754e-05, acc 1\n",
      "2022-03-25T10:38:24.977020: step 7748, loss 0.000158076, acc 1\n",
      "2022-03-25T10:38:25.116737: step 7749, loss 0.00104343, acc 1\n",
      "2022-03-25T10:38:25.274633: step 7750, loss 0.00240539, acc 1\n",
      "2022-03-25T10:38:25.426508: step 7751, loss 0.0002073, acc 1\n",
      "2022-03-25T10:38:25.573965: step 7752, loss 0.000110351, acc 1\n",
      "2022-03-25T10:38:25.724280: step 7753, loss 0.000105857, acc 1\n",
      "2022-03-25T10:38:25.865945: step 7754, loss 0.00015431, acc 1\n",
      "2022-03-25T10:38:26.017977: step 7755, loss 3.38729e-05, acc 1\n",
      "2022-03-25T10:38:26.164623: step 7756, loss 0.000341562, acc 1\n",
      "2022-03-25T10:38:26.308090: step 7757, loss 5.66087e-05, acc 1\n",
      "2022-03-25T10:38:26.460949: step 7758, loss 0.000658907, acc 1\n",
      "2022-03-25T10:38:26.611878: step 7759, loss 0.000160338, acc 1\n",
      "2022-03-25T10:38:26.736172: step 7760, loss 0.008118, acc 1\n",
      "2022-03-25T10:38:26.886689: step 7761, loss 0.00023291, acc 1\n",
      "2022-03-25T10:38:27.029922: step 7762, loss 0.000210544, acc 1\n",
      "2022-03-25T10:38:27.179984: step 7763, loss 1.31325e-05, acc 1\n",
      "2022-03-25T10:38:27.329573: step 7764, loss 3.99335e-05, acc 1\n",
      "2022-03-25T10:38:27.486037: step 7765, loss 4.72378e-05, acc 1\n",
      "2022-03-25T10:38:27.637924: step 7766, loss 1.57759e-05, acc 1\n",
      "2022-03-25T10:38:27.791914: step 7767, loss 0.000139832, acc 1\n",
      "2022-03-25T10:38:27.951639: step 7768, loss 0.00349513, acc 1\n",
      "2022-03-25T10:38:28.102207: step 7769, loss 5.88697e-05, acc 1\n",
      "2022-03-25T10:38:28.241803: step 7770, loss 0.00111931, acc 1\n",
      "2022-03-25T10:38:28.383810: step 7771, loss 0.00118496, acc 1\n",
      "2022-03-25T10:38:28.530752: step 7772, loss 7.50265e-05, acc 1\n",
      "2022-03-25T10:38:28.682955: step 7773, loss 0.00599346, acc 1\n",
      "2022-03-25T10:38:28.838935: step 7774, loss 2.24272e-05, acc 1\n",
      "2022-03-25T10:38:28.990634: step 7775, loss 0.000129496, acc 1\n",
      "2022-03-25T10:38:29.135686: step 7776, loss 0.020859, acc 0.984375\n",
      "2022-03-25T10:38:29.285172: step 7777, loss 0.00275659, acc 1\n",
      "2022-03-25T10:38:29.431678: step 7778, loss 0.000132329, acc 1\n",
      "2022-03-25T10:38:29.586989: step 7779, loss 0.00470709, acc 1\n",
      "2022-03-25T10:38:29.735262: step 7780, loss 0.000111938, acc 1\n",
      "2022-03-25T10:38:29.876170: step 7781, loss 0.0011181, acc 1\n",
      "2022-03-25T10:38:30.027883: step 7782, loss 3.04277e-05, acc 1\n",
      "2022-03-25T10:38:30.169025: step 7783, loss 4.89702e-05, acc 1\n",
      "2022-03-25T10:38:30.316066: step 7784, loss 3.85903e-05, acc 1\n",
      "2022-03-25T10:38:30.460886: step 7785, loss 1.82825e-05, acc 1\n",
      "2022-03-25T10:38:30.608162: step 7786, loss 0.0428077, acc 0.984375\n",
      "2022-03-25T10:38:30.747480: step 7787, loss 0.000122704, acc 1\n",
      "2022-03-25T10:38:30.902337: step 7788, loss 0.00228687, acc 1\n",
      "2022-03-25T10:38:31.051787: step 7789, loss 0.000305672, acc 1\n",
      "2022-03-25T10:38:31.197168: step 7790, loss 5.21924e-05, acc 1\n",
      "2022-03-25T10:38:31.338821: step 7791, loss 9.35712e-06, acc 1\n",
      "2022-03-25T10:38:31.489219: step 7792, loss 0.000319181, acc 1\n",
      "2022-03-25T10:38:31.626596: step 7793, loss 1.92287e-05, acc 1\n",
      "2022-03-25T10:38:31.776019: step 7794, loss 1.90423e-05, acc 1\n",
      "2022-03-25T10:38:31.915985: step 7795, loss 0.000141712, acc 1\n",
      "2022-03-25T10:38:32.071574: step 7796, loss 0.0027517, acc 1\n",
      "2022-03-25T10:38:32.220239: step 7797, loss 0.00091407, acc 1\n",
      "2022-03-25T10:38:32.363385: step 7798, loss 0.00185696, acc 1\n",
      "2022-03-25T10:38:32.497450: step 7799, loss 0.000187178, acc 1\n",
      "2022-03-25T10:38:32.650422: step 7800, loss 0.000264856, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:38:32.786273: step 7800, loss 0.662979, acc 0.915698\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7800\n",
      "\n",
      "2022-03-25T10:38:33.032391: step 7801, loss 6.10074e-05, acc 1\n",
      "2022-03-25T10:38:33.181307: step 7802, loss 0.0131376, acc 0.984375\n",
      "2022-03-25T10:38:33.322174: step 7803, loss 0.000494308, acc 1\n",
      "2022-03-25T10:38:33.479919: step 7804, loss 0.000944378, acc 1\n",
      "2022-03-25T10:38:33.618121: step 7805, loss 4.85652e-05, acc 1\n",
      "2022-03-25T10:38:33.769492: step 7806, loss 6.31749e-05, acc 1\n",
      "2022-03-25T10:38:33.913890: step 7807, loss 0.000122415, acc 1\n",
      "2022-03-25T10:38:34.064932: step 7808, loss 0.000140721, acc 1\n",
      "2022-03-25T10:38:34.209921: step 7809, loss 0.000401592, acc 1\n",
      "2022-03-25T10:38:34.352118: step 7810, loss 6.97341e-05, acc 1\n",
      "2022-03-25T10:38:34.482550: step 7811, loss 1.8251e-05, acc 1\n",
      "2022-03-25T10:38:34.627351: step 7812, loss 2.56115e-05, acc 1\n",
      "2022-03-25T10:38:34.761008: step 7813, loss 9.09612e-05, acc 1\n",
      "2022-03-25T10:38:34.912759: step 7814, loss 0.00195932, acc 1\n",
      "2022-03-25T10:38:35.049710: step 7815, loss 9.8478e-05, acc 1\n",
      "2022-03-25T10:38:35.197614: step 7816, loss 0.000934803, acc 1\n",
      "2022-03-25T10:38:35.327815: step 7817, loss 4.52631e-05, acc 1\n",
      "2022-03-25T10:38:35.470453: step 7818, loss 2.73543e-05, acc 1\n",
      "2022-03-25T10:38:35.606218: step 7819, loss 0.000123462, acc 1\n",
      "2022-03-25T10:38:35.753714: step 7820, loss 4.47922e-05, acc 1\n",
      "2022-03-25T10:38:35.886885: step 7821, loss 0.00461572, acc 1\n",
      "2022-03-25T10:38:36.035292: step 7822, loss 3.0992e-05, acc 1\n",
      "2022-03-25T10:38:36.186274: step 7823, loss 6.39807e-05, acc 1\n",
      "2022-03-25T10:38:36.327399: step 7824, loss 2.82594e-05, acc 1\n",
      "2022-03-25T10:38:36.457691: step 7825, loss 0.000188875, acc 1\n",
      "2022-03-25T10:38:36.612389: step 7826, loss 4.62195e-05, acc 1\n",
      "2022-03-25T10:38:36.750548: step 7827, loss 0.00485513, acc 1\n",
      "2022-03-25T10:38:36.906748: step 7828, loss 0.000139688, acc 1\n",
      "2022-03-25T10:38:37.041470: step 7829, loss 0.000297615, acc 1\n",
      "2022-03-25T10:38:37.200542: step 7830, loss 6.90609e-06, acc 1\n",
      "2022-03-25T10:38:37.333432: step 7831, loss 0.00206505, acc 1\n",
      "2022-03-25T10:38:37.481773: step 7832, loss 0.00111272, acc 1\n",
      "2022-03-25T10:38:37.617133: step 7833, loss 0.000115043, acc 1\n",
      "2022-03-25T10:38:37.762243: step 7834, loss 0.0179559, acc 0.984375\n",
      "2022-03-25T10:38:37.900205: step 7835, loss 7.16065e-05, acc 1\n",
      "2022-03-25T10:38:38.048042: step 7836, loss 1.16812e-05, acc 1\n",
      "2022-03-25T10:38:38.194381: step 7837, loss 0.000201087, acc 1\n",
      "2022-03-25T10:38:38.331908: step 7838, loss 4.96344e-05, acc 1\n",
      "2022-03-25T10:38:38.468191: step 7839, loss 0.000595716, acc 1\n",
      "2022-03-25T10:38:38.608239: step 7840, loss 6.30458e-05, acc 1\n",
      "2022-03-25T10:38:38.742991: step 7841, loss 0.000153419, acc 1\n",
      "2022-03-25T10:38:38.903587: step 7842, loss 0.000134825, acc 1\n",
      "2022-03-25T10:38:39.041265: step 7843, loss 2.27042e-05, acc 1\n",
      "2022-03-25T10:38:39.195255: step 7844, loss 5.39285e-05, acc 1\n",
      "2022-03-25T10:38:39.331697: step 7845, loss 0.000122295, acc 1\n",
      "2022-03-25T10:38:39.487811: step 7846, loss 0.000160131, acc 1\n",
      "2022-03-25T10:38:39.632930: step 7847, loss 5.03305e-05, acc 1\n",
      "2022-03-25T10:38:39.781418: step 7848, loss 7.80796e-05, acc 1\n",
      "2022-03-25T10:38:39.928862: step 7849, loss 0.000160127, acc 1\n",
      "2022-03-25T10:38:40.069721: step 7850, loss 1.5126e-05, acc 1\n",
      "2022-03-25T10:38:40.218335: step 7851, loss 0.0560494, acc 0.984375\n",
      "2022-03-25T10:38:40.360264: step 7852, loss 0.00230801, acc 1\n",
      "2022-03-25T10:38:40.501250: step 7853, loss 1.86951e-05, acc 1\n",
      "2022-03-25T10:38:40.653177: step 7854, loss 9.8246e-06, acc 1\n",
      "2022-03-25T10:38:40.797994: step 7855, loss 3.07828e-05, acc 1\n",
      "2022-03-25T10:38:40.951438: step 7856, loss 0.00346482, acc 1\n",
      "2022-03-25T10:38:41.097245: step 7857, loss 0.00347785, acc 1\n",
      "2022-03-25T10:38:41.261426: step 7858, loss 0.00114612, acc 1\n",
      "2022-03-25T10:38:41.408122: step 7859, loss 1.77994e-05, acc 1\n",
      "2022-03-25T10:38:41.555617: step 7860, loss 1.04635e-05, acc 1\n",
      "2022-03-25T10:38:41.704895: step 7861, loss 0.00108023, acc 1\n",
      "2022-03-25T10:38:41.851699: step 7862, loss 0.0384667, acc 0.984375\n",
      "2022-03-25T10:38:42.008228: step 7863, loss 0.00332819, acc 1\n",
      "2022-03-25T10:38:42.150914: step 7864, loss 0.00154217, acc 1\n",
      "2022-03-25T10:38:42.313926: step 7865, loss 7.11461e-06, acc 1\n",
      "2022-03-25T10:38:42.461361: step 7866, loss 0.000333512, acc 1\n",
      "2022-03-25T10:38:42.609632: step 7867, loss 8.1909e-05, acc 1\n",
      "2022-03-25T10:38:42.751451: step 7868, loss 7.60637e-05, acc 1\n",
      "2022-03-25T10:38:42.898668: step 7869, loss 0.000196429, acc 1\n",
      "2022-03-25T10:38:43.049364: step 7870, loss 0.000159417, acc 1\n",
      "2022-03-25T10:38:43.212139: step 7871, loss 3.73882e-05, acc 1\n",
      "2022-03-25T10:38:43.363851: step 7872, loss 0.000133447, acc 1\n",
      "2022-03-25T10:38:43.516373: step 7873, loss 0.00218681, acc 1\n",
      "2022-03-25T10:38:43.666072: step 7874, loss 9.0342e-05, acc 1\n",
      "2022-03-25T10:38:43.820409: step 7875, loss 0.000664936, acc 1\n",
      "2022-03-25T10:38:43.966450: step 7876, loss 0.00012922, acc 1\n",
      "2022-03-25T10:38:44.118869: step 7877, loss 0.000155344, acc 1\n",
      "2022-03-25T10:38:44.265762: step 7878, loss 0.000849953, acc 1\n",
      "2022-03-25T10:38:44.424014: step 7879, loss 8.90843e-05, acc 1\n",
      "2022-03-25T10:38:44.569550: step 7880, loss 3.68452e-05, acc 1\n",
      "2022-03-25T10:38:44.719327: step 7881, loss 0.000378395, acc 1\n",
      "2022-03-25T10:38:44.868444: step 7882, loss 3.22806e-05, acc 1\n",
      "2022-03-25T10:38:45.020659: step 7883, loss 2.40497e-05, acc 1\n",
      "2022-03-25T10:38:45.174043: step 7884, loss 0.00475821, acc 1\n",
      "2022-03-25T10:38:45.330625: step 7885, loss 0.000729252, acc 1\n",
      "2022-03-25T10:38:45.477567: step 7886, loss 0.00268551, acc 1\n",
      "2022-03-25T10:38:45.631391: step 7887, loss 3.55295e-05, acc 1\n",
      "2022-03-25T10:38:45.778889: step 7888, loss 7.4073e-05, acc 1\n",
      "2022-03-25T10:38:45.937157: step 7889, loss 0.00247566, acc 1\n",
      "2022-03-25T10:38:46.094806: step 7890, loss 2.78383e-05, acc 1\n",
      "2022-03-25T10:38:46.253553: step 7891, loss 0.000291688, acc 1\n",
      "2022-03-25T10:38:46.411006: step 7892, loss 7.46927e-05, acc 1\n",
      "2022-03-25T10:38:46.555348: step 7893, loss 0.00210207, acc 1\n",
      "2022-03-25T10:38:46.705013: step 7894, loss 0.00165769, acc 1\n",
      "2022-03-25T10:38:46.856490: step 7895, loss 7.50202e-05, acc 1\n",
      "2022-03-25T10:38:47.005649: step 7896, loss 9.25073e-06, acc 1\n",
      "2022-03-25T10:38:47.162761: step 7897, loss 5.26877e-05, acc 1\n",
      "2022-03-25T10:38:47.311481: step 7898, loss 1.6549e-05, acc 1\n",
      "2022-03-25T10:38:47.458625: step 7899, loss 0.034699, acc 0.984375\n",
      "2022-03-25T10:38:47.608918: step 7900, loss 0.000960878, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:38:47.757823: step 7900, loss 0.713694, acc 0.921512\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-7900\n",
      "\n",
      "2022-03-25T10:38:48.019625: step 7901, loss 0.00118152, acc 1\n",
      "2022-03-25T10:38:48.174595: step 7902, loss 2.50566e-05, acc 1\n",
      "2022-03-25T10:38:48.321355: step 7903, loss 0.00111837, acc 1\n",
      "2022-03-25T10:38:48.476163: step 7904, loss 7.28428e-05, acc 1\n",
      "2022-03-25T10:38:48.628623: step 7905, loss 0.00174997, acc 1\n",
      "2022-03-25T10:38:48.782713: step 7906, loss 0.000765171, acc 1\n",
      "2022-03-25T10:38:48.940566: step 7907, loss 0.0023044, acc 1\n",
      "2022-03-25T10:38:49.093773: step 7908, loss 0.000346196, acc 1\n",
      "2022-03-25T10:38:49.244084: step 7909, loss 0.000289933, acc 1\n",
      "2022-03-25T10:38:49.387374: step 7910, loss 0.000433337, acc 1\n",
      "2022-03-25T10:38:49.540353: step 7911, loss 0.000114116, acc 1\n",
      "2022-03-25T10:38:49.690873: step 7912, loss 0.00240137, acc 1\n",
      "2022-03-25T10:38:49.847285: step 7913, loss 0.00132442, acc 1\n",
      "2022-03-25T10:38:50.001980: step 7914, loss 0.00213429, acc 1\n",
      "2022-03-25T10:38:50.158145: step 7915, loss 8.07126e-05, acc 1\n",
      "2022-03-25T10:38:50.305424: step 7916, loss 0.000278666, acc 1\n",
      "2022-03-25T10:38:50.464425: step 7917, loss 2.63948e-05, acc 1\n",
      "2022-03-25T10:38:50.617404: step 7918, loss 0.000745584, acc 1\n",
      "2022-03-25T10:38:50.768983: step 7919, loss 0.00246844, acc 1\n",
      "2022-03-25T10:38:50.917618: step 7920, loss 0.000103687, acc 1\n",
      "2022-03-25T10:38:51.069531: step 7921, loss 0.000960997, acc 1\n",
      "2022-03-25T10:38:51.225120: step 7922, loss 9.35017e-05, acc 1\n",
      "2022-03-25T10:38:51.367917: step 7923, loss 0.000446442, acc 1\n",
      "2022-03-25T10:38:51.526767: step 7924, loss 0.000316528, acc 1\n",
      "2022-03-25T10:38:51.674463: step 7925, loss 2.15555e-05, acc 1\n",
      "2022-03-25T10:38:51.827853: step 7926, loss 0.000264163, acc 1\n",
      "2022-03-25T10:38:51.981099: step 7927, loss 0.000174537, acc 1\n",
      "2022-03-25T10:38:52.147855: step 7928, loss 0.000196522, acc 1\n",
      "2022-03-25T10:38:52.298759: step 7929, loss 0.00233804, acc 1\n",
      "2022-03-25T10:38:52.458291: step 7930, loss 0.000114469, acc 1\n",
      "2022-03-25T10:38:52.610405: step 7931, loss 6.96398e-05, acc 1\n",
      "2022-03-25T10:38:52.762830: step 7932, loss 0.000149193, acc 1\n",
      "2022-03-25T10:38:52.920655: step 7933, loss 0.00612182, acc 1\n",
      "2022-03-25T10:38:53.062908: step 7934, loss 0.000122845, acc 1\n",
      "2022-03-25T10:38:53.212798: step 7935, loss 0.000225653, acc 1\n",
      "2022-03-25T10:38:53.364263: step 7936, loss 0.000190063, acc 1\n",
      "2022-03-25T10:38:53.526300: step 7937, loss 7.39768e-05, acc 1\n",
      "2022-03-25T10:38:53.683422: step 7938, loss 0.00164171, acc 1\n",
      "2022-03-25T10:38:53.840049: step 7939, loss 2.19489e-05, acc 1\n",
      "2022-03-25T10:38:53.988949: step 7940, loss 0.000266222, acc 1\n",
      "2022-03-25T10:38:54.141811: step 7941, loss 7.2788e-05, acc 1\n",
      "2022-03-25T10:38:54.290716: step 7942, loss 0.000478849, acc 1\n",
      "2022-03-25T10:38:54.434385: step 7943, loss 0.000705268, acc 1\n",
      "2022-03-25T10:38:54.595224: step 7944, loss 0.00208729, acc 1\n",
      "2022-03-25T10:38:54.750305: step 7945, loss 7.51224e-05, acc 1\n",
      "2022-03-25T10:38:54.895615: step 7946, loss 0.000378232, acc 1\n",
      "2022-03-25T10:38:55.056501: step 7947, loss 0.000749118, acc 1\n",
      "2022-03-25T10:38:55.206739: step 7948, loss 2.57278e-05, acc 1\n",
      "2022-03-25T10:38:55.354158: step 7949, loss 0.00269232, acc 1\n",
      "2022-03-25T10:38:55.507640: step 7950, loss 0.00188367, acc 1\n",
      "2022-03-25T10:38:55.660347: step 7951, loss 0.0182323, acc 0.984375\n",
      "2022-03-25T10:38:55.810810: step 7952, loss 0.000412991, acc 1\n",
      "2022-03-25T10:38:55.967968: step 7953, loss 0.0298781, acc 0.984375\n",
      "2022-03-25T10:38:56.085663: step 7954, loss 0.000279786, acc 1\n",
      "2022-03-25T10:38:56.244842: step 7955, loss 4.19737e-05, acc 1\n",
      "2022-03-25T10:38:56.395643: step 7956, loss 8.78125e-05, acc 1\n",
      "2022-03-25T10:38:56.553774: step 7957, loss 0.00645212, acc 1\n",
      "2022-03-25T10:38:56.709245: step 7958, loss 2.45138e-05, acc 1\n",
      "2022-03-25T10:38:56.857790: step 7959, loss 1.8926e-05, acc 1\n",
      "2022-03-25T10:38:57.013543: step 7960, loss 4.41656e-05, acc 1\n",
      "2022-03-25T10:38:57.172973: step 7961, loss 4.62814e-05, acc 1\n",
      "2022-03-25T10:38:57.322484: step 7962, loss 0.00197871, acc 1\n",
      "2022-03-25T10:38:57.476652: step 7963, loss 3.67857e-06, acc 1\n",
      "2022-03-25T10:38:57.630823: step 7964, loss 1.55049e-05, acc 1\n",
      "2022-03-25T10:38:57.787861: step 7965, loss 4.04691e-05, acc 1\n",
      "2022-03-25T10:38:57.945019: step 7966, loss 0.00462793, acc 1\n",
      "2022-03-25T10:38:58.099648: step 7967, loss 0.00188602, acc 1\n",
      "2022-03-25T10:38:58.252268: step 7968, loss 0.00388568, acc 1\n",
      "2022-03-25T10:38:58.407282: step 7969, loss 0.000157298, acc 1\n",
      "2022-03-25T10:38:58.550604: step 7970, loss 1.48856e-05, acc 1\n",
      "2022-03-25T10:38:58.703414: step 7971, loss 6.74413e-06, acc 1\n",
      "2022-03-25T10:38:58.844418: step 7972, loss 0.0170965, acc 0.984375\n",
      "2022-03-25T10:38:58.999698: step 7973, loss 0.00344612, acc 1\n",
      "2022-03-25T10:38:59.153335: step 7974, loss 0.000627921, acc 1\n",
      "2022-03-25T10:38:59.314060: step 7975, loss 1.15643e-05, acc 1\n",
      "2022-03-25T10:38:59.460945: step 7976, loss 0.000168715, acc 1\n",
      "2022-03-25T10:38:59.616931: step 7977, loss 1.06755e-05, acc 1\n",
      "2022-03-25T10:38:59.766117: step 7978, loss 8.06056e-05, acc 1\n",
      "2022-03-25T10:38:59.916867: step 7979, loss 4.65771e-05, acc 1\n",
      "2022-03-25T10:39:00.068023: step 7980, loss 7.78037e-05, acc 1\n",
      "2022-03-25T10:39:00.221636: step 7981, loss 9.33919e-05, acc 1\n",
      "2022-03-25T10:39:00.362029: step 7982, loss 0.00284972, acc 1\n",
      "2022-03-25T10:39:00.510245: step 7983, loss 0.0120104, acc 0.984375\n",
      "2022-03-25T10:39:00.675896: step 7984, loss 0.000166472, acc 1\n",
      "2022-03-25T10:39:00.841520: step 7985, loss 0.000546665, acc 1\n",
      "2022-03-25T10:39:00.989233: step 7986, loss 3.06275e-05, acc 1\n",
      "2022-03-25T10:39:01.142824: step 7987, loss 0.000142163, acc 1\n",
      "2022-03-25T10:39:01.294246: step 7988, loss 2.38878e-05, acc 1\n",
      "2022-03-25T10:39:01.444467: step 7989, loss 1.5257e-05, acc 1\n",
      "2022-03-25T10:39:01.597533: step 7990, loss 0.000246131, acc 1\n",
      "2022-03-25T10:39:01.760289: step 7991, loss 0.000533082, acc 1\n",
      "2022-03-25T10:39:01.916039: step 7992, loss 0.0302965, acc 0.984375\n",
      "2022-03-25T10:39:02.073598: step 7993, loss 0.000236792, acc 1\n",
      "2022-03-25T10:39:02.228908: step 7994, loss 3.0063e-05, acc 1\n",
      "2022-03-25T10:39:02.379113: step 7995, loss 0.0252775, acc 0.984375\n",
      "2022-03-25T10:39:02.530673: step 7996, loss 0.000252396, acc 1\n",
      "2022-03-25T10:39:02.686221: step 7997, loss 0.016338, acc 0.984375\n",
      "2022-03-25T10:39:02.843729: step 7998, loss 0.0135051, acc 0.984375\n",
      "2022-03-25T10:39:02.991266: step 7999, loss 0.00365045, acc 1\n",
      "2022-03-25T10:39:03.147999: step 8000, loss 4.45545e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:39:03.297442: step 8000, loss 0.751129, acc 0.921512\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8000\n",
      "\n",
      "2022-03-25T10:39:03.534273: step 8001, loss 0.000201188, acc 1\n",
      "2022-03-25T10:39:03.687233: step 8002, loss 1.86982e-05, acc 1\n",
      "2022-03-25T10:39:03.823404: step 8003, loss 0.0134487, acc 1\n",
      "2022-03-25T10:39:03.976281: step 8004, loss 5.49643e-06, acc 1\n",
      "2022-03-25T10:39:04.125622: step 8005, loss 0.00150554, acc 1\n",
      "2022-03-25T10:39:04.266743: step 8006, loss 9.01254e-05, acc 1\n",
      "2022-03-25T10:39:04.404159: step 8007, loss 0.000122446, acc 1\n",
      "2022-03-25T10:39:04.553164: step 8008, loss 0.000701997, acc 1\n",
      "2022-03-25T10:39:04.703921: step 8009, loss 0.000824807, acc 1\n",
      "2022-03-25T10:39:04.853593: step 8010, loss 0.023157, acc 0.984375\n",
      "2022-03-25T10:39:05.002668: step 8011, loss 0.0127064, acc 1\n",
      "2022-03-25T10:39:05.147467: step 8012, loss 0.000426545, acc 1\n",
      "2022-03-25T10:39:05.282705: step 8013, loss 0.000790474, acc 1\n",
      "2022-03-25T10:39:05.427434: step 8014, loss 0.000432178, acc 1\n",
      "2022-03-25T10:39:05.578519: step 8015, loss 0.00232084, acc 1\n",
      "2022-03-25T10:39:05.725714: step 8016, loss 0.0160343, acc 0.984375\n",
      "2022-03-25T10:39:05.875178: step 8017, loss 0.0246375, acc 0.984375\n",
      "2022-03-25T10:39:06.023019: step 8018, loss 0.00016581, acc 1\n",
      "2022-03-25T10:39:06.164927: step 8019, loss 0.000109421, acc 1\n",
      "2022-03-25T10:39:06.311673: step 8020, loss 0.00115105, acc 1\n",
      "2022-03-25T10:39:06.453613: step 8021, loss 0.000109277, acc 1\n",
      "2022-03-25T10:39:06.600609: step 8022, loss 9.4938e-05, acc 1\n",
      "2022-03-25T10:39:06.742570: step 8023, loss 0.00016577, acc 1\n",
      "2022-03-25T10:39:06.894955: step 8024, loss 1.01631e-05, acc 1\n",
      "2022-03-25T10:39:07.040833: step 8025, loss 0.00161459, acc 1\n",
      "2022-03-25T10:39:07.197949: step 8026, loss 0.000537338, acc 1\n",
      "2022-03-25T10:39:07.355388: step 8027, loss 6.76504e-05, acc 1\n",
      "2022-03-25T10:39:07.515151: step 8028, loss 6.99719e-06, acc 1\n",
      "2022-03-25T10:39:07.667049: step 8029, loss 0.000158028, acc 1\n",
      "2022-03-25T10:39:07.820901: step 8030, loss 0.00510514, acc 1\n",
      "2022-03-25T10:39:07.968749: step 8031, loss 0.00438486, acc 1\n",
      "2022-03-25T10:39:08.113749: step 8032, loss 0.000189337, acc 1\n",
      "2022-03-25T10:39:08.253703: step 8033, loss 0.000128275, acc 1\n",
      "2022-03-25T10:39:08.403797: step 8034, loss 3.81327e-05, acc 1\n",
      "2022-03-25T10:39:08.549971: step 8035, loss 0.000252034, acc 1\n",
      "2022-03-25T10:39:08.698848: step 8036, loss 0.000698788, acc 1\n",
      "2022-03-25T10:39:08.850597: step 8037, loss 0.000281401, acc 1\n",
      "2022-03-25T10:39:08.993247: step 8038, loss 0.00334731, acc 1\n",
      "2022-03-25T10:39:09.139126: step 8039, loss 0.000499803, acc 1\n",
      "2022-03-25T10:39:09.283694: step 8040, loss 3.7978e-06, acc 1\n",
      "2022-03-25T10:39:09.425642: step 8041, loss 0.00161826, acc 1\n",
      "2022-03-25T10:39:09.571261: step 8042, loss 0.00015014, acc 1\n",
      "2022-03-25T10:39:09.719242: step 8043, loss 0.00719146, acc 1\n",
      "2022-03-25T10:39:09.884041: step 8044, loss 0.0001447, acc 1\n",
      "2022-03-25T10:39:10.024993: step 8045, loss 1.43985e-05, acc 1\n",
      "2022-03-25T10:39:10.177578: step 8046, loss 4.23178e-05, acc 1\n",
      "2022-03-25T10:39:10.315026: step 8047, loss 0.000132788, acc 1\n",
      "2022-03-25T10:39:10.463920: step 8048, loss 0.000123195, acc 1\n",
      "2022-03-25T10:39:10.610982: step 8049, loss 0.000172164, acc 1\n",
      "2022-03-25T10:39:10.762729: step 8050, loss 0.000227146, acc 1\n",
      "2022-03-25T10:39:10.915219: step 8051, loss 1.06086e-05, acc 1\n",
      "2022-03-25T10:39:11.065949: step 8052, loss 0.00705181, acc 1\n",
      "2022-03-25T10:39:11.207182: step 8053, loss 1.4639e-05, acc 1\n",
      "2022-03-25T10:39:11.360659: step 8054, loss 0.00643996, acc 1\n",
      "2022-03-25T10:39:11.500636: step 8055, loss 0.000282515, acc 1\n",
      "2022-03-25T10:39:11.647605: step 8056, loss 0.000270488, acc 1\n",
      "2022-03-25T10:39:11.794719: step 8057, loss 0.00200359, acc 1\n",
      "2022-03-25T10:39:11.947798: step 8058, loss 0.000133242, acc 1\n",
      "2022-03-25T10:39:12.092626: step 8059, loss 0.000150273, acc 1\n",
      "2022-03-25T10:39:12.246255: step 8060, loss 9.255e-06, acc 1\n",
      "2022-03-25T10:39:12.393129: step 8061, loss 0.000499881, acc 1\n",
      "2022-03-25T10:39:12.540772: step 8062, loss 0.000155286, acc 1\n",
      "2022-03-25T10:39:12.687110: step 8063, loss 0.000485648, acc 1\n",
      "2022-03-25T10:39:12.834642: step 8064, loss 0.000130521, acc 1\n",
      "2022-03-25T10:39:12.991454: step 8065, loss 0.000323708, acc 1\n",
      "2022-03-25T10:39:13.142610: step 8066, loss 0.000206135, acc 1\n",
      "2022-03-25T10:39:13.279633: step 8067, loss 0.026843, acc 0.984375\n",
      "2022-03-25T10:39:13.429395: step 8068, loss 0.00919113, acc 1\n",
      "2022-03-25T10:39:13.570262: step 8069, loss 9.5149e-05, acc 1\n",
      "2022-03-25T10:39:13.723557: step 8070, loss 0.000303147, acc 1\n",
      "2022-03-25T10:39:13.868218: step 8071, loss 0.00419612, acc 1\n",
      "2022-03-25T10:39:14.022567: step 8072, loss 5.40783e-05, acc 1\n",
      "2022-03-25T10:39:14.172259: step 8073, loss 1.46733e-05, acc 1\n",
      "2022-03-25T10:39:14.318349: step 8074, loss 0.000208173, acc 1\n",
      "2022-03-25T10:39:14.466103: step 8075, loss 0.000538488, acc 1\n",
      "2022-03-25T10:39:14.612059: step 8076, loss 5.78501e-05, acc 1\n",
      "2022-03-25T10:39:14.756200: step 8077, loss 7.70122e-06, acc 1\n",
      "2022-03-25T10:39:14.902742: step 8078, loss 3.26872e-06, acc 1\n",
      "2022-03-25T10:39:15.053040: step 8079, loss 1.77568e-05, acc 1\n",
      "2022-03-25T10:39:15.204370: step 8080, loss 0.0312464, acc 0.984375\n",
      "2022-03-25T10:39:15.353722: step 8081, loss 0.00616672, acc 1\n",
      "2022-03-25T10:39:15.503004: step 8082, loss 0.000677986, acc 1\n",
      "2022-03-25T10:39:15.654279: step 8083, loss 0.00740725, acc 1\n",
      "2022-03-25T10:39:15.803888: step 8084, loss 0.00165474, acc 1\n",
      "2022-03-25T10:39:15.948288: step 8085, loss 0.00163233, acc 1\n",
      "2022-03-25T10:39:16.110478: step 8086, loss 1.40079e-05, acc 1\n",
      "2022-03-25T10:39:16.250702: step 8087, loss 0.0126314, acc 0.984375\n",
      "2022-03-25T10:39:16.397031: step 8088, loss 0.00101793, acc 1\n",
      "2022-03-25T10:39:16.537411: step 8089, loss 2.00433e-05, acc 1\n",
      "2022-03-25T10:39:16.683673: step 8090, loss 8.76563e-05, acc 1\n",
      "2022-03-25T10:39:16.825152: step 8091, loss 5.50593e-05, acc 1\n",
      "2022-03-25T10:39:16.968937: step 8092, loss 0.00424419, acc 1\n",
      "2022-03-25T10:39:17.122881: step 8093, loss 0.00586296, acc 1\n",
      "2022-03-25T10:39:17.272643: step 8094, loss 7.5012e-05, acc 1\n",
      "2022-03-25T10:39:17.421465: step 8095, loss 0.000433131, acc 1\n",
      "2022-03-25T10:39:17.567717: step 8096, loss 0.000435292, acc 1\n",
      "2022-03-25T10:39:17.710141: step 8097, loss 0.00367685, acc 1\n",
      "2022-03-25T10:39:17.859943: step 8098, loss 0.00296176, acc 1\n",
      "2022-03-25T10:39:18.013143: step 8099, loss 0.00279783, acc 1\n",
      "2022-03-25T10:39:18.170169: step 8100, loss 0.00381534, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:39:18.305698: step 8100, loss 0.711931, acc 0.90843\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8100\n",
      "\n",
      "2022-03-25T10:39:18.553892: step 8101, loss 0.000151073, acc 1\n",
      "2022-03-25T10:39:18.702387: step 8102, loss 0.000323931, acc 1\n",
      "2022-03-25T10:39:18.849512: step 8103, loss 0.000273388, acc 1\n",
      "2022-03-25T10:39:18.999206: step 8104, loss 0.00109718, acc 1\n",
      "2022-03-25T10:39:19.149223: step 8105, loss 9.19828e-05, acc 1\n",
      "2022-03-25T10:39:19.293805: step 8106, loss 0.00218251, acc 1\n",
      "2022-03-25T10:39:19.441470: step 8107, loss 0.00176102, acc 1\n",
      "2022-03-25T10:39:19.583546: step 8108, loss 0.000219416, acc 1\n",
      "2022-03-25T10:39:19.734614: step 8109, loss 0.00400427, acc 1\n",
      "2022-03-25T10:39:19.879836: step 8110, loss 7.69293e-05, acc 1\n",
      "2022-03-25T10:39:20.020836: step 8111, loss 0.00149041, acc 1\n",
      "2022-03-25T10:39:20.179630: step 8112, loss 0.000500535, acc 1\n",
      "2022-03-25T10:39:20.324434: step 8113, loss 2.45497e-05, acc 1\n",
      "2022-03-25T10:39:20.464923: step 8114, loss 0.00695979, acc 1\n",
      "2022-03-25T10:39:20.611617: step 8115, loss 0.0003687, acc 1\n",
      "2022-03-25T10:39:20.752665: step 8116, loss 0.000137436, acc 1\n",
      "2022-03-25T10:39:20.894244: step 8117, loss 1.42607e-05, acc 1\n",
      "2022-03-25T10:39:21.038189: step 8118, loss 1.9836e-05, acc 1\n",
      "2022-03-25T10:39:21.183680: step 8119, loss 0.00012997, acc 1\n",
      "2022-03-25T10:39:21.335193: step 8120, loss 0.00622709, acc 1\n",
      "2022-03-25T10:39:21.480615: step 8121, loss 0.000114966, acc 1\n",
      "2022-03-25T10:39:21.629069: step 8122, loss 0.000184253, acc 1\n",
      "2022-03-25T10:39:21.773288: step 8123, loss 7.9355e-05, acc 1\n",
      "2022-03-25T10:39:21.916577: step 8124, loss 0.000207466, acc 1\n",
      "2022-03-25T10:39:22.054504: step 8125, loss 0.0305651, acc 0.984375\n",
      "2022-03-25T10:39:22.219048: step 8126, loss 3.81462e-06, acc 1\n",
      "2022-03-25T10:39:22.365641: step 8127, loss 1.28471e-05, acc 1\n",
      "2022-03-25T10:39:22.510252: step 8128, loss 0.0735962, acc 0.984375\n",
      "2022-03-25T10:39:22.652882: step 8129, loss 0.000368068, acc 1\n",
      "2022-03-25T10:39:22.802638: step 8130, loss 7.83527e-06, acc 1\n",
      "2022-03-25T10:39:22.949011: step 8131, loss 0.000438744, acc 1\n",
      "2022-03-25T10:39:23.097383: step 8132, loss 6.09069e-06, acc 1\n",
      "2022-03-25T10:39:23.255471: step 8133, loss 3.52766e-06, acc 1\n",
      "2022-03-25T10:39:23.391978: step 8134, loss 0.000947587, acc 1\n",
      "2022-03-25T10:39:23.535262: step 8135, loss 2.33108e-05, acc 1\n",
      "2022-03-25T10:39:23.678301: step 8136, loss 0.000743099, acc 1\n",
      "2022-03-25T10:39:23.835704: step 8137, loss 5.69144e-05, acc 1\n",
      "2022-03-25T10:39:23.979793: step 8138, loss 4.91383e-05, acc 1\n",
      "2022-03-25T10:39:24.129134: step 8139, loss 4.65564e-05, acc 1\n",
      "2022-03-25T10:39:24.283440: step 8140, loss 3.72941e-05, acc 1\n",
      "2022-03-25T10:39:24.431240: step 8141, loss 8.41099e-06, acc 1\n",
      "2022-03-25T10:39:24.583378: step 8142, loss 1.34278e-05, acc 1\n",
      "2022-03-25T10:39:24.735769: step 8143, loss 8.86341e-06, acc 1\n",
      "2022-03-25T10:39:24.883973: step 8144, loss 6.24888e-06, acc 1\n",
      "2022-03-25T10:39:25.029522: step 8145, loss 0.000409353, acc 1\n",
      "2022-03-25T10:39:25.171244: step 8146, loss 0.000687543, acc 1\n",
      "2022-03-25T10:39:25.326710: step 8147, loss 0.000655732, acc 1\n",
      "2022-03-25T10:39:25.450856: step 8148, loss 7.51539e-05, acc 1\n",
      "2022-03-25T10:39:25.599604: step 8149, loss 0.00146903, acc 1\n",
      "2022-03-25T10:39:25.747266: step 8150, loss 1.49337e-05, acc 1\n",
      "2022-03-25T10:39:25.886493: step 8151, loss 4.67501e-05, acc 1\n",
      "2022-03-25T10:39:26.033561: step 8152, loss 0.00347857, acc 1\n",
      "2022-03-25T10:39:26.178877: step 8153, loss 0.00717707, acc 1\n",
      "2022-03-25T10:39:26.334288: step 8154, loss 4.49969e-06, acc 1\n",
      "2022-03-25T10:39:26.481884: step 8155, loss 0.000638857, acc 1\n",
      "2022-03-25T10:39:26.624016: step 8156, loss 0.00644806, acc 1\n",
      "2022-03-25T10:39:26.770589: step 8157, loss 0.00563169, acc 1\n",
      "2022-03-25T10:39:26.918783: step 8158, loss 1.3482e-05, acc 1\n",
      "2022-03-25T10:39:27.058685: step 8159, loss 1.92398e-05, acc 1\n",
      "2022-03-25T10:39:27.208894: step 8160, loss 0.00356191, acc 1\n",
      "2022-03-25T10:39:27.353206: step 8161, loss 9.52971e-05, acc 1\n",
      "2022-03-25T10:39:27.499269: step 8162, loss 6.63488e-05, acc 1\n",
      "2022-03-25T10:39:27.649297: step 8163, loss 2.78115e-05, acc 1\n",
      "2022-03-25T10:39:27.793576: step 8164, loss 0.00132089, acc 1\n",
      "2022-03-25T10:39:27.952643: step 8165, loss 8.62399e-07, acc 1\n",
      "2022-03-25T10:39:28.102667: step 8166, loss 0.000443223, acc 1\n",
      "2022-03-25T10:39:28.245270: step 8167, loss 0.000104655, acc 1\n",
      "2022-03-25T10:39:28.403106: step 8168, loss 0.00439549, acc 1\n",
      "2022-03-25T10:39:28.545396: step 8169, loss 2.65756e-05, acc 1\n",
      "2022-03-25T10:39:28.690365: step 8170, loss 4.6975e-05, acc 1\n",
      "2022-03-25T10:39:28.841731: step 8171, loss 0.000579732, acc 1\n",
      "2022-03-25T10:39:28.986547: step 8172, loss 6.47737e-05, acc 1\n",
      "2022-03-25T10:39:29.131665: step 8173, loss 0.0076821, acc 1\n",
      "2022-03-25T10:39:29.284181: step 8174, loss 0.000924234, acc 1\n",
      "2022-03-25T10:39:29.442905: step 8175, loss 0.00218917, acc 1\n",
      "2022-03-25T10:39:29.587184: step 8176, loss 3.20862e-05, acc 1\n",
      "2022-03-25T10:39:29.740712: step 8177, loss 7.89301e-05, acc 1\n",
      "2022-03-25T10:39:29.883450: step 8178, loss 3.59947e-05, acc 1\n",
      "2022-03-25T10:39:30.027022: step 8179, loss 0.000279901, acc 1\n",
      "2022-03-25T10:39:30.173450: step 8180, loss 0.00114252, acc 1\n",
      "2022-03-25T10:39:30.315640: step 8181, loss 5.68009e-05, acc 1\n",
      "2022-03-25T10:39:30.476548: step 8182, loss 5.13502e-05, acc 1\n",
      "2022-03-25T10:39:30.622454: step 8183, loss 0.000183497, acc 1\n",
      "2022-03-25T10:39:30.776191: step 8184, loss 0.00139936, acc 1\n",
      "2022-03-25T10:39:30.930922: step 8185, loss 0.000817946, acc 1\n",
      "2022-03-25T10:39:31.084734: step 8186, loss 0.000183801, acc 1\n",
      "2022-03-25T10:39:31.233644: step 8187, loss 4.82738e-05, acc 1\n",
      "2022-03-25T10:39:31.387173: step 8188, loss 3.67412e-05, acc 1\n",
      "2022-03-25T10:39:31.538570: step 8189, loss 5.56621e-05, acc 1\n",
      "2022-03-25T10:39:31.680908: step 8190, loss 0.00271206, acc 1\n",
      "2022-03-25T10:39:31.829977: step 8191, loss 0.0001419, acc 1\n",
      "2022-03-25T10:39:31.982411: step 8192, loss 0.00378312, acc 1\n",
      "2022-03-25T10:39:32.134971: step 8193, loss 0.00674918, acc 1\n",
      "2022-03-25T10:39:32.279748: step 8194, loss 0.000130505, acc 1\n",
      "2022-03-25T10:39:32.449274: step 8195, loss 4.96944e-05, acc 1\n",
      "2022-03-25T10:39:32.598271: step 8196, loss 5.66788e-06, acc 1\n",
      "2022-03-25T10:39:32.755490: step 8197, loss 0.000748461, acc 1\n",
      "2022-03-25T10:39:32.897904: step 8198, loss 3.83166e-05, acc 1\n",
      "2022-03-25T10:39:33.048643: step 8199, loss 7.24896e-06, acc 1\n",
      "2022-03-25T10:39:33.199437: step 8200, loss 0.00157434, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:39:33.352609: step 8200, loss 0.799053, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8200\n",
      "\n",
      "2022-03-25T10:39:33.618600: step 8201, loss 6.91383e-06, acc 1\n",
      "2022-03-25T10:39:33.763658: step 8202, loss 0.000388181, acc 1\n",
      "2022-03-25T10:39:33.916960: step 8203, loss 0.000355733, acc 1\n",
      "2022-03-25T10:39:34.062120: step 8204, loss 0.000150541, acc 1\n",
      "2022-03-25T10:39:34.211195: step 8205, loss 2.38161e-05, acc 1\n",
      "2022-03-25T10:39:34.357669: step 8206, loss 0.000688406, acc 1\n",
      "2022-03-25T10:39:34.511633: step 8207, loss 1.08019e-05, acc 1\n",
      "2022-03-25T10:39:34.657944: step 8208, loss 2.41145e-05, acc 1\n",
      "2022-03-25T10:39:34.809536: step 8209, loss 0.000116972, acc 1\n",
      "2022-03-25T10:39:34.955564: step 8210, loss 2.30723e-05, acc 1\n",
      "2022-03-25T10:39:35.104412: step 8211, loss 5.10516e-06, acc 1\n",
      "2022-03-25T10:39:35.257741: step 8212, loss 0.000183966, acc 1\n",
      "2022-03-25T10:39:35.404696: step 8213, loss 0.00301427, acc 1\n",
      "2022-03-25T10:39:35.565259: step 8214, loss 0.0135398, acc 0.984375\n",
      "2022-03-25T10:39:35.717555: step 8215, loss 3.7382e-05, acc 1\n",
      "2022-03-25T10:39:35.868468: step 8216, loss 0.000252167, acc 1\n",
      "2022-03-25T10:39:36.017769: step 8217, loss 1.2014e-06, acc 1\n",
      "2022-03-25T10:39:36.166591: step 8218, loss 0.00130261, acc 1\n",
      "2022-03-25T10:39:36.317764: step 8219, loss 0.0213066, acc 0.984375\n",
      "2022-03-25T10:39:36.466686: step 8220, loss 9.01149e-05, acc 1\n",
      "2022-03-25T10:39:36.626702: step 8221, loss 0.000226642, acc 1\n",
      "2022-03-25T10:39:36.769310: step 8222, loss 0.000221599, acc 1\n",
      "2022-03-25T10:39:36.930973: step 8223, loss 0.000320794, acc 1\n",
      "2022-03-25T10:39:37.074220: step 8224, loss 0.00133736, acc 1\n",
      "2022-03-25T10:39:37.221718: step 8225, loss 0.00248508, acc 1\n",
      "2022-03-25T10:39:37.371989: step 8226, loss 0.00281492, acc 1\n",
      "2022-03-25T10:39:37.527115: step 8227, loss 0.00114565, acc 1\n",
      "2022-03-25T10:39:37.685219: step 8228, loss 0.000987696, acc 1\n",
      "2022-03-25T10:39:37.831327: step 8229, loss 0.000789247, acc 1\n",
      "2022-03-25T10:39:37.972856: step 8230, loss 0.00169319, acc 1\n",
      "2022-03-25T10:39:38.127864: step 8231, loss 0.00350916, acc 1\n",
      "2022-03-25T10:39:38.289057: step 8232, loss 0.00354835, acc 1\n",
      "2022-03-25T10:39:38.431648: step 8233, loss 0.0110564, acc 1\n",
      "2022-03-25T10:39:38.591741: step 8234, loss 3.50865e-05, acc 1\n",
      "2022-03-25T10:39:38.743453: step 8235, loss 2.09129e-05, acc 1\n",
      "2022-03-25T10:39:38.896708: step 8236, loss 0.000223511, acc 1\n",
      "2022-03-25T10:39:39.060246: step 8237, loss 0.024061, acc 0.984375\n",
      "2022-03-25T10:39:39.211599: step 8238, loss 0.00138049, acc 1\n",
      "2022-03-25T10:39:39.369670: step 8239, loss 9.79348e-05, acc 1\n",
      "2022-03-25T10:39:39.515837: step 8240, loss 0.000218435, acc 1\n",
      "2022-03-25T10:39:39.670034: step 8241, loss 0.0534743, acc 0.984375\n",
      "2022-03-25T10:39:39.824972: step 8242, loss 4.79131e-05, acc 1\n",
      "2022-03-25T10:39:39.977265: step 8243, loss 5.97496e-06, acc 1\n",
      "2022-03-25T10:39:40.119691: step 8244, loss 0.00023916, acc 1\n",
      "2022-03-25T10:39:40.282062: step 8245, loss 0.000129733, acc 1\n",
      "2022-03-25T10:39:40.436011: step 8246, loss 0.000265489, acc 1\n",
      "2022-03-25T10:39:40.581181: step 8247, loss 0.000165083, acc 1\n",
      "2022-03-25T10:39:40.724734: step 8248, loss 1.36013e-05, acc 1\n",
      "2022-03-25T10:39:40.870418: step 8249, loss 2.34615e-05, acc 1\n",
      "2022-03-25T10:39:41.024997: step 8250, loss 3.99664e-05, acc 1\n",
      "2022-03-25T10:39:41.179650: step 8251, loss 0.000979497, acc 1\n",
      "2022-03-25T10:39:41.334174: step 8252, loss 0.00337332, acc 1\n",
      "2022-03-25T10:39:41.487184: step 8253, loss 0.00021731, acc 1\n",
      "2022-03-25T10:39:41.643541: step 8254, loss 6.02396e-05, acc 1\n",
      "2022-03-25T10:39:41.795316: step 8255, loss 0.0381153, acc 0.984375\n",
      "2022-03-25T10:39:41.941968: step 8256, loss 3.43102e-05, acc 1\n",
      "2022-03-25T10:39:42.092064: step 8257, loss 8.53799e-05, acc 1\n",
      "2022-03-25T10:39:42.240450: step 8258, loss 2.66559e-05, acc 1\n",
      "2022-03-25T10:39:42.397138: step 8259, loss 0.00377196, acc 1\n",
      "2022-03-25T10:39:42.541385: step 8260, loss 0.0114918, acc 1\n",
      "2022-03-25T10:39:42.704881: step 8261, loss 0.00433501, acc 1\n",
      "2022-03-25T10:39:42.860898: step 8262, loss 7.67734e-06, acc 1\n",
      "2022-03-25T10:39:43.022020: step 8263, loss 0.000101431, acc 1\n",
      "2022-03-25T10:39:43.172488: step 8264, loss 0.000153622, acc 1\n",
      "2022-03-25T10:39:43.328067: step 8265, loss 3.41016e-05, acc 1\n",
      "2022-03-25T10:39:43.472240: step 8266, loss 0.00624572, acc 1\n",
      "2022-03-25T10:39:43.621303: step 8267, loss 0.0017703, acc 1\n",
      "2022-03-25T10:39:43.775177: step 8268, loss 0.000673194, acc 1\n",
      "2022-03-25T10:39:43.926250: step 8269, loss 0.00311673, acc 1\n",
      "2022-03-25T10:39:44.074304: step 8270, loss 0.018231, acc 0.984375\n",
      "2022-03-25T10:39:44.220213: step 8271, loss 0.00031233, acc 1\n",
      "2022-03-25T10:39:44.368517: step 8272, loss 0.000142577, acc 1\n",
      "2022-03-25T10:39:44.519442: step 8273, loss 0.00722823, acc 1\n",
      "2022-03-25T10:39:44.670201: step 8274, loss 0.00139107, acc 1\n",
      "2022-03-25T10:39:44.812271: step 8275, loss 0.000148103, acc 1\n",
      "2022-03-25T10:39:44.962831: step 8276, loss 0.000436019, acc 1\n",
      "2022-03-25T10:39:45.122895: step 8277, loss 1.19646e-05, acc 1\n",
      "2022-03-25T10:39:45.274476: step 8278, loss 0.00231208, acc 1\n",
      "2022-03-25T10:39:45.420382: step 8279, loss 4.94947e-05, acc 1\n",
      "2022-03-25T10:39:45.563437: step 8280, loss 2.96644e-05, acc 1\n",
      "2022-03-25T10:39:45.702446: step 8281, loss 0.000103433, acc 1\n",
      "2022-03-25T10:39:45.862971: step 8282, loss 0.00125772, acc 1\n",
      "2022-03-25T10:39:46.028356: step 8283, loss 2.70351e-05, acc 1\n",
      "2022-03-25T10:39:46.178025: step 8284, loss 1.29345e-05, acc 1\n",
      "2022-03-25T10:39:46.329105: step 8285, loss 9.69311e-05, acc 1\n",
      "2022-03-25T10:39:46.476018: step 8286, loss 0.000193412, acc 1\n",
      "2022-03-25T10:39:46.631151: step 8287, loss 0.000129724, acc 1\n",
      "2022-03-25T10:39:46.798950: step 8288, loss 5.73377e-05, acc 1\n",
      "2022-03-25T10:39:46.951637: step 8289, loss 0.000425859, acc 1\n",
      "2022-03-25T10:39:47.111549: step 8290, loss 0.000218932, acc 1\n",
      "2022-03-25T10:39:47.263647: step 8291, loss 0.000527109, acc 1\n",
      "2022-03-25T10:39:47.421252: step 8292, loss 4.00067e-06, acc 1\n",
      "2022-03-25T10:39:47.575722: step 8293, loss 0.000279865, acc 1\n",
      "2022-03-25T10:39:47.730890: step 8294, loss 0.0015992, acc 1\n",
      "2022-03-25T10:39:47.892013: step 8295, loss 4.24574e-05, acc 1\n",
      "2022-03-25T10:39:48.039482: step 8296, loss 9.01832e-06, acc 1\n",
      "2022-03-25T10:39:48.187630: step 8297, loss 7.57438e-05, acc 1\n",
      "2022-03-25T10:39:48.334379: step 8298, loss 3.21115e-06, acc 1\n",
      "2022-03-25T10:39:48.482681: step 8299, loss 1.55205e-05, acc 1\n",
      "2022-03-25T10:39:48.629893: step 8300, loss 0.00170416, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:39:48.784484: step 8300, loss 0.804825, acc 0.921512\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8300\n",
      "\n",
      "2022-03-25T10:39:49.058482: step 8301, loss 0.000104086, acc 1\n",
      "2022-03-25T10:39:49.210453: step 8302, loss 0.0593841, acc 0.984375\n",
      "2022-03-25T10:39:49.367164: step 8303, loss 2.2392e-05, acc 1\n",
      "2022-03-25T10:39:49.523478: step 8304, loss 0.000595425, acc 1\n",
      "2022-03-25T10:39:49.669442: step 8305, loss 6.72713e-06, acc 1\n",
      "2022-03-25T10:39:49.833443: step 8306, loss 0.0157657, acc 0.984375\n",
      "2022-03-25T10:39:49.983492: step 8307, loss 3.17982e-05, acc 1\n",
      "2022-03-25T10:39:50.132950: step 8308, loss 4.7809e-05, acc 1\n",
      "2022-03-25T10:39:50.288409: step 8309, loss 0.0371258, acc 0.984375\n",
      "2022-03-25T10:39:50.440871: step 8310, loss 0.000207111, acc 1\n",
      "2022-03-25T10:39:50.589847: step 8311, loss 0.000327581, acc 1\n",
      "2022-03-25T10:39:50.741132: step 8312, loss 2.85879e-05, acc 1\n",
      "2022-03-25T10:39:50.897369: step 8313, loss 0.00027684, acc 1\n",
      "2022-03-25T10:39:51.049144: step 8314, loss 0.000750555, acc 1\n",
      "2022-03-25T10:39:51.198377: step 8315, loss 0.000206977, acc 1\n",
      "2022-03-25T10:39:51.347289: step 8316, loss 8.10029e-05, acc 1\n",
      "2022-03-25T10:39:51.506900: step 8317, loss 0.000215808, acc 1\n",
      "2022-03-25T10:39:51.660855: step 8318, loss 2.02113e-05, acc 1\n",
      "2022-03-25T10:39:51.819395: step 8319, loss 0.000129721, acc 1\n",
      "2022-03-25T10:39:51.968239: step 8320, loss 4.72457e-05, acc 1\n",
      "2022-03-25T10:39:52.113398: step 8321, loss 5.09779e-06, acc 1\n",
      "2022-03-25T10:39:52.260554: step 8322, loss 0.00312125, acc 1\n",
      "2022-03-25T10:39:52.406094: step 8323, loss 6.32012e-05, acc 1\n",
      "2022-03-25T10:39:52.559844: step 8324, loss 0.00638503, acc 1\n",
      "2022-03-25T10:39:52.711146: step 8325, loss 1.84087e-05, acc 1\n",
      "2022-03-25T10:39:52.857897: step 8326, loss 0.00161766, acc 1\n",
      "2022-03-25T10:39:53.014358: step 8327, loss 1.75049e-05, acc 1\n",
      "2022-03-25T10:39:53.160394: step 8328, loss 0.000712824, acc 1\n",
      "2022-03-25T10:39:53.308808: step 8329, loss 0.00102591, acc 1\n",
      "2022-03-25T10:39:53.465983: step 8330, loss 0.00148209, acc 1\n",
      "2022-03-25T10:39:53.643878: step 8331, loss 4.03283e-05, acc 1\n",
      "2022-03-25T10:39:53.796726: step 8332, loss 3.06947e-05, acc 1\n",
      "2022-03-25T10:39:53.949139: step 8333, loss 7.83233e-05, acc 1\n",
      "2022-03-25T10:39:54.102024: step 8334, loss 0.000271319, acc 1\n",
      "2022-03-25T10:39:54.242796: step 8335, loss 3.83565e-05, acc 1\n",
      "2022-03-25T10:39:54.391276: step 8336, loss 5.87482e-05, acc 1\n",
      "2022-03-25T10:39:54.546557: step 8337, loss 9.40108e-05, acc 1\n",
      "2022-03-25T10:39:54.699717: step 8338, loss 0.00978737, acc 1\n",
      "2022-03-25T10:39:54.855597: step 8339, loss 0.00276314, acc 1\n",
      "2022-03-25T10:39:55.008697: step 8340, loss 6.70489e-06, acc 1\n",
      "2022-03-25T10:39:55.161632: step 8341, loss 1.0504e-05, acc 1\n",
      "2022-03-25T10:39:55.281933: step 8342, loss 7.26614e-06, acc 1\n",
      "2022-03-25T10:39:55.436226: step 8343, loss 1.52574e-05, acc 1\n",
      "2022-03-25T10:39:55.582548: step 8344, loss 0.000916514, acc 1\n",
      "2022-03-25T10:39:55.731918: step 8345, loss 2.54279e-05, acc 1\n",
      "2022-03-25T10:39:55.887102: step 8346, loss 0.000791046, acc 1\n",
      "2022-03-25T10:39:56.033565: step 8347, loss 0.000187269, acc 1\n",
      "2022-03-25T10:39:56.184572: step 8348, loss 0.00782357, acc 1\n",
      "2022-03-25T10:39:56.334540: step 8349, loss 0.000808996, acc 1\n",
      "2022-03-25T10:39:56.482294: step 8350, loss 1.82928e-05, acc 1\n",
      "2022-03-25T10:39:56.635091: step 8351, loss 0.000260077, acc 1\n",
      "2022-03-25T10:39:56.790740: step 8352, loss 0.000240728, acc 1\n",
      "2022-03-25T10:39:56.949146: step 8353, loss 0.000228588, acc 1\n",
      "2022-03-25T10:39:57.095413: step 8354, loss 0.000670288, acc 1\n",
      "2022-03-25T10:39:57.247930: step 8355, loss 7.20309e-05, acc 1\n",
      "2022-03-25T10:39:57.396641: step 8356, loss 1.07814e-05, acc 1\n",
      "2022-03-25T10:39:57.542043: step 8357, loss 0.00401302, acc 1\n",
      "2022-03-25T10:39:57.698054: step 8358, loss 0.00390699, acc 1\n",
      "2022-03-25T10:39:57.838674: step 8359, loss 3.05353e-05, acc 1\n",
      "2022-03-25T10:39:58.004093: step 8360, loss 0.0229841, acc 0.984375\n",
      "2022-03-25T10:39:58.157121: step 8361, loss 0.0001565, acc 1\n",
      "2022-03-25T10:39:58.303158: step 8362, loss 0.000297167, acc 1\n",
      "2022-03-25T10:39:58.450244: step 8363, loss 1.24282e-05, acc 1\n",
      "2022-03-25T10:39:58.606875: step 8364, loss 2.2922e-05, acc 1\n",
      "2022-03-25T10:39:58.760178: step 8365, loss 0.000862871, acc 1\n",
      "2022-03-25T10:39:58.908043: step 8366, loss 1.64221e-05, acc 1\n",
      "2022-03-25T10:39:59.070979: step 8367, loss 3.05654e-06, acc 1\n",
      "2022-03-25T10:39:59.231215: step 8368, loss 0.000162142, acc 1\n",
      "2022-03-25T10:39:59.380772: step 8369, loss 2.03182e-05, acc 1\n",
      "2022-03-25T10:39:59.528923: step 8370, loss 1.57621e-05, acc 1\n",
      "2022-03-25T10:39:59.686989: step 8371, loss 8.92713e-05, acc 1\n",
      "2022-03-25T10:39:59.835254: step 8372, loss 0.000494765, acc 1\n",
      "2022-03-25T10:39:59.981157: step 8373, loss 0.000243309, acc 1\n",
      "2022-03-25T10:40:00.129133: step 8374, loss 0.000161497, acc 1\n",
      "2022-03-25T10:40:00.277775: step 8375, loss 2.97216e-05, acc 1\n",
      "2022-03-25T10:40:00.418311: step 8376, loss 0.000528756, acc 1\n",
      "2022-03-25T10:40:00.564279: step 8377, loss 2.45678e-05, acc 1\n",
      "2022-03-25T10:40:00.716721: step 8378, loss 0.00087192, acc 1\n",
      "2022-03-25T10:40:00.864042: step 8379, loss 1.26524e-05, acc 1\n",
      "2022-03-25T10:40:01.010190: step 8380, loss 0.0142115, acc 0.984375\n",
      "2022-03-25T10:40:01.158017: step 8381, loss 1.7567e-05, acc 1\n",
      "2022-03-25T10:40:01.298818: step 8382, loss 7.0397e-06, acc 1\n",
      "2022-03-25T10:40:01.448211: step 8383, loss 4.66394e-05, acc 1\n",
      "2022-03-25T10:40:01.597687: step 8384, loss 2.09754e-05, acc 1\n",
      "2022-03-25T10:40:01.752796: step 8385, loss 6.91576e-05, acc 1\n",
      "2022-03-25T10:40:01.903980: step 8386, loss 1.60175e-05, acc 1\n",
      "2022-03-25T10:40:02.046713: step 8387, loss 0.01029, acc 1\n",
      "2022-03-25T10:40:02.196261: step 8388, loss 0.00215383, acc 1\n",
      "2022-03-25T10:40:02.340622: step 8389, loss 7.30138e-05, acc 1\n",
      "2022-03-25T10:40:02.495712: step 8390, loss 3.49267e-05, acc 1\n",
      "2022-03-25T10:40:02.652256: step 8391, loss 0.00566755, acc 1\n",
      "2022-03-25T10:40:02.797392: step 8392, loss 0.000132471, acc 1\n",
      "2022-03-25T10:40:02.950886: step 8393, loss 5.67703e-06, acc 1\n",
      "2022-03-25T10:40:03.118896: step 8394, loss 0.000105287, acc 1\n",
      "2022-03-25T10:40:03.278089: step 8395, loss 0.0034029, acc 1\n",
      "2022-03-25T10:40:03.432575: step 8396, loss 0.000124621, acc 1\n",
      "2022-03-25T10:40:03.574533: step 8397, loss 0.000744601, acc 1\n",
      "2022-03-25T10:40:03.730486: step 8398, loss 0.000264222, acc 1\n",
      "2022-03-25T10:40:03.876065: step 8399, loss 5.25752e-05, acc 1\n",
      "2022-03-25T10:40:04.026019: step 8400, loss 1.81554e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:40:04.173841: step 8400, loss 0.759406, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8400\n",
      "\n",
      "2022-03-25T10:40:04.416752: step 8401, loss 1.38719e-05, acc 1\n",
      "2022-03-25T10:40:04.564938: step 8402, loss 5.05574e-05, acc 1\n",
      "2022-03-25T10:40:04.704677: step 8403, loss 0.000627603, acc 1\n",
      "2022-03-25T10:40:04.855333: step 8404, loss 0.000123781, acc 1\n",
      "2022-03-25T10:40:04.995922: step 8405, loss 0.000372931, acc 1\n",
      "2022-03-25T10:40:05.156733: step 8406, loss 0.000148151, acc 1\n",
      "2022-03-25T10:40:05.299863: step 8407, loss 0.000718023, acc 1\n",
      "2022-03-25T10:40:05.444359: step 8408, loss 2.92158e-05, acc 1\n",
      "2022-03-25T10:40:05.588634: step 8409, loss 0.00237581, acc 1\n",
      "2022-03-25T10:40:05.746495: step 8410, loss 8.15524e-05, acc 1\n",
      "2022-03-25T10:40:05.894257: step 8411, loss 0.000112272, acc 1\n",
      "2022-03-25T10:40:06.043038: step 8412, loss 9.51366e-06, acc 1\n",
      "2022-03-25T10:40:06.203954: step 8413, loss 0.00112075, acc 1\n",
      "2022-03-25T10:40:06.345217: step 8414, loss 0.000516384, acc 1\n",
      "2022-03-25T10:40:06.493748: step 8415, loss 0.000607309, acc 1\n",
      "2022-03-25T10:40:06.644793: step 8416, loss 6.64447e-05, acc 1\n",
      "2022-03-25T10:40:06.798214: step 8417, loss 3.06199e-05, acc 1\n",
      "2022-03-25T10:40:06.942247: step 8418, loss 0.000405282, acc 1\n",
      "2022-03-25T10:40:07.089136: step 8419, loss 0.000215404, acc 1\n",
      "2022-03-25T10:40:07.254142: step 8420, loss 0.000887636, acc 1\n",
      "2022-03-25T10:40:07.413445: step 8421, loss 0.000104857, acc 1\n",
      "2022-03-25T10:40:07.561116: step 8422, loss 5.1379e-05, acc 1\n",
      "2022-03-25T10:40:07.718601: step 8423, loss 0.00017427, acc 1\n",
      "2022-03-25T10:40:07.868621: step 8424, loss 7.40817e-05, acc 1\n",
      "2022-03-25T10:40:08.018194: step 8425, loss 4.0656e-05, acc 1\n",
      "2022-03-25T10:40:08.165678: step 8426, loss 0.0125364, acc 1\n",
      "2022-03-25T10:40:08.310632: step 8427, loss 0.000405715, acc 1\n",
      "2022-03-25T10:40:08.461242: step 8428, loss 0.00223597, acc 1\n",
      "2022-03-25T10:40:08.604170: step 8429, loss 0.000237829, acc 1\n",
      "2022-03-25T10:40:08.757588: step 8430, loss 3.13594e-05, acc 1\n",
      "2022-03-25T10:40:08.924961: step 8431, loss 0.0006116, acc 1\n",
      "2022-03-25T10:40:09.079600: step 8432, loss 2.65049e-06, acc 1\n",
      "2022-03-25T10:40:09.238790: step 8433, loss 4.60567e-05, acc 1\n",
      "2022-03-25T10:40:09.395153: step 8434, loss 0.000231999, acc 1\n",
      "2022-03-25T10:40:09.548303: step 8435, loss 0.0969798, acc 0.984375\n",
      "2022-03-25T10:40:09.696231: step 8436, loss 0.000178608, acc 1\n",
      "2022-03-25T10:40:09.843189: step 8437, loss 0.0037175, acc 1\n",
      "2022-03-25T10:40:09.986590: step 8438, loss 6.72199e-05, acc 1\n",
      "2022-03-25T10:40:10.134693: step 8439, loss 3.2931e-05, acc 1\n",
      "2022-03-25T10:40:10.288334: step 8440, loss 2.63546e-05, acc 1\n",
      "2022-03-25T10:40:10.436150: step 8441, loss 0.000192167, acc 1\n",
      "2022-03-25T10:40:10.586668: step 8442, loss 0.000109481, acc 1\n",
      "2022-03-25T10:40:10.736456: step 8443, loss 2.23681e-05, acc 1\n",
      "2022-03-25T10:40:10.892808: step 8444, loss 0.00499117, acc 1\n",
      "2022-03-25T10:40:11.043473: step 8445, loss 0.0012587, acc 1\n",
      "2022-03-25T10:40:11.193596: step 8446, loss 0.000194013, acc 1\n",
      "2022-03-25T10:40:11.349102: step 8447, loss 8.56407e-05, acc 1\n",
      "2022-03-25T10:40:11.489504: step 8448, loss 0.000648296, acc 1\n",
      "2022-03-25T10:40:11.635199: step 8449, loss 0.000308836, acc 1\n",
      "2022-03-25T10:40:11.788817: step 8450, loss 0.000120836, acc 1\n",
      "2022-03-25T10:40:11.941620: step 8451, loss 7.99947e-06, acc 1\n",
      "2022-03-25T10:40:12.088259: step 8452, loss 0.00125581, acc 1\n",
      "2022-03-25T10:40:12.238828: step 8453, loss 6.11168e-05, acc 1\n",
      "2022-03-25T10:40:12.379974: step 8454, loss 0.000651709, acc 1\n",
      "2022-03-25T10:40:12.521081: step 8455, loss 6.31703e-05, acc 1\n",
      "2022-03-25T10:40:12.666927: step 8456, loss 0.000683336, acc 1\n",
      "2022-03-25T10:40:12.814196: step 8457, loss 9.19192e-05, acc 1\n",
      "2022-03-25T10:40:12.963252: step 8458, loss 0.000473291, acc 1\n",
      "2022-03-25T10:40:13.118631: step 8459, loss 0.000253802, acc 1\n",
      "2022-03-25T10:40:13.260193: step 8460, loss 0.000182904, acc 1\n",
      "2022-03-25T10:40:13.407158: step 8461, loss 8.08991e-05, acc 1\n",
      "2022-03-25T10:40:13.551582: step 8462, loss 0.000204325, acc 1\n",
      "2022-03-25T10:40:13.688194: step 8463, loss 9.11908e-06, acc 1\n",
      "2022-03-25T10:40:13.844259: step 8464, loss 4.36754e-05, acc 1\n",
      "2022-03-25T10:40:13.995097: step 8465, loss 2.44699e-05, acc 1\n",
      "2022-03-25T10:40:14.142770: step 8466, loss 0.000127672, acc 1\n",
      "2022-03-25T10:40:14.282272: step 8467, loss 8.74179e-05, acc 1\n",
      "2022-03-25T10:40:14.436897: step 8468, loss 0.000704693, acc 1\n",
      "2022-03-25T10:40:14.586230: step 8469, loss 0.00337665, acc 1\n",
      "2022-03-25T10:40:14.735938: step 8470, loss 5.24394e-05, acc 1\n",
      "2022-03-25T10:40:14.882217: step 8471, loss 9.68603e-05, acc 1\n",
      "2022-03-25T10:40:15.029000: step 8472, loss 0.000341024, acc 1\n",
      "2022-03-25T10:40:15.175549: step 8473, loss 3.84959e-05, acc 1\n",
      "2022-03-25T10:40:15.321935: step 8474, loss 0.00714877, acc 1\n",
      "2022-03-25T10:40:15.480043: step 8475, loss 5.3284e-05, acc 1\n",
      "2022-03-25T10:40:15.630175: step 8476, loss 5.42308e-05, acc 1\n",
      "2022-03-25T10:40:15.785819: step 8477, loss 4.2796e-05, acc 1\n",
      "2022-03-25T10:40:15.925967: step 8478, loss 2.37854e-06, acc 1\n",
      "2022-03-25T10:40:16.076616: step 8479, loss 9.09988e-06, acc 1\n",
      "2022-03-25T10:40:16.226553: step 8480, loss 0.000734784, acc 1\n",
      "2022-03-25T10:40:16.375455: step 8481, loss 4.37385e-05, acc 1\n",
      "2022-03-25T10:40:16.523585: step 8482, loss 0.0001282, acc 1\n",
      "2022-03-25T10:40:16.665256: step 8483, loss 0.0301129, acc 0.984375\n",
      "2022-03-25T10:40:16.823267: step 8484, loss 3.84388e-05, acc 1\n",
      "2022-03-25T10:40:16.979868: step 8485, loss 0.00795458, acc 1\n",
      "2022-03-25T10:40:17.135014: step 8486, loss 0.000194779, acc 1\n",
      "2022-03-25T10:40:17.294175: step 8487, loss 0.000666392, acc 1\n",
      "2022-03-25T10:40:17.450378: step 8488, loss 2.58307e-05, acc 1\n",
      "2022-03-25T10:40:17.609023: step 8489, loss 1.37915e-05, acc 1\n",
      "2022-03-25T10:40:17.763761: step 8490, loss 3.34957e-05, acc 1\n",
      "2022-03-25T10:40:17.917805: step 8491, loss 0.000877823, acc 1\n",
      "2022-03-25T10:40:18.079658: step 8492, loss 2.80061e-05, acc 1\n",
      "2022-03-25T10:40:18.226519: step 8493, loss 0.0143708, acc 0.984375\n",
      "2022-03-25T10:40:18.377012: step 8494, loss 0.000758931, acc 1\n",
      "2022-03-25T10:40:18.531147: step 8495, loss 1.1949e-05, acc 1\n",
      "2022-03-25T10:40:18.676003: step 8496, loss 1.79928e-06, acc 1\n",
      "2022-03-25T10:40:18.828066: step 8497, loss 2.22587e-05, acc 1\n",
      "2022-03-25T10:40:18.974772: step 8498, loss 0.00235531, acc 1\n",
      "2022-03-25T10:40:19.123115: step 8499, loss 0.000599055, acc 1\n",
      "2022-03-25T10:40:19.268677: step 8500, loss 1.41866e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:40:19.417281: step 8500, loss 0.904353, acc 0.921512\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8500\n",
      "\n",
      "2022-03-25T10:40:19.670521: step 8501, loss 0.000797748, acc 1\n",
      "2022-03-25T10:40:19.816452: step 8502, loss 0.0333924, acc 0.984375\n",
      "2022-03-25T10:40:19.952356: step 8503, loss 3.16442e-06, acc 1\n",
      "2022-03-25T10:40:20.109065: step 8504, loss 0.00637324, acc 1\n",
      "2022-03-25T10:40:20.243380: step 8505, loss 0.000449193, acc 1\n",
      "2022-03-25T10:40:20.390481: step 8506, loss 0.00198608, acc 1\n",
      "2022-03-25T10:40:20.540318: step 8507, loss 3.89461e-06, acc 1\n",
      "2022-03-25T10:40:20.681724: step 8508, loss 8.18265e-06, acc 1\n",
      "2022-03-25T10:40:20.817683: step 8509, loss 0.000797825, acc 1\n",
      "2022-03-25T10:40:20.968380: step 8510, loss 1.61604e-05, acc 1\n",
      "2022-03-25T10:40:21.110117: step 8511, loss 0.0149908, acc 0.984375\n",
      "2022-03-25T10:40:21.256724: step 8512, loss 3.64047e-05, acc 1\n",
      "2022-03-25T10:40:21.403295: step 8513, loss 4.027e-05, acc 1\n",
      "2022-03-25T10:40:21.568279: step 8514, loss 8.32154e-06, acc 1\n",
      "2022-03-25T10:40:21.710582: step 8515, loss 0.000222343, acc 1\n",
      "2022-03-25T10:40:21.856873: step 8516, loss 0.00018552, acc 1\n",
      "2022-03-25T10:40:22.000027: step 8517, loss 0.000191603, acc 1\n",
      "2022-03-25T10:40:22.154174: step 8518, loss 0.00120172, acc 1\n",
      "2022-03-25T10:40:22.288982: step 8519, loss 0.000299807, acc 1\n",
      "2022-03-25T10:40:22.445419: step 8520, loss 6.94739e-05, acc 1\n",
      "2022-03-25T10:40:22.599176: step 8521, loss 0.000215578, acc 1\n",
      "2022-03-25T10:40:22.740236: step 8522, loss 0.000162051, acc 1\n",
      "2022-03-25T10:40:22.890895: step 8523, loss 0.000126305, acc 1\n",
      "2022-03-25T10:40:23.039632: step 8524, loss 0.000387727, acc 1\n",
      "2022-03-25T10:40:23.185704: step 8525, loss 0.000127438, acc 1\n",
      "2022-03-25T10:40:23.329162: step 8526, loss 0.019952, acc 0.984375\n",
      "2022-03-25T10:40:23.466801: step 8527, loss 0.000648998, acc 1\n",
      "2022-03-25T10:40:23.628165: step 8528, loss 0.00057148, acc 1\n",
      "2022-03-25T10:40:23.764164: step 8529, loss 4.92243e-05, acc 1\n",
      "2022-03-25T10:40:23.915797: step 8530, loss 0.00384013, acc 1\n",
      "2022-03-25T10:40:24.056280: step 8531, loss 8.36446e-05, acc 1\n",
      "2022-03-25T10:40:24.209054: step 8532, loss 4.4176e-05, acc 1\n",
      "2022-03-25T10:40:24.372001: step 8533, loss 0.000167432, acc 1\n",
      "2022-03-25T10:40:24.518414: step 8534, loss 0.000833582, acc 1\n",
      "2022-03-25T10:40:24.668191: step 8535, loss 0.000806138, acc 1\n",
      "2022-03-25T10:40:24.795701: step 8536, loss 1.92805e-06, acc 1\n",
      "2022-03-25T10:40:24.931553: step 8537, loss 4.0157e-05, acc 1\n",
      "2022-03-25T10:40:25.081717: step 8538, loss 3.88576e-05, acc 1\n",
      "2022-03-25T10:40:25.229321: step 8539, loss 3.08443e-05, acc 1\n",
      "2022-03-25T10:40:25.379001: step 8540, loss 0.0202718, acc 0.984375\n",
      "2022-03-25T10:40:25.522493: step 8541, loss 1.11712e-05, acc 1\n",
      "2022-03-25T10:40:25.688281: step 8542, loss 1.2203e-05, acc 1\n",
      "2022-03-25T10:40:25.837274: step 8543, loss 7.58216e-06, acc 1\n",
      "2022-03-25T10:40:25.984417: step 8544, loss 0.00301582, acc 1\n",
      "2022-03-25T10:40:26.134144: step 8545, loss 0.0312548, acc 0.984375\n",
      "2022-03-25T10:40:26.270443: step 8546, loss 6.09045e-06, acc 1\n",
      "2022-03-25T10:40:26.408252: step 8547, loss 0.000734471, acc 1\n",
      "2022-03-25T10:40:26.557021: step 8548, loss 0.00638031, acc 1\n",
      "2022-03-25T10:40:26.709515: step 8549, loss 6.34039e-05, acc 1\n",
      "2022-03-25T10:40:26.852000: step 8550, loss 0.000347102, acc 1\n",
      "2022-03-25T10:40:26.987089: step 8551, loss 0.000671154, acc 1\n",
      "2022-03-25T10:40:27.138785: step 8552, loss 0.0211379, acc 0.984375\n",
      "2022-03-25T10:40:27.271927: step 8553, loss 4.77891e-05, acc 1\n",
      "2022-03-25T10:40:27.422306: step 8554, loss 9.42086e-05, acc 1\n",
      "2022-03-25T10:40:27.569926: step 8555, loss 1.69471e-05, acc 1\n",
      "2022-03-25T10:40:27.732116: step 8556, loss 0.000131699, acc 1\n",
      "2022-03-25T10:40:27.875652: step 8557, loss 4.59498e-05, acc 1\n",
      "2022-03-25T10:40:28.020428: step 8558, loss 8.81138e-05, acc 1\n",
      "2022-03-25T10:40:28.166544: step 8559, loss 0.000350103, acc 1\n",
      "2022-03-25T10:40:28.310503: step 8560, loss 9.93705e-05, acc 1\n",
      "2022-03-25T10:40:28.462391: step 8561, loss 0.0116575, acc 0.984375\n",
      "2022-03-25T10:40:28.606831: step 8562, loss 9.38462e-05, acc 1\n",
      "2022-03-25T10:40:28.755766: step 8563, loss 3.53382e-05, acc 1\n",
      "2022-03-25T10:40:28.909630: step 8564, loss 0.0241148, acc 0.984375\n",
      "2022-03-25T10:40:29.049543: step 8565, loss 0.00348807, acc 1\n",
      "2022-03-25T10:40:29.201577: step 8566, loss 2.77713e-06, acc 1\n",
      "2022-03-25T10:40:29.344453: step 8567, loss 3.94687e-05, acc 1\n",
      "2022-03-25T10:40:29.499169: step 8568, loss 0.00882126, acc 1\n",
      "2022-03-25T10:40:29.640541: step 8569, loss 0.000361557, acc 1\n",
      "2022-03-25T10:40:29.797774: step 8570, loss 3.89088e-06, acc 1\n",
      "2022-03-25T10:40:29.947264: step 8571, loss 0.00574417, acc 1\n",
      "2022-03-25T10:40:30.086183: step 8572, loss 1.43807e-05, acc 1\n",
      "2022-03-25T10:40:30.223616: step 8573, loss 6.59476e-05, acc 1\n",
      "2022-03-25T10:40:30.367312: step 8574, loss 0.00019312, acc 1\n",
      "2022-03-25T10:40:30.511852: step 8575, loss 3.17939e-05, acc 1\n",
      "2022-03-25T10:40:30.658995: step 8576, loss 0.000139605, acc 1\n",
      "2022-03-25T10:40:30.812043: step 8577, loss 0.000146756, acc 1\n",
      "2022-03-25T10:40:30.966630: step 8578, loss 0.000204123, acc 1\n",
      "2022-03-25T10:40:31.117114: step 8579, loss 0.000507226, acc 1\n",
      "2022-03-25T10:40:31.263274: step 8580, loss 0.0166642, acc 0.984375\n",
      "2022-03-25T10:40:31.407606: step 8581, loss 0.000857568, acc 1\n",
      "2022-03-25T10:40:31.549958: step 8582, loss 8.26389e-05, acc 1\n",
      "2022-03-25T10:40:31.690746: step 8583, loss 0.000269294, acc 1\n",
      "2022-03-25T10:40:31.850189: step 8584, loss 6.094e-05, acc 1\n",
      "2022-03-25T10:40:31.984437: step 8585, loss 0.000421062, acc 1\n",
      "2022-03-25T10:40:32.136390: step 8586, loss 0.00314936, acc 1\n",
      "2022-03-25T10:40:32.273598: step 8587, loss 0.000306733, acc 1\n",
      "2022-03-25T10:40:32.421535: step 8588, loss 0.00151701, acc 1\n",
      "2022-03-25T10:40:32.560220: step 8589, loss 1.2691e-05, acc 1\n",
      "2022-03-25T10:40:32.704662: step 8590, loss 0.0141139, acc 0.984375\n",
      "2022-03-25T10:40:32.851800: step 8591, loss 0.000617155, acc 1\n",
      "2022-03-25T10:40:33.000661: step 8592, loss 0.00211462, acc 1\n",
      "2022-03-25T10:40:33.141958: step 8593, loss 0.000540742, acc 1\n",
      "2022-03-25T10:40:33.292844: step 8594, loss 0.000447812, acc 1\n",
      "2022-03-25T10:40:33.434169: step 8595, loss 0.000419736, acc 1\n",
      "2022-03-25T10:40:33.584879: step 8596, loss 0.00243133, acc 1\n",
      "2022-03-25T10:40:33.728216: step 8597, loss 0.00105852, acc 1\n",
      "2022-03-25T10:40:33.880513: step 8598, loss 0.0302177, acc 0.984375\n",
      "2022-03-25T10:40:34.011934: step 8599, loss 0.000178622, acc 1\n",
      "2022-03-25T10:40:34.161546: step 8600, loss 2.25961e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:40:34.296546: step 8600, loss 0.811238, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8600\n",
      "\n",
      "2022-03-25T10:40:34.565705: step 8601, loss 0.000413897, acc 1\n",
      "2022-03-25T10:40:34.703907: step 8602, loss 2.30037e-05, acc 1\n",
      "2022-03-25T10:40:34.838667: step 8603, loss 0.0015065, acc 1\n",
      "2022-03-25T10:40:34.999217: step 8604, loss 0.000113388, acc 1\n",
      "2022-03-25T10:40:35.155154: step 8605, loss 0.0038417, acc 1\n",
      "2022-03-25T10:40:35.299759: step 8606, loss 0.0161738, acc 1\n",
      "2022-03-25T10:40:35.444604: step 8607, loss 0.0014575, acc 1\n",
      "2022-03-25T10:40:35.589153: step 8608, loss 0.00261767, acc 1\n",
      "2022-03-25T10:40:35.728945: step 8609, loss 0.000116929, acc 1\n",
      "2022-03-25T10:40:35.867005: step 8610, loss 7.98459e-06, acc 1\n",
      "2022-03-25T10:40:36.024630: step 8611, loss 0.000663376, acc 1\n",
      "2022-03-25T10:40:36.156758: step 8612, loss 4.06196e-05, acc 1\n",
      "2022-03-25T10:40:36.296925: step 8613, loss 6.17444e-06, acc 1\n",
      "2022-03-25T10:40:36.442519: step 8614, loss 0.000909668, acc 1\n",
      "2022-03-25T10:40:36.582307: step 8615, loss 2.52088e-05, acc 1\n",
      "2022-03-25T10:40:36.728186: step 8616, loss 7.0037e-05, acc 1\n",
      "2022-03-25T10:40:36.864116: step 8617, loss 3.65616e-05, acc 1\n",
      "2022-03-25T10:40:37.016638: step 8618, loss 0.000234486, acc 1\n",
      "2022-03-25T10:40:37.157351: step 8619, loss 3.03651e-05, acc 1\n",
      "2022-03-25T10:40:37.302962: step 8620, loss 0.000799716, acc 1\n",
      "2022-03-25T10:40:37.445274: step 8621, loss 0.001848, acc 1\n",
      "2022-03-25T10:40:37.600245: step 8622, loss 0.0021664, acc 1\n",
      "2022-03-25T10:40:37.749577: step 8623, loss 0.000118016, acc 1\n",
      "2022-03-25T10:40:37.888862: step 8624, loss 0.000995574, acc 1\n",
      "2022-03-25T10:40:38.027200: step 8625, loss 0.0197806, acc 0.984375\n",
      "2022-03-25T10:40:38.176652: step 8626, loss 0.000865905, acc 1\n",
      "2022-03-25T10:40:38.312102: step 8627, loss 4.3777e-05, acc 1\n",
      "2022-03-25T10:40:38.456978: step 8628, loss 9.70001e-06, acc 1\n",
      "2022-03-25T10:40:38.599575: step 8629, loss 0.00219028, acc 1\n",
      "2022-03-25T10:40:38.733294: step 8630, loss 0.000122663, acc 1\n",
      "2022-03-25T10:40:38.866576: step 8631, loss 3.43157e-05, acc 1\n",
      "2022-03-25T10:40:39.025012: step 8632, loss 3.0649e-05, acc 1\n",
      "2022-03-25T10:40:39.172191: step 8633, loss 3.74927e-06, acc 1\n",
      "2022-03-25T10:40:39.307010: step 8634, loss 4.50439e-05, acc 1\n",
      "2022-03-25T10:40:39.443250: step 8635, loss 1.81169e-05, acc 1\n",
      "2022-03-25T10:40:39.587015: step 8636, loss 4.34816e-05, acc 1\n",
      "2022-03-25T10:40:39.721097: step 8637, loss 0.000252132, acc 1\n",
      "2022-03-25T10:40:39.873730: step 8638, loss 8.36134e-05, acc 1\n",
      "2022-03-25T10:40:40.022216: step 8639, loss 0.0178491, acc 0.984375\n",
      "2022-03-25T10:40:40.165210: step 8640, loss 2.0255e-05, acc 1\n",
      "2022-03-25T10:40:40.301191: step 8641, loss 0.0040773, acc 1\n",
      "2022-03-25T10:40:40.440588: step 8642, loss 0.000181834, acc 1\n",
      "2022-03-25T10:40:40.570781: step 8643, loss 5.54809e-05, acc 1\n",
      "2022-03-25T10:40:40.716948: step 8644, loss 4.31901e-05, acc 1\n",
      "2022-03-25T10:40:40.848887: step 8645, loss 2.21518e-05, acc 1\n",
      "2022-03-25T10:40:41.002591: step 8646, loss 0.000458834, acc 1\n",
      "2022-03-25T10:40:41.148518: step 8647, loss 0.000143256, acc 1\n",
      "2022-03-25T10:40:41.287618: step 8648, loss 0.000308824, acc 1\n",
      "2022-03-25T10:40:41.428866: step 8649, loss 0.000118813, acc 1\n",
      "2022-03-25T10:40:41.581388: step 8650, loss 0.00489261, acc 1\n",
      "2022-03-25T10:40:41.726816: step 8651, loss 0.000106623, acc 1\n",
      "2022-03-25T10:40:41.867484: step 8652, loss 1.74253e-05, acc 1\n",
      "2022-03-25T10:40:41.999300: step 8653, loss 7.00864e-05, acc 1\n",
      "2022-03-25T10:40:42.159448: step 8654, loss 1.85739e-05, acc 1\n",
      "2022-03-25T10:40:42.295144: step 8655, loss 0.000118322, acc 1\n",
      "2022-03-25T10:40:42.439618: step 8656, loss 7.04589e-05, acc 1\n",
      "2022-03-25T10:40:42.570001: step 8657, loss 0.000181957, acc 1\n",
      "2022-03-25T10:40:42.715404: step 8658, loss 0.000220704, acc 1\n",
      "2022-03-25T10:40:42.856330: step 8659, loss 0.000282686, acc 1\n",
      "2022-03-25T10:40:42.998286: step 8660, loss 8.83634e-05, acc 1\n",
      "2022-03-25T10:40:43.144981: step 8661, loss 0.0246358, acc 0.984375\n",
      "2022-03-25T10:40:43.290409: step 8662, loss 0.000165719, acc 1\n",
      "2022-03-25T10:40:43.429827: step 8663, loss 1.81269e-05, acc 1\n",
      "2022-03-25T10:40:43.576945: step 8664, loss 0.00702536, acc 1\n",
      "2022-03-25T10:40:43.715278: step 8665, loss 0.000132532, acc 1\n",
      "2022-03-25T10:40:43.858204: step 8666, loss 0.00112574, acc 1\n",
      "2022-03-25T10:40:43.992913: step 8667, loss 0.00066826, acc 1\n",
      "2022-03-25T10:40:44.142796: step 8668, loss 5.06806e-06, acc 1\n",
      "2022-03-25T10:40:44.276686: step 8669, loss 6.79651e-06, acc 1\n",
      "2022-03-25T10:40:44.422207: step 8670, loss 1.1527e-05, acc 1\n",
      "2022-03-25T10:40:44.556111: step 8671, loss 0.00121427, acc 1\n",
      "2022-03-25T10:40:44.703390: step 8672, loss 5.38332e-05, acc 1\n",
      "2022-03-25T10:40:44.836033: step 8673, loss 2.96483e-05, acc 1\n",
      "2022-03-25T10:40:44.983235: step 8674, loss 9.41889e-06, acc 1\n",
      "2022-03-25T10:40:45.138826: step 8675, loss 0.00473663, acc 1\n",
      "2022-03-25T10:40:45.280191: step 8676, loss 0.000474523, acc 1\n",
      "2022-03-25T10:40:45.425690: step 8677, loss 0.00286432, acc 1\n",
      "2022-03-25T10:40:45.562667: step 8678, loss 6.31434e-07, acc 1\n",
      "2022-03-25T10:40:45.701088: step 8679, loss 5.01241e-05, acc 1\n",
      "2022-03-25T10:40:45.846157: step 8680, loss 0.000151104, acc 1\n",
      "2022-03-25T10:40:45.993573: step 8681, loss 0.000168208, acc 1\n",
      "2022-03-25T10:40:46.142606: step 8682, loss 2.57465e-05, acc 1\n",
      "2022-03-25T10:40:46.278304: step 8683, loss 5.83356e-06, acc 1\n",
      "2022-03-25T10:40:46.419600: step 8684, loss 0.000704014, acc 1\n",
      "2022-03-25T10:40:46.553233: step 8685, loss 0.000221739, acc 1\n",
      "2022-03-25T10:40:46.694653: step 8686, loss 4.61811e-05, acc 1\n",
      "2022-03-25T10:40:46.828264: step 8687, loss 6.686e-05, acc 1\n",
      "2022-03-25T10:40:46.967488: step 8688, loss 0.000329136, acc 1\n",
      "2022-03-25T10:40:47.108580: step 8689, loss 0.000122963, acc 1\n",
      "2022-03-25T10:40:47.254340: step 8690, loss 0.0144447, acc 0.984375\n",
      "2022-03-25T10:40:47.398865: step 8691, loss 0.000145726, acc 1\n",
      "2022-03-25T10:40:47.550722: step 8692, loss 0.000457771, acc 1\n",
      "2022-03-25T10:40:47.697897: step 8693, loss 0.00270212, acc 1\n",
      "2022-03-25T10:40:47.841968: step 8694, loss 0.000529711, acc 1\n",
      "2022-03-25T10:40:47.972727: step 8695, loss 0.000450927, acc 1\n",
      "2022-03-25T10:40:48.119264: step 8696, loss 0.000129155, acc 1\n",
      "2022-03-25T10:40:48.253719: step 8697, loss 0.000438657, acc 1\n",
      "2022-03-25T10:40:48.397730: step 8698, loss 6.8723e-05, acc 1\n",
      "2022-03-25T10:40:48.542656: step 8699, loss 8.76313e-05, acc 1\n",
      "2022-03-25T10:40:48.691405: step 8700, loss 4.1284e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:40:48.815190: step 8700, loss 0.862935, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8700\n",
      "\n",
      "2022-03-25T10:40:49.056362: step 8701, loss 0.00176503, acc 1\n",
      "2022-03-25T10:40:49.200956: step 8702, loss 0.000508261, acc 1\n",
      "2022-03-25T10:40:49.336651: step 8703, loss 0.0010406, acc 1\n",
      "2022-03-25T10:40:49.475851: step 8704, loss 8.8142e-05, acc 1\n",
      "2022-03-25T10:40:49.622971: step 8705, loss 1.10974e-05, acc 1\n",
      "2022-03-25T10:40:49.774981: step 8706, loss 7.87415e-06, acc 1\n",
      "2022-03-25T10:40:49.909394: step 8707, loss 2.09169e-05, acc 1\n",
      "2022-03-25T10:40:50.055474: step 8708, loss 2.51673e-05, acc 1\n",
      "2022-03-25T10:40:50.191768: step 8709, loss 0.00136354, acc 1\n",
      "2022-03-25T10:40:50.341192: step 8710, loss 6.29529e-06, acc 1\n",
      "2022-03-25T10:40:50.478457: step 8711, loss 9.49129e-05, acc 1\n",
      "2022-03-25T10:40:50.624704: step 8712, loss 5.66387e-06, acc 1\n",
      "2022-03-25T10:40:50.767940: step 8713, loss 7.31134e-05, acc 1\n",
      "2022-03-25T10:40:50.914673: step 8714, loss 0.000178077, acc 1\n",
      "2022-03-25T10:40:51.050851: step 8715, loss 8.73515e-05, acc 1\n",
      "2022-03-25T10:40:51.196248: step 8716, loss 1.01474e-05, acc 1\n",
      "2022-03-25T10:40:51.348764: step 8717, loss 0.000259588, acc 1\n",
      "2022-03-25T10:40:51.487445: step 8718, loss 1.59986e-05, acc 1\n",
      "2022-03-25T10:40:51.638666: step 8719, loss 0.000168662, acc 1\n",
      "2022-03-25T10:40:51.780294: step 8720, loss 2.36386e-05, acc 1\n",
      "2022-03-25T10:40:51.916798: step 8721, loss 0.00051941, acc 1\n",
      "2022-03-25T10:40:52.052975: step 8722, loss 0.000870443, acc 1\n",
      "2022-03-25T10:40:52.187587: step 8723, loss 1.85445e-05, acc 1\n",
      "2022-03-25T10:40:52.336565: step 8724, loss 0.0238659, acc 0.984375\n",
      "2022-03-25T10:40:52.475751: step 8725, loss 6.13677e-06, acc 1\n",
      "2022-03-25T10:40:52.627036: step 8726, loss 6.56353e-05, acc 1\n",
      "2022-03-25T10:40:52.763542: step 8727, loss 9.43571e-05, acc 1\n",
      "2022-03-25T10:40:52.911652: step 8728, loss 1.05208e-05, acc 1\n",
      "2022-03-25T10:40:53.059022: step 8729, loss 0.00131897, acc 1\n",
      "2022-03-25T10:40:53.180424: step 8730, loss 1.23354e-06, acc 1\n",
      "2022-03-25T10:40:53.336145: step 8731, loss 0.000746969, acc 1\n",
      "2022-03-25T10:40:53.487257: step 8732, loss 0.00939467, acc 1\n",
      "2022-03-25T10:40:53.619796: step 8733, loss 2.69706e-06, acc 1\n",
      "2022-03-25T10:40:53.763906: step 8734, loss 3.61174e-05, acc 1\n",
      "2022-03-25T10:40:53.896900: step 8735, loss 6.08249e-05, acc 1\n",
      "2022-03-25T10:40:54.037655: step 8736, loss 0.00562042, acc 1\n",
      "2022-03-25T10:40:54.175270: step 8737, loss 6.96717e-05, acc 1\n",
      "2022-03-25T10:40:54.324995: step 8738, loss 5.81852e-05, acc 1\n",
      "2022-03-25T10:40:54.456216: step 8739, loss 2.21781e-05, acc 1\n",
      "2022-03-25T10:40:54.604230: step 8740, loss 0.000104871, acc 1\n",
      "2022-03-25T10:40:54.743277: step 8741, loss 0.000692228, acc 1\n",
      "2022-03-25T10:40:54.894640: step 8742, loss 4.8286e-05, acc 1\n",
      "2022-03-25T10:40:55.030675: step 8743, loss 0.00106372, acc 1\n",
      "2022-03-25T10:40:55.175200: step 8744, loss 0.000417884, acc 1\n",
      "2022-03-25T10:40:55.307750: step 8745, loss 1.61214e-05, acc 1\n",
      "2022-03-25T10:40:55.455267: step 8746, loss 1.77447e-05, acc 1\n",
      "2022-03-25T10:40:55.598392: step 8747, loss 0.000382811, acc 1\n",
      "2022-03-25T10:40:55.745791: step 8748, loss 0.000203847, acc 1\n",
      "2022-03-25T10:40:55.880576: step 8749, loss 4.18703e-06, acc 1\n",
      "2022-03-25T10:40:56.025285: step 8750, loss 1.59068e-06, acc 1\n",
      "2022-03-25T10:40:56.173790: step 8751, loss 0.000106935, acc 1\n",
      "2022-03-25T10:40:56.315281: step 8752, loss 2.821e-05, acc 1\n",
      "2022-03-25T10:40:56.448836: step 8753, loss 0.023642, acc 0.984375\n",
      "2022-03-25T10:40:56.593072: step 8754, loss 5.64595e-05, acc 1\n",
      "2022-03-25T10:40:56.735238: step 8755, loss 3.38826e-05, acc 1\n",
      "2022-03-25T10:40:56.889119: step 8756, loss 0.037774, acc 0.984375\n",
      "2022-03-25T10:40:57.034987: step 8757, loss 0.00230714, acc 1\n",
      "2022-03-25T10:40:57.176046: step 8758, loss 8.85031e-06, acc 1\n",
      "2022-03-25T10:40:57.309978: step 8759, loss 0.000173665, acc 1\n",
      "2022-03-25T10:40:57.453395: step 8760, loss 5.69249e-05, acc 1\n",
      "2022-03-25T10:40:57.583365: step 8761, loss 0.00248824, acc 1\n",
      "2022-03-25T10:40:57.736562: step 8762, loss 0.000135998, acc 1\n",
      "2022-03-25T10:40:57.870942: step 8763, loss 0.00012704, acc 1\n",
      "2022-03-25T10:40:58.016907: step 8764, loss 0.0485294, acc 0.984375\n",
      "2022-03-25T10:40:58.150900: step 8765, loss 0.0131709, acc 0.984375\n",
      "2022-03-25T10:40:58.295243: step 8766, loss 0.000589766, acc 1\n",
      "2022-03-25T10:40:58.450613: step 8767, loss 0.000150796, acc 1\n",
      "2022-03-25T10:40:58.588009: step 8768, loss 0.00365683, acc 1\n",
      "2022-03-25T10:40:58.730151: step 8769, loss 0.00165307, acc 1\n",
      "2022-03-25T10:40:58.875260: step 8770, loss 0.0942288, acc 0.984375\n",
      "2022-03-25T10:40:59.010714: step 8771, loss 0.00100369, acc 1\n",
      "2022-03-25T10:40:59.167202: step 8772, loss 0.0032215, acc 1\n",
      "2022-03-25T10:40:59.309203: step 8773, loss 0.000380354, acc 1\n",
      "2022-03-25T10:40:59.463616: step 8774, loss 0.0168296, acc 0.984375\n",
      "2022-03-25T10:40:59.610636: step 8775, loss 0.000131332, acc 1\n",
      "2022-03-25T10:40:59.764526: step 8776, loss 0.000459726, acc 1\n",
      "2022-03-25T10:40:59.904543: step 8777, loss 2.38421e-05, acc 1\n",
      "2022-03-25T10:41:00.047191: step 8778, loss 1.12523e-05, acc 1\n",
      "2022-03-25T10:41:00.186217: step 8779, loss 0.00096973, acc 1\n",
      "2022-03-25T10:41:00.333116: step 8780, loss 6.30455e-05, acc 1\n",
      "2022-03-25T10:41:00.479500: step 8781, loss 0.0461778, acc 0.96875\n",
      "2022-03-25T10:41:00.623310: step 8782, loss 2.45728e-05, acc 1\n",
      "2022-03-25T10:41:00.763818: step 8783, loss 0.000136832, acc 1\n",
      "2022-03-25T10:41:00.918606: step 8784, loss 0.00289219, acc 1\n",
      "2022-03-25T10:41:01.058550: step 8785, loss 0.000361262, acc 1\n",
      "2022-03-25T10:41:01.198382: step 8786, loss 4.48498e-06, acc 1\n",
      "2022-03-25T10:41:01.334839: step 8787, loss 0.000148438, acc 1\n",
      "2022-03-25T10:41:01.493169: step 8788, loss 2.4333e-05, acc 1\n",
      "2022-03-25T10:41:01.631450: step 8789, loss 0.0002512, acc 1\n",
      "2022-03-25T10:41:01.786270: step 8790, loss 0.068672, acc 0.984375\n",
      "2022-03-25T10:41:01.933588: step 8791, loss 0.000352122, acc 1\n",
      "2022-03-25T10:41:02.073588: step 8792, loss 0.00225367, acc 1\n",
      "2022-03-25T10:41:02.211064: step 8793, loss 0.00478712, acc 1\n",
      "2022-03-25T10:41:02.356118: step 8794, loss 4.45445e-05, acc 1\n",
      "2022-03-25T10:41:02.503291: step 8795, loss 0.0163229, acc 0.984375\n",
      "2022-03-25T10:41:02.654308: step 8796, loss 4.62169e-05, acc 1\n",
      "2022-03-25T10:41:02.792487: step 8797, loss 3.99285e-05, acc 1\n",
      "2022-03-25T10:41:02.941173: step 8798, loss 6.99435e-05, acc 1\n",
      "2022-03-25T10:41:03.079314: step 8799, loss 0.000177944, acc 1\n",
      "2022-03-25T10:41:03.234891: step 8800, loss 0.000329408, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:41:03.358553: step 8800, loss 0.811047, acc 0.917151\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8800\n",
      "\n",
      "2022-03-25T10:41:03.614433: step 8801, loss 0.000429584, acc 1\n",
      "2022-03-25T10:41:03.765257: step 8802, loss 0.00338583, acc 1\n",
      "2022-03-25T10:41:03.900245: step 8803, loss 0.0016908, acc 1\n",
      "2022-03-25T10:41:04.057641: step 8804, loss 4.72044e-05, acc 1\n",
      "2022-03-25T10:41:04.213170: step 8805, loss 1.32403e-05, acc 1\n",
      "2022-03-25T10:41:04.359020: step 8806, loss 4.80082e-05, acc 1\n",
      "2022-03-25T10:41:04.517432: step 8807, loss 0.000149439, acc 1\n",
      "2022-03-25T10:41:04.670215: step 8808, loss 1.70394e-05, acc 1\n",
      "2022-03-25T10:41:04.822175: step 8809, loss 0.00712677, acc 1\n",
      "2022-03-25T10:41:04.972911: step 8810, loss 0.00178366, acc 1\n",
      "2022-03-25T10:41:05.122541: step 8811, loss 6.35123e-06, acc 1\n",
      "2022-03-25T10:41:05.279039: step 8812, loss 7.166e-05, acc 1\n",
      "2022-03-25T10:41:05.423384: step 8813, loss 2.59866e-05, acc 1\n",
      "2022-03-25T10:41:05.577869: step 8814, loss 0.000152761, acc 1\n",
      "2022-03-25T10:41:05.736904: step 8815, loss 0.00109814, acc 1\n",
      "2022-03-25T10:41:05.880195: step 8816, loss 0.000111136, acc 1\n",
      "2022-03-25T10:41:06.033614: step 8817, loss 4.68883e-05, acc 1\n",
      "2022-03-25T10:41:06.179727: step 8818, loss 5.54512e-05, acc 1\n",
      "2022-03-25T10:41:06.333142: step 8819, loss 6.11658e-06, acc 1\n",
      "2022-03-25T10:41:06.481371: step 8820, loss 1.00445e-05, acc 1\n",
      "2022-03-25T10:41:06.633172: step 8821, loss 0.0026889, acc 1\n",
      "2022-03-25T10:41:06.781255: step 8822, loss 0.0026702, acc 1\n",
      "2022-03-25T10:41:06.946958: step 8823, loss 0.00358772, acc 1\n",
      "2022-03-25T10:41:07.094793: step 8824, loss 3.00907e-05, acc 1\n",
      "2022-03-25T10:41:07.253581: step 8825, loss 9.23635e-06, acc 1\n",
      "2022-03-25T10:41:07.409981: step 8826, loss 7.93606e-06, acc 1\n",
      "2022-03-25T10:41:07.561073: step 8827, loss 8.56181e-06, acc 1\n",
      "2022-03-25T10:41:07.720955: step 8828, loss 0.00265704, acc 1\n",
      "2022-03-25T10:41:07.870566: step 8829, loss 0.000176661, acc 1\n",
      "2022-03-25T10:41:08.018232: step 8830, loss 0.00917625, acc 1\n",
      "2022-03-25T10:41:08.166946: step 8831, loss 0.000192862, acc 1\n",
      "2022-03-25T10:41:08.321170: step 8832, loss 0.000489098, acc 1\n",
      "2022-03-25T10:41:08.470603: step 8833, loss 3.62528e-05, acc 1\n",
      "2022-03-25T10:41:08.629423: step 8834, loss 0.00190561, acc 1\n",
      "2022-03-25T10:41:08.788685: step 8835, loss 7.01104e-05, acc 1\n",
      "2022-03-25T10:41:08.938121: step 8836, loss 0.000515758, acc 1\n",
      "2022-03-25T10:41:09.087533: step 8837, loss 0.000384995, acc 1\n",
      "2022-03-25T10:41:09.230191: step 8838, loss 0.000169209, acc 1\n",
      "2022-03-25T10:41:09.373515: step 8839, loss 0.000109691, acc 1\n",
      "2022-03-25T10:41:09.518813: step 8840, loss 0.000899583, acc 1\n",
      "2022-03-25T10:41:09.683552: step 8841, loss 0.0234694, acc 0.984375\n",
      "2022-03-25T10:41:09.837280: step 8842, loss 5.49802e-05, acc 1\n",
      "2022-03-25T10:41:09.990584: step 8843, loss 0.000355309, acc 1\n",
      "2022-03-25T10:41:10.142285: step 8844, loss 0.00717661, acc 1\n",
      "2022-03-25T10:41:10.291842: step 8845, loss 5.82598e-05, acc 1\n",
      "2022-03-25T10:41:10.476032: step 8846, loss 5.90651e-05, acc 1\n",
      "2022-03-25T10:41:10.616384: step 8847, loss 8.39503e-05, acc 1\n",
      "2022-03-25T10:41:10.768025: step 8848, loss 5.21683e-05, acc 1\n",
      "2022-03-25T10:41:10.930328: step 8849, loss 0.00434686, acc 1\n",
      "2022-03-25T10:41:11.082910: step 8850, loss 0.000399695, acc 1\n",
      "2022-03-25T10:41:11.246429: step 8851, loss 6.0014e-05, acc 1\n",
      "2022-03-25T10:41:11.398231: step 8852, loss 3.8775e-05, acc 1\n",
      "2022-03-25T10:41:11.558887: step 8853, loss 0.000243259, acc 1\n",
      "2022-03-25T10:41:11.727337: step 8854, loss 0.00089399, acc 1\n",
      "2022-03-25T10:41:11.882577: step 8855, loss 0.000188853, acc 1\n",
      "2022-03-25T10:41:12.031150: step 8856, loss 0.000624988, acc 1\n",
      "2022-03-25T10:41:12.184487: step 8857, loss 4.96791e-05, acc 1\n",
      "2022-03-25T10:41:12.328537: step 8858, loss 8.88163e-05, acc 1\n",
      "2022-03-25T10:41:12.479015: step 8859, loss 1.15046e-05, acc 1\n",
      "2022-03-25T10:41:12.632424: step 8860, loss 0.000145063, acc 1\n",
      "2022-03-25T10:41:12.776757: step 8861, loss 6.08832e-05, acc 1\n",
      "2022-03-25T10:41:12.922772: step 8862, loss 7.59028e-05, acc 1\n",
      "2022-03-25T10:41:13.077281: step 8863, loss 0.000205843, acc 1\n",
      "2022-03-25T10:41:13.224914: step 8864, loss 0.000235794, acc 1\n",
      "2022-03-25T10:41:13.385762: step 8865, loss 0.0730842, acc 0.984375\n",
      "2022-03-25T10:41:13.535714: step 8866, loss 0.000194001, acc 1\n",
      "2022-03-25T10:41:13.699820: step 8867, loss 1.42805e-05, acc 1\n",
      "2022-03-25T10:41:13.858138: step 8868, loss 0.000470187, acc 1\n",
      "2022-03-25T10:41:14.004743: step 8869, loss 0.00048777, acc 1\n",
      "2022-03-25T10:41:14.159925: step 8870, loss 0.00397229, acc 1\n",
      "2022-03-25T10:41:14.314134: step 8871, loss 0.0106526, acc 1\n",
      "2022-03-25T10:41:14.454357: step 8872, loss 0.000136953, acc 1\n",
      "2022-03-25T10:41:14.601452: step 8873, loss 0.00358744, acc 1\n",
      "2022-03-25T10:41:14.758935: step 8874, loss 0.00234744, acc 1\n",
      "2022-03-25T10:41:14.920014: step 8875, loss 0.00233871, acc 1\n",
      "2022-03-25T10:41:15.075421: step 8876, loss 0.00210446, acc 1\n",
      "2022-03-25T10:41:15.230476: step 8877, loss 7.01129e-05, acc 1\n",
      "2022-03-25T10:41:15.383029: step 8878, loss 0.00194077, acc 1\n",
      "2022-03-25T10:41:15.545531: step 8879, loss 0.00274937, acc 1\n",
      "2022-03-25T10:41:15.690344: step 8880, loss 0.00341458, acc 1\n",
      "2022-03-25T10:41:15.844251: step 8881, loss 4.55601e-05, acc 1\n",
      "2022-03-25T10:41:16.001803: step 8882, loss 6.19911e-05, acc 1\n",
      "2022-03-25T10:41:16.152941: step 8883, loss 0.00121316, acc 1\n",
      "2022-03-25T10:41:16.306073: step 8884, loss 1.81073e-05, acc 1\n",
      "2022-03-25T10:41:16.456337: step 8885, loss 5.28349e-05, acc 1\n",
      "2022-03-25T10:41:16.599253: step 8886, loss 2.79126e-05, acc 1\n",
      "2022-03-25T10:41:16.749208: step 8887, loss 0.000477623, acc 1\n",
      "2022-03-25T10:41:16.906308: step 8888, loss 0.000648297, acc 1\n",
      "2022-03-25T10:41:17.060416: step 8889, loss 1.29111e-05, acc 1\n",
      "2022-03-25T10:41:17.206825: step 8890, loss 5.48838e-05, acc 1\n",
      "2022-03-25T10:41:17.360473: step 8891, loss 0.000815995, acc 1\n",
      "2022-03-25T10:41:17.510875: step 8892, loss 0.000152632, acc 1\n",
      "2022-03-25T10:41:17.657911: step 8893, loss 4.85936e-06, acc 1\n",
      "2022-03-25T10:41:17.808696: step 8894, loss 7.27273e-05, acc 1\n",
      "2022-03-25T10:41:17.958305: step 8895, loss 4.93422e-05, acc 1\n",
      "2022-03-25T10:41:18.105030: step 8896, loss 6.63163e-05, acc 1\n",
      "2022-03-25T10:41:18.257428: step 8897, loss 2.78858e-05, acc 1\n",
      "2022-03-25T10:41:18.402275: step 8898, loss 0.000142335, acc 1\n",
      "2022-03-25T10:41:18.551463: step 8899, loss 1.85902e-05, acc 1\n",
      "2022-03-25T10:41:18.696812: step 8900, loss 2.05914e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:41:18.846043: step 8900, loss 0.911987, acc 0.917151\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-8900\n",
      "\n",
      "2022-03-25T10:41:19.122282: step 8901, loss 0.000568624, acc 1\n",
      "2022-03-25T10:41:19.278825: step 8902, loss 0.0127862, acc 1\n",
      "2022-03-25T10:41:19.442350: step 8903, loss 0.0240508, acc 0.984375\n",
      "2022-03-25T10:41:19.590249: step 8904, loss 6.27311e-06, acc 1\n",
      "2022-03-25T10:41:19.740267: step 8905, loss 1.56682e-05, acc 1\n",
      "2022-03-25T10:41:19.904070: step 8906, loss 5.61725e-05, acc 1\n",
      "2022-03-25T10:41:20.057196: step 8907, loss 0.00593248, acc 1\n",
      "2022-03-25T10:41:20.204814: step 8908, loss 3.31906e-06, acc 1\n",
      "2022-03-25T10:41:20.347876: step 8909, loss 0.00188263, acc 1\n",
      "2022-03-25T10:41:20.494005: step 8910, loss 3.98086e-05, acc 1\n",
      "2022-03-25T10:41:20.642383: step 8911, loss 0.000228147, acc 1\n",
      "2022-03-25T10:41:20.789161: step 8912, loss 0.00050542, acc 1\n",
      "2022-03-25T10:41:20.946528: step 8913, loss 0.000430697, acc 1\n",
      "2022-03-25T10:41:21.096787: step 8914, loss 1.16975e-05, acc 1\n",
      "2022-03-25T10:41:21.243699: step 8915, loss 2.96902e-06, acc 1\n",
      "2022-03-25T10:41:21.398985: step 8916, loss 1.89429e-06, acc 1\n",
      "2022-03-25T10:41:21.561053: step 8917, loss 0.0015286, acc 1\n",
      "2022-03-25T10:41:21.715814: step 8918, loss 0.00142288, acc 1\n",
      "2022-03-25T10:41:21.870286: step 8919, loss 3.19234e-05, acc 1\n",
      "2022-03-25T10:41:22.017717: step 8920, loss 0.00180522, acc 1\n",
      "2022-03-25T10:41:22.175991: step 8921, loss 0.00537786, acc 1\n",
      "2022-03-25T10:41:22.326108: step 8922, loss 2.93769e-05, acc 1\n",
      "2022-03-25T10:41:22.483188: step 8923, loss 0.000219678, acc 1\n",
      "2022-03-25T10:41:22.598932: step 8924, loss 0.000210634, acc 1\n",
      "2022-03-25T10:41:22.748303: step 8925, loss 0.01161, acc 0.984375\n",
      "2022-03-25T10:41:22.915625: step 8926, loss 3.75659e-05, acc 1\n",
      "2022-03-25T10:41:23.067763: step 8927, loss 7.50597e-06, acc 1\n",
      "2022-03-25T10:41:23.215901: step 8928, loss 0.00272859, acc 1\n",
      "2022-03-25T10:41:23.365413: step 8929, loss 0.000898952, acc 1\n",
      "2022-03-25T10:41:23.520943: step 8930, loss 0.000249545, acc 1\n",
      "2022-03-25T10:41:23.673761: step 8931, loss 0.0200834, acc 0.984375\n",
      "2022-03-25T10:41:23.819399: step 8932, loss 0.000220325, acc 1\n",
      "2022-03-25T10:41:23.971951: step 8933, loss 0.00170059, acc 1\n",
      "2022-03-25T10:41:24.119317: step 8934, loss 0.00114962, acc 1\n",
      "2022-03-25T10:41:24.272624: step 8935, loss 0.00176202, acc 1\n",
      "2022-03-25T10:41:24.426634: step 8936, loss 0.00539499, acc 1\n",
      "2022-03-25T10:41:24.575923: step 8937, loss 0.0141138, acc 0.984375\n",
      "2022-03-25T10:41:24.727921: step 8938, loss 5.15679e-05, acc 1\n",
      "2022-03-25T10:41:24.885519: step 8939, loss 3.20252e-05, acc 1\n",
      "2022-03-25T10:41:25.038840: step 8940, loss 0.000615631, acc 1\n",
      "2022-03-25T10:41:25.190199: step 8941, loss 6.42059e-05, acc 1\n",
      "2022-03-25T10:41:25.336285: step 8942, loss 1.51897e-05, acc 1\n",
      "2022-03-25T10:41:25.491607: step 8943, loss 3.37092e-05, acc 1\n",
      "2022-03-25T10:41:25.641838: step 8944, loss 0.00475813, acc 1\n",
      "2022-03-25T10:41:25.787275: step 8945, loss 5.94025e-05, acc 1\n",
      "2022-03-25T10:41:25.938806: step 8946, loss 0.0015748, acc 1\n",
      "2022-03-25T10:41:26.092297: step 8947, loss 2.00219e-05, acc 1\n",
      "2022-03-25T10:41:26.251171: step 8948, loss 0.0196086, acc 0.984375\n",
      "2022-03-25T10:41:26.400647: step 8949, loss 0.000104634, acc 1\n",
      "2022-03-25T10:41:26.546430: step 8950, loss 0.000194283, acc 1\n",
      "2022-03-25T10:41:26.701364: step 8951, loss 7.85976e-05, acc 1\n",
      "2022-03-25T10:41:26.853772: step 8952, loss 9.26659e-05, acc 1\n",
      "2022-03-25T10:41:27.010806: step 8953, loss 0.00300966, acc 1\n",
      "2022-03-25T10:41:27.153612: step 8954, loss 0.00027256, acc 1\n",
      "2022-03-25T10:41:27.303186: step 8955, loss 0.00684834, acc 1\n",
      "2022-03-25T10:41:27.446143: step 8956, loss 2.53575e-05, acc 1\n",
      "2022-03-25T10:41:27.590543: step 8957, loss 1.37837e-05, acc 1\n",
      "2022-03-25T10:41:27.744207: step 8958, loss 6.82333e-05, acc 1\n",
      "2022-03-25T10:41:27.895439: step 8959, loss 0.00167533, acc 1\n",
      "2022-03-25T10:41:28.052639: step 8960, loss 0.000974311, acc 1\n",
      "2022-03-25T10:41:28.189810: step 8961, loss 7.14107e-06, acc 1\n",
      "2022-03-25T10:41:28.341406: step 8962, loss 0.00633971, acc 1\n",
      "2022-03-25T10:41:28.497811: step 8963, loss 2.21033e-05, acc 1\n",
      "2022-03-25T10:41:28.657781: step 8964, loss 0.00316945, acc 1\n",
      "2022-03-25T10:41:28.809869: step 8965, loss 0.000293236, acc 1\n",
      "2022-03-25T10:41:28.970302: step 8966, loss 1.19367e-05, acc 1\n",
      "2022-03-25T10:41:29.126530: step 8967, loss 1.48312e-05, acc 1\n",
      "2022-03-25T10:41:29.277783: step 8968, loss 1.509e-05, acc 1\n",
      "2022-03-25T10:41:29.429588: step 8969, loss 4.69399e-05, acc 1\n",
      "2022-03-25T10:41:29.582921: step 8970, loss 0.00908632, acc 1\n",
      "2022-03-25T10:41:29.734178: step 8971, loss 1.17073e-05, acc 1\n",
      "2022-03-25T10:41:29.879515: step 8972, loss 3.01006e-05, acc 1\n",
      "2022-03-25T10:41:30.025174: step 8973, loss 2.88581e-05, acc 1\n",
      "2022-03-25T10:41:30.172427: step 8974, loss 0.000395306, acc 1\n",
      "2022-03-25T10:41:30.319286: step 8975, loss 0.000102346, acc 1\n",
      "2022-03-25T10:41:30.475499: step 8976, loss 0.00894373, acc 1\n",
      "2022-03-25T10:41:30.628710: step 8977, loss 0.0080768, acc 1\n",
      "2022-03-25T10:41:30.781232: step 8978, loss 0.00361188, acc 1\n",
      "2022-03-25T10:41:30.925878: step 8979, loss 9.22657e-06, acc 1\n",
      "2022-03-25T10:41:31.080690: step 8980, loss 0.000106272, acc 1\n",
      "2022-03-25T10:41:31.228787: step 8981, loss 0.000101696, acc 1\n",
      "2022-03-25T10:41:31.379885: step 8982, loss 0.000321961, acc 1\n",
      "2022-03-25T10:41:31.533450: step 8983, loss 0.000287829, acc 1\n",
      "2022-03-25T10:41:31.681241: step 8984, loss 4.23581e-05, acc 1\n",
      "2022-03-25T10:41:31.824783: step 8985, loss 9.23022e-06, acc 1\n",
      "2022-03-25T10:41:31.972417: step 8986, loss 0.0156372, acc 0.984375\n",
      "2022-03-25T10:41:32.127752: step 8987, loss 3.6986e-05, acc 1\n",
      "2022-03-25T10:41:32.267762: step 8988, loss 2.01404e-05, acc 1\n",
      "2022-03-25T10:41:32.421280: step 8989, loss 0.0345349, acc 0.984375\n",
      "2022-03-25T10:41:32.576247: step 8990, loss 8.54427e-05, acc 1\n",
      "2022-03-25T10:41:32.717362: step 8991, loss 0.000124001, acc 1\n",
      "2022-03-25T10:41:32.871140: step 8992, loss 3.83138e-05, acc 1\n",
      "2022-03-25T10:41:33.030361: step 8993, loss 0.000354462, acc 1\n",
      "2022-03-25T10:41:33.189040: step 8994, loss 0.000292507, acc 1\n",
      "2022-03-25T10:41:33.339830: step 8995, loss 0.000223187, acc 1\n",
      "2022-03-25T10:41:33.491215: step 8996, loss 0.000498564, acc 1\n",
      "2022-03-25T10:41:33.644203: step 8997, loss 0.000573977, acc 1\n",
      "2022-03-25T10:41:33.793580: step 8998, loss 0.00147418, acc 1\n",
      "2022-03-25T10:41:33.936447: step 8999, loss 0.00172836, acc 1\n",
      "2022-03-25T10:41:34.080438: step 9000, loss 0.00050562, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:41:34.231389: step 9000, loss 0.799466, acc 0.912791\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9000\n",
      "\n",
      "2022-03-25T10:41:34.482935: step 9001, loss 9.33023e-05, acc 1\n",
      "2022-03-25T10:41:34.626675: step 9002, loss 0.000857445, acc 1\n",
      "2022-03-25T10:41:34.769569: step 9003, loss 2.34126e-05, acc 1\n",
      "2022-03-25T10:41:34.915807: step 9004, loss 0.00143223, acc 1\n",
      "2022-03-25T10:41:35.063230: step 9005, loss 0.000106414, acc 1\n",
      "2022-03-25T10:41:35.216507: step 9006, loss 0.000117801, acc 1\n",
      "2022-03-25T10:41:35.372044: step 9007, loss 4.34198e-05, acc 1\n",
      "2022-03-25T10:41:35.520338: step 9008, loss 0.0002108, acc 1\n",
      "2022-03-25T10:41:35.673169: step 9009, loss 9.5349e-05, acc 1\n",
      "2022-03-25T10:41:35.822898: step 9010, loss 0.000144478, acc 1\n",
      "2022-03-25T10:41:36.001750: step 9011, loss 0.000210275, acc 1\n",
      "2022-03-25T10:41:36.160189: step 9012, loss 0.00520969, acc 1\n",
      "2022-03-25T10:41:36.314134: step 9013, loss 1.55641e-05, acc 1\n",
      "2022-03-25T10:41:36.461953: step 9014, loss 0.000170316, acc 1\n",
      "2022-03-25T10:41:36.603794: step 9015, loss 0.00018395, acc 1\n",
      "2022-03-25T10:41:36.760694: step 9016, loss 5.17194e-05, acc 1\n",
      "2022-03-25T10:41:36.915219: step 9017, loss 0.000443364, acc 1\n",
      "2022-03-25T10:41:37.069938: step 9018, loss 0.00013766, acc 1\n",
      "2022-03-25T10:41:37.243986: step 9019, loss 1.9158e-05, acc 1\n",
      "2022-03-25T10:41:37.396065: step 9020, loss 6.40915e-06, acc 1\n",
      "2022-03-25T10:41:37.544167: step 9021, loss 3.3903e-05, acc 1\n",
      "2022-03-25T10:41:37.693850: step 9022, loss 4.71006e-05, acc 1\n",
      "2022-03-25T10:41:37.844096: step 9023, loss 0.00273947, acc 1\n",
      "2022-03-25T10:41:37.991016: step 9024, loss 0.000124679, acc 1\n",
      "2022-03-25T10:41:38.144028: step 9025, loss 2.90753e-06, acc 1\n",
      "2022-03-25T10:41:38.309159: step 9026, loss 4.70144e-05, acc 1\n",
      "2022-03-25T10:41:38.455639: step 9027, loss 2.23708e-05, acc 1\n",
      "2022-03-25T10:41:38.601487: step 9028, loss 7.11667e-05, acc 1\n",
      "2022-03-25T10:41:38.750655: step 9029, loss 0.000174181, acc 1\n",
      "2022-03-25T10:41:38.906015: step 9030, loss 3.0379e-05, acc 1\n",
      "2022-03-25T10:41:39.060666: step 9031, loss 0.0328316, acc 0.984375\n",
      "2022-03-25T10:41:39.211403: step 9032, loss 0.00118028, acc 1\n",
      "2022-03-25T10:41:39.366230: step 9033, loss 2.9571e-05, acc 1\n",
      "2022-03-25T10:41:39.512665: step 9034, loss 9.09295e-06, acc 1\n",
      "2022-03-25T10:41:39.660428: step 9035, loss 0.000993205, acc 1\n",
      "2022-03-25T10:41:39.811132: step 9036, loss 0.000150659, acc 1\n",
      "2022-03-25T10:41:39.960682: step 9037, loss 1.33e-05, acc 1\n",
      "2022-03-25T10:41:40.114813: step 9038, loss 3.55556e-06, acc 1\n",
      "2022-03-25T10:41:40.261902: step 9039, loss 1.42931e-05, acc 1\n",
      "2022-03-25T10:41:40.414412: step 9040, loss 3.91502e-06, acc 1\n",
      "2022-03-25T10:41:40.564895: step 9041, loss 0.000258521, acc 1\n",
      "2022-03-25T10:41:40.713161: step 9042, loss 8.14056e-06, acc 1\n",
      "2022-03-25T10:41:40.861684: step 9043, loss 0.000422992, acc 1\n",
      "2022-03-25T10:41:41.007556: step 9044, loss 3.17379e-06, acc 1\n",
      "2022-03-25T10:41:41.159893: step 9045, loss 4.41445e-07, acc 1\n",
      "2022-03-25T10:41:41.313324: step 9046, loss 0.000885474, acc 1\n",
      "2022-03-25T10:41:41.467419: step 9047, loss 0.00148988, acc 1\n",
      "2022-03-25T10:41:41.614717: step 9048, loss 3.88957e-05, acc 1\n",
      "2022-03-25T10:41:41.769470: step 9049, loss 0.00286372, acc 1\n",
      "2022-03-25T10:41:41.919723: step 9050, loss 0.0176032, acc 0.984375\n",
      "2022-03-25T10:41:42.067922: step 9051, loss 4.90936e-06, acc 1\n",
      "2022-03-25T10:41:42.218412: step 9052, loss 1.56932e-05, acc 1\n",
      "2022-03-25T10:41:42.385739: step 9053, loss 1.26376e-05, acc 1\n",
      "2022-03-25T10:41:42.535312: step 9054, loss 6.53672e-06, acc 1\n",
      "2022-03-25T10:41:42.681911: step 9055, loss 0.000587448, acc 1\n",
      "2022-03-25T10:41:42.828909: step 9056, loss 0.000107142, acc 1\n",
      "2022-03-25T10:41:42.978526: step 9057, loss 2.76777e-06, acc 1\n",
      "2022-03-25T10:41:43.127020: step 9058, loss 3.53527e-05, acc 1\n",
      "2022-03-25T10:41:43.268427: step 9059, loss 0.000108219, acc 1\n",
      "2022-03-25T10:41:43.430610: step 9060, loss 2.60371e-05, acc 1\n",
      "2022-03-25T10:41:43.590874: step 9061, loss 8.00343e-06, acc 1\n",
      "2022-03-25T10:41:43.748128: step 9062, loss 1.98775e-05, acc 1\n",
      "2022-03-25T10:41:43.894968: step 9063, loss 0.00234261, acc 1\n",
      "2022-03-25T10:41:44.046145: step 9064, loss 1.51164e-05, acc 1\n",
      "2022-03-25T10:41:44.204346: step 9065, loss 3.54142e-05, acc 1\n",
      "2022-03-25T10:41:44.354184: step 9066, loss 0.000181358, acc 1\n",
      "2022-03-25T10:41:44.509993: step 9067, loss 0.000965547, acc 1\n",
      "2022-03-25T10:41:44.655232: step 9068, loss 0.000839886, acc 1\n",
      "2022-03-25T10:41:44.813131: step 9069, loss 0.000168621, acc 1\n",
      "2022-03-25T10:41:44.957030: step 9070, loss 0.019327, acc 0.984375\n",
      "2022-03-25T10:41:45.110572: step 9071, loss 0.0507496, acc 0.984375\n",
      "2022-03-25T10:41:45.264876: step 9072, loss 0.000567765, acc 1\n",
      "2022-03-25T10:41:45.418910: step 9073, loss 0.000185874, acc 1\n",
      "2022-03-25T10:41:45.571820: step 9074, loss 0.00482254, acc 1\n",
      "2022-03-25T10:41:45.723995: step 9075, loss 0.000435565, acc 1\n",
      "2022-03-25T10:41:45.872461: step 9076, loss 0.00540517, acc 1\n",
      "2022-03-25T10:41:46.030445: step 9077, loss 0.000539732, acc 1\n",
      "2022-03-25T10:41:46.185207: step 9078, loss 0.000657837, acc 1\n",
      "2022-03-25T10:41:46.338557: step 9079, loss 6.89159e-05, acc 1\n",
      "2022-03-25T10:41:46.502079: step 9080, loss 0.0120883, acc 0.984375\n",
      "2022-03-25T10:41:46.656894: step 9081, loss 0.000852935, acc 1\n",
      "2022-03-25T10:41:46.816121: step 9082, loss 0.00351954, acc 1\n",
      "2022-03-25T10:41:46.976789: step 9083, loss 0.000820148, acc 1\n",
      "2022-03-25T10:41:47.123270: step 9084, loss 0.0447784, acc 0.984375\n",
      "2022-03-25T10:41:47.280839: step 9085, loss 0.000542008, acc 1\n",
      "2022-03-25T10:41:47.440075: step 9086, loss 0.000839741, acc 1\n",
      "2022-03-25T10:41:47.607816: step 9087, loss 0.00270931, acc 1\n",
      "2022-03-25T10:41:47.758224: step 9088, loss 1.7987e-05, acc 1\n",
      "2022-03-25T10:41:47.916375: step 9089, loss 0.0100309, acc 1\n",
      "2022-03-25T10:41:48.062377: step 9090, loss 2.11718e-05, acc 1\n",
      "2022-03-25T10:41:48.216468: step 9091, loss 5.1891e-06, acc 1\n",
      "2022-03-25T10:41:48.359148: step 9092, loss 0.00161364, acc 1\n",
      "2022-03-25T10:41:48.520264: step 9093, loss 0.00247758, acc 1\n",
      "2022-03-25T10:41:48.669692: step 9094, loss 3.76253e-07, acc 1\n",
      "2022-03-25T10:41:48.814505: step 9095, loss 2.33004e-06, acc 1\n",
      "2022-03-25T10:41:48.963439: step 9096, loss 0.0011755, acc 1\n",
      "2022-03-25T10:41:49.113713: step 9097, loss 3.5761e-05, acc 1\n",
      "2022-03-25T10:41:49.269701: step 9098, loss 1.4247e-05, acc 1\n",
      "2022-03-25T10:41:49.410162: step 9099, loss 2.10662e-06, acc 1\n",
      "2022-03-25T10:41:49.570272: step 9100, loss 0.0253953, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:41:49.718692: step 9100, loss 1.0704, acc 0.922965\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9100\n",
      "\n",
      "2022-03-25T10:41:49.967230: step 9101, loss 1.29728e-05, acc 1\n",
      "2022-03-25T10:41:50.113687: step 9102, loss 0.00199518, acc 1\n",
      "2022-03-25T10:41:50.254655: step 9103, loss 7.14167e-05, acc 1\n",
      "2022-03-25T10:41:50.395580: step 9104, loss 0.000110843, acc 1\n",
      "2022-03-25T10:41:50.543958: step 9105, loss 0.000802007, acc 1\n",
      "2022-03-25T10:41:50.694458: step 9106, loss 0.0118429, acc 0.984375\n",
      "2022-03-25T10:41:50.834636: step 9107, loss 2.33009e-06, acc 1\n",
      "2022-03-25T10:41:50.988538: step 9108, loss 0.00826191, acc 1\n",
      "2022-03-25T10:41:51.132191: step 9109, loss 7.38246e-05, acc 1\n",
      "2022-03-25T10:41:51.275842: step 9110, loss 0.000102575, acc 1\n",
      "2022-03-25T10:41:51.424148: step 9111, loss 3.40292e-05, acc 1\n",
      "2022-03-25T10:41:51.580478: step 9112, loss 0.000102618, acc 1\n",
      "2022-03-25T10:41:51.729909: step 9113, loss 0.000560112, acc 1\n",
      "2022-03-25T10:41:51.877197: step 9114, loss 0.000393559, acc 1\n",
      "2022-03-25T10:41:52.019271: step 9115, loss 4.67683e-06, acc 1\n",
      "2022-03-25T10:41:52.160439: step 9116, loss 3.05694e-05, acc 1\n",
      "2022-03-25T10:41:52.303900: step 9117, loss 9.77924e-05, acc 1\n",
      "2022-03-25T10:41:52.430784: step 9118, loss 0.000394154, acc 1\n",
      "2022-03-25T10:41:52.595173: step 9119, loss 0.00169465, acc 1\n",
      "2022-03-25T10:41:52.750027: step 9120, loss 0.00333441, acc 1\n",
      "2022-03-25T10:41:52.908135: step 9121, loss 0.00275739, acc 1\n",
      "2022-03-25T10:41:53.054805: step 9122, loss 0.00025017, acc 1\n",
      "2022-03-25T10:41:53.208214: step 9123, loss 6.05958e-05, acc 1\n",
      "2022-03-25T10:41:53.357551: step 9124, loss 0.000166472, acc 1\n",
      "2022-03-25T10:41:53.507986: step 9125, loss 0.000466168, acc 1\n",
      "2022-03-25T10:41:53.672538: step 9126, loss 2.99944e-05, acc 1\n",
      "2022-03-25T10:41:53.827444: step 9127, loss 8.78373e-05, acc 1\n",
      "2022-03-25T10:41:53.965766: step 9128, loss 4.19451e-06, acc 1\n",
      "2022-03-25T10:41:54.123842: step 9129, loss 0.000492389, acc 1\n",
      "2022-03-25T10:41:54.270891: step 9130, loss 9.02167e-06, acc 1\n",
      "2022-03-25T10:41:54.428515: step 9131, loss 8.71966e-05, acc 1\n",
      "2022-03-25T10:41:54.585677: step 9132, loss 0.000145662, acc 1\n",
      "2022-03-25T10:41:54.743237: step 9133, loss 0.000189189, acc 1\n",
      "2022-03-25T10:41:54.892664: step 9134, loss 0.000868599, acc 1\n",
      "2022-03-25T10:41:55.043031: step 9135, loss 6.67841e-05, acc 1\n",
      "2022-03-25T10:41:55.188836: step 9136, loss 1.25344e-05, acc 1\n",
      "2022-03-25T10:41:55.339650: step 9137, loss 0.000115362, acc 1\n",
      "2022-03-25T10:41:55.481611: step 9138, loss 4.71256e-05, acc 1\n",
      "2022-03-25T10:41:55.632600: step 9139, loss 3.41562e-05, acc 1\n",
      "2022-03-25T10:41:55.785357: step 9140, loss 0.000378667, acc 1\n",
      "2022-03-25T10:41:55.937192: step 9141, loss 0.00200093, acc 1\n",
      "2022-03-25T10:41:56.086495: step 9142, loss 3.08076e-06, acc 1\n",
      "2022-03-25T10:41:56.235239: step 9143, loss 6.65135e-05, acc 1\n",
      "2022-03-25T10:41:56.377518: step 9144, loss 0.000153745, acc 1\n",
      "2022-03-25T10:41:56.526047: step 9145, loss 5.57659e-05, acc 1\n",
      "2022-03-25T10:41:56.695971: step 9146, loss 0.00020997, acc 1\n",
      "2022-03-25T10:41:56.858851: step 9147, loss 0.000194062, acc 1\n",
      "2022-03-25T10:41:57.014488: step 9148, loss 0.00101308, acc 1\n",
      "2022-03-25T10:41:57.172760: step 9149, loss 6.74233e-06, acc 1\n",
      "2022-03-25T10:41:57.318516: step 9150, loss 5.53896e-06, acc 1\n",
      "2022-03-25T10:41:57.474658: step 9151, loss 0.000403985, acc 1\n",
      "2022-03-25T10:41:57.631303: step 9152, loss 8.15719e-06, acc 1\n",
      "2022-03-25T10:41:57.788818: step 9153, loss 0.0360905, acc 0.984375\n",
      "2022-03-25T10:41:57.942361: step 9154, loss 0.000401627, acc 1\n",
      "2022-03-25T10:41:58.095176: step 9155, loss 4.27695e-05, acc 1\n",
      "2022-03-25T10:41:58.242799: step 9156, loss 6.64297e-05, acc 1\n",
      "2022-03-25T10:41:58.392157: step 9157, loss 0.000167022, acc 1\n",
      "2022-03-25T10:41:58.539779: step 9158, loss 6.48281e-05, acc 1\n",
      "2022-03-25T10:41:58.694006: step 9159, loss 0.00102471, acc 1\n",
      "2022-03-25T10:41:58.866614: step 9160, loss 0.000107469, acc 1\n",
      "2022-03-25T10:41:59.023308: step 9161, loss 0.0519868, acc 0.984375\n",
      "2022-03-25T10:41:59.171893: step 9162, loss 0.000139724, acc 1\n",
      "2022-03-25T10:41:59.316966: step 9163, loss 7.61957e-05, acc 1\n",
      "2022-03-25T10:41:59.470266: step 9164, loss 0.0353336, acc 0.984375\n",
      "2022-03-25T10:41:59.618219: step 9165, loss 2.2556e-06, acc 1\n",
      "2022-03-25T10:41:59.774448: step 9166, loss 0.0108743, acc 1\n",
      "2022-03-25T10:41:59.915727: step 9167, loss 0.000627002, acc 1\n",
      "2022-03-25T10:42:00.064145: step 9168, loss 8.50359e-06, acc 1\n",
      "2022-03-25T10:42:00.212199: step 9169, loss 8.62499e-06, acc 1\n",
      "2022-03-25T10:42:00.362781: step 9170, loss 1.98178e-06, acc 1\n",
      "2022-03-25T10:42:00.505991: step 9171, loss 0.00011505, acc 1\n",
      "2022-03-25T10:42:00.647465: step 9172, loss 0.00145157, acc 1\n",
      "2022-03-25T10:42:00.814025: step 9173, loss 0.00133778, acc 1\n",
      "2022-03-25T10:42:00.967080: step 9174, loss 6.40464e-05, acc 1\n",
      "2022-03-25T10:42:01.114531: step 9175, loss 0.000211959, acc 1\n",
      "2022-03-25T10:42:01.259270: step 9176, loss 0.000755082, acc 1\n",
      "2022-03-25T10:42:01.409901: step 9177, loss 1.57793e-05, acc 1\n",
      "2022-03-25T10:42:01.561743: step 9178, loss 1.27774e-06, acc 1\n",
      "2022-03-25T10:42:01.715854: step 9179, loss 0.00152849, acc 1\n",
      "2022-03-25T10:42:01.878566: step 9180, loss 0.000648748, acc 1\n",
      "2022-03-25T10:42:02.042163: step 9181, loss 5.27669e-05, acc 1\n",
      "2022-03-25T10:42:02.193689: step 9182, loss 0.0530605, acc 0.984375\n",
      "2022-03-25T10:42:02.348092: step 9183, loss 3.66549e-06, acc 1\n",
      "2022-03-25T10:42:02.513012: step 9184, loss 7.56824e-06, acc 1\n",
      "2022-03-25T10:42:02.675037: step 9185, loss 6.52383e-06, acc 1\n",
      "2022-03-25T10:42:02.838259: step 9186, loss 4.06413e-06, acc 1\n",
      "2022-03-25T10:42:02.988571: step 9187, loss 0.104841, acc 0.984375\n",
      "2022-03-25T10:42:03.142752: step 9188, loss 3.12923e-07, acc 1\n",
      "2022-03-25T10:42:03.302546: step 9189, loss 0.00229143, acc 1\n",
      "2022-03-25T10:42:03.448809: step 9190, loss 6.77765e-06, acc 1\n",
      "2022-03-25T10:42:03.608798: step 9191, loss 5.90455e-07, acc 1\n",
      "2022-03-25T10:42:03.761766: step 9192, loss 4.24273e-06, acc 1\n",
      "2022-03-25T10:42:03.937118: step 9193, loss 0.00228117, acc 1\n",
      "2022-03-25T10:42:04.092367: step 9194, loss 0.000627086, acc 1\n",
      "2022-03-25T10:42:04.245755: step 9195, loss 0.000242803, acc 1\n",
      "2022-03-25T10:42:04.396899: step 9196, loss 0.000682238, acc 1\n",
      "2022-03-25T10:42:04.556581: step 9197, loss 3.47845e-05, acc 1\n",
      "2022-03-25T10:42:04.705168: step 9198, loss 0.00582472, acc 1\n",
      "2022-03-25T10:42:04.862966: step 9199, loss 0.0050639, acc 1\n",
      "2022-03-25T10:42:05.019065: step 9200, loss 0.000150959, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:42:05.174703: step 9200, loss 0.829836, acc 0.912791\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9200\n",
      "\n",
      "2022-03-25T10:42:05.433015: step 9201, loss 0.000385269, acc 1\n",
      "2022-03-25T10:42:05.577418: step 9202, loss 0.000259495, acc 1\n",
      "2022-03-25T10:42:05.725295: step 9203, loss 0.000384739, acc 1\n",
      "2022-03-25T10:42:05.867650: step 9204, loss 0.000326853, acc 1\n",
      "2022-03-25T10:42:06.028430: step 9205, loss 0.0107645, acc 1\n",
      "2022-03-25T10:42:06.177246: step 9206, loss 1.60425e-05, acc 1\n",
      "2022-03-25T10:42:06.329704: step 9207, loss 0.0647883, acc 0.984375\n",
      "2022-03-25T10:42:06.474020: step 9208, loss 1.45922e-05, acc 1\n",
      "2022-03-25T10:42:06.624465: step 9209, loss 0.00048884, acc 1\n",
      "2022-03-25T10:42:06.780380: step 9210, loss 0.0136592, acc 0.984375\n",
      "2022-03-25T10:42:06.945667: step 9211, loss 1.7321e-05, acc 1\n",
      "2022-03-25T10:42:07.106055: step 9212, loss 0.0180197, acc 0.984375\n",
      "2022-03-25T10:42:07.261175: step 9213, loss 0.000991851, acc 1\n",
      "2022-03-25T10:42:07.405335: step 9214, loss 4.57816e-06, acc 1\n",
      "2022-03-25T10:42:07.557966: step 9215, loss 0.000188735, acc 1\n",
      "2022-03-25T10:42:07.706813: step 9216, loss 0.000106268, acc 1\n",
      "2022-03-25T10:42:07.850462: step 9217, loss 0.000360922, acc 1\n",
      "2022-03-25T10:42:08.003455: step 9218, loss 1.58734e-05, acc 1\n",
      "2022-03-25T10:42:08.150262: step 9219, loss 0.00104101, acc 1\n",
      "2022-03-25T10:42:08.308027: step 9220, loss 0.00372328, acc 1\n",
      "2022-03-25T10:42:08.462980: step 9221, loss 0.0437947, acc 0.984375\n",
      "2022-03-25T10:42:08.606216: step 9222, loss 0.00106343, acc 1\n",
      "2022-03-25T10:42:08.763232: step 9223, loss 9.30611e-05, acc 1\n",
      "2022-03-25T10:42:08.917285: step 9224, loss 4.26695e-05, acc 1\n",
      "2022-03-25T10:42:09.073493: step 9225, loss 0.000543721, acc 1\n",
      "2022-03-25T10:42:09.221195: step 9226, loss 0.00263669, acc 1\n",
      "2022-03-25T10:42:09.373088: step 9227, loss 6.73508e-05, acc 1\n",
      "2022-03-25T10:42:09.516303: step 9228, loss 5.49994e-06, acc 1\n",
      "2022-03-25T10:42:09.667694: step 9229, loss 2.62804e-06, acc 1\n",
      "2022-03-25T10:42:09.812470: step 9230, loss 0.000596271, acc 1\n",
      "2022-03-25T10:42:09.965973: step 9231, loss 0.00135942, acc 1\n",
      "2022-03-25T10:42:10.116283: step 9232, loss 0.0390073, acc 0.984375\n",
      "2022-03-25T10:42:10.255100: step 9233, loss 0.000849405, acc 1\n",
      "2022-03-25T10:42:10.401443: step 9234, loss 8.85238e-06, acc 1\n",
      "2022-03-25T10:42:10.550269: step 9235, loss 3.16793e-05, acc 1\n",
      "2022-03-25T10:42:10.701069: step 9236, loss 0.000540106, acc 1\n",
      "2022-03-25T10:42:10.852187: step 9237, loss 4.44167e-05, acc 1\n",
      "2022-03-25T10:42:11.003000: step 9238, loss 0.000220299, acc 1\n",
      "2022-03-25T10:42:11.159261: step 9239, loss 2.00392e-05, acc 1\n",
      "2022-03-25T10:42:11.300383: step 9240, loss 0.000662941, acc 1\n",
      "2022-03-25T10:42:11.443180: step 9241, loss 0.000102681, acc 1\n",
      "2022-03-25T10:42:11.597517: step 9242, loss 0.00602434, acc 1\n",
      "2022-03-25T10:42:11.748981: step 9243, loss 0.000743706, acc 1\n",
      "2022-03-25T10:42:11.899438: step 9244, loss 0.00222921, acc 1\n",
      "2022-03-25T10:42:12.058890: step 9245, loss 0.000339463, acc 1\n",
      "2022-03-25T10:42:12.206106: step 9246, loss 0.000165304, acc 1\n",
      "2022-03-25T10:42:12.352203: step 9247, loss 0.00223187, acc 1\n",
      "2022-03-25T10:42:12.505149: step 9248, loss 0.00250889, acc 1\n",
      "2022-03-25T10:42:12.650255: step 9249, loss 6.62362e-05, acc 1\n",
      "2022-03-25T10:42:12.802641: step 9250, loss 0.000140131, acc 1\n",
      "2022-03-25T10:42:12.955988: step 9251, loss 0.000398222, acc 1\n",
      "2022-03-25T10:42:13.108233: step 9252, loss 7.23224e-05, acc 1\n",
      "2022-03-25T10:42:13.258496: step 9253, loss 0.0010658, acc 1\n",
      "2022-03-25T10:42:13.405525: step 9254, loss 0.000202514, acc 1\n",
      "2022-03-25T10:42:13.545477: step 9255, loss 0.000645046, acc 1\n",
      "2022-03-25T10:42:13.699278: step 9256, loss 0.000155497, acc 1\n",
      "2022-03-25T10:42:13.862924: step 9257, loss 0.000295645, acc 1\n",
      "2022-03-25T10:42:14.010279: step 9258, loss 0.00020516, acc 1\n",
      "2022-03-25T10:42:14.159235: step 9259, loss 0.000491537, acc 1\n",
      "2022-03-25T10:42:14.299015: step 9260, loss 9.04989e-05, acc 1\n",
      "2022-03-25T10:42:14.450148: step 9261, loss 9.03617e-05, acc 1\n",
      "2022-03-25T10:42:14.592183: step 9262, loss 0.000137891, acc 1\n",
      "2022-03-25T10:42:14.738705: step 9263, loss 0.000217489, acc 1\n",
      "2022-03-25T10:42:14.889910: step 9264, loss 0.00360465, acc 1\n",
      "2022-03-25T10:42:15.041089: step 9265, loss 0.00155076, acc 1\n",
      "2022-03-25T10:42:15.191013: step 9266, loss 0.000310218, acc 1\n",
      "2022-03-25T10:42:15.339508: step 9267, loss 0.0096972, acc 1\n",
      "2022-03-25T10:42:15.493757: step 9268, loss 6.78625e-05, acc 1\n",
      "2022-03-25T10:42:15.645886: step 9269, loss 4.28946e-06, acc 1\n",
      "2022-03-25T10:42:15.799303: step 9270, loss 2.6784e-06, acc 1\n",
      "2022-03-25T10:42:15.951211: step 9271, loss 3.54717e-05, acc 1\n",
      "2022-03-25T10:42:16.097720: step 9272, loss 0.000197868, acc 1\n",
      "2022-03-25T10:42:16.249759: step 9273, loss 0.0148992, acc 0.984375\n",
      "2022-03-25T10:42:16.395919: step 9274, loss 0.00814385, acc 1\n",
      "2022-03-25T10:42:16.546339: step 9275, loss 0.0138153, acc 1\n",
      "2022-03-25T10:42:16.689273: step 9276, loss 1.0928e-05, acc 1\n",
      "2022-03-25T10:42:16.833273: step 9277, loss 0.000278402, acc 1\n",
      "2022-03-25T10:42:16.973661: step 9278, loss 0.000100706, acc 1\n",
      "2022-03-25T10:42:17.113685: step 9279, loss 4.0473e-06, acc 1\n",
      "2022-03-25T10:42:17.257966: step 9280, loss 1.83095e-06, acc 1\n",
      "2022-03-25T10:42:17.411559: step 9281, loss 0.0004912, acc 1\n",
      "2022-03-25T10:42:17.554079: step 9282, loss 0.000306788, acc 1\n",
      "2022-03-25T10:42:17.703543: step 9283, loss 3.16891e-05, acc 1\n",
      "2022-03-25T10:42:17.856229: step 9284, loss 7.37288e-06, acc 1\n",
      "2022-03-25T10:42:17.994700: step 9285, loss 4.89162e-05, acc 1\n",
      "2022-03-25T10:42:18.148866: step 9286, loss 0.00425834, acc 1\n",
      "2022-03-25T10:42:18.305245: step 9287, loss 0.00271711, acc 1\n",
      "2022-03-25T10:42:18.449734: step 9288, loss 2.42143e-07, acc 1\n",
      "2022-03-25T10:42:18.593559: step 9289, loss 0.00265337, acc 1\n",
      "2022-03-25T10:42:18.735102: step 9290, loss 1.07363e-05, acc 1\n",
      "2022-03-25T10:42:18.883491: step 9291, loss 0.000282966, acc 1\n",
      "2022-03-25T10:42:19.028583: step 9292, loss 0.00755285, acc 1\n",
      "2022-03-25T10:42:19.172798: step 9293, loss 9.95077e-05, acc 1\n",
      "2022-03-25T10:42:19.317775: step 9294, loss 7.81161e-05, acc 1\n",
      "2022-03-25T10:42:19.461061: step 9295, loss 0.00183662, acc 1\n",
      "2022-03-25T10:42:19.606748: step 9296, loss 0.000111875, acc 1\n",
      "2022-03-25T10:42:19.754130: step 9297, loss 3.22032e-06, acc 1\n",
      "2022-03-25T10:42:19.904053: step 9298, loss 0.0781833, acc 0.984375\n",
      "2022-03-25T10:42:20.050935: step 9299, loss 1.4496e-05, acc 1\n",
      "2022-03-25T10:42:20.194931: step 9300, loss 0.000517331, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:42:20.332437: step 9300, loss 0.885112, acc 0.915698\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9300\n",
      "\n",
      "2022-03-25T10:42:20.574460: step 9301, loss 0.000126687, acc 1\n",
      "2022-03-25T10:42:20.727299: step 9302, loss 0.000279151, acc 1\n",
      "2022-03-25T10:42:20.871629: step 9303, loss 0.000196135, acc 1\n",
      "2022-03-25T10:42:21.017029: step 9304, loss 8.85449e-05, acc 1\n",
      "2022-03-25T10:42:21.156281: step 9305, loss 0.00081933, acc 1\n",
      "2022-03-25T10:42:21.318542: step 9306, loss 5.5802e-05, acc 1\n",
      "2022-03-25T10:42:21.463827: step 9307, loss 0.00142193, acc 1\n",
      "2022-03-25T10:42:21.610142: step 9308, loss 5.34329e-05, acc 1\n",
      "2022-03-25T10:42:21.752426: step 9309, loss 0.0309888, acc 0.984375\n",
      "2022-03-25T10:42:21.901797: step 9310, loss 1.37075e-05, acc 1\n",
      "2022-03-25T10:42:22.058705: step 9311, loss 1.47761e-05, acc 1\n",
      "2022-03-25T10:42:22.179351: step 9312, loss 0.000338051, acc 1\n",
      "2022-03-25T10:42:22.331406: step 9313, loss 6.47007e-05, acc 1\n",
      "2022-03-25T10:42:22.483988: step 9314, loss 0.000623897, acc 1\n",
      "2022-03-25T10:42:22.634745: step 9315, loss 2.3283e-07, acc 1\n",
      "2022-03-25T10:42:22.788216: step 9316, loss 0.0083599, acc 1\n",
      "2022-03-25T10:42:22.942149: step 9317, loss 4.24112e-06, acc 1\n",
      "2022-03-25T10:42:23.086314: step 9318, loss 6.46123e-06, acc 1\n",
      "2022-03-25T10:42:23.239221: step 9319, loss 0.00108111, acc 1\n",
      "2022-03-25T10:42:23.397136: step 9320, loss 6.63763e-06, acc 1\n",
      "2022-03-25T10:42:23.541876: step 9321, loss 0.000404951, acc 1\n",
      "2022-03-25T10:42:23.693761: step 9322, loss 0.000121996, acc 1\n",
      "2022-03-25T10:42:23.838867: step 9323, loss 0.000244801, acc 1\n",
      "2022-03-25T10:42:23.991050: step 9324, loss 5.63421e-06, acc 1\n",
      "2022-03-25T10:42:24.139947: step 9325, loss 5.80686e-05, acc 1\n",
      "2022-03-25T10:42:24.291860: step 9326, loss 9.64844e-07, acc 1\n",
      "2022-03-25T10:42:24.445588: step 9327, loss 7.79583e-06, acc 1\n",
      "2022-03-25T10:42:24.599166: step 9328, loss 0.000265749, acc 1\n",
      "2022-03-25T10:42:24.759178: step 9329, loss 0.000185824, acc 1\n",
      "2022-03-25T10:42:24.907751: step 9330, loss 2.40154e-05, acc 1\n",
      "2022-03-25T10:42:25.055020: step 9331, loss 7.41256e-06, acc 1\n",
      "2022-03-25T10:42:25.213614: step 9332, loss 6.99323e-06, acc 1\n",
      "2022-03-25T10:42:25.378926: step 9333, loss 6.25021e-06, acc 1\n",
      "2022-03-25T10:42:25.537883: step 9334, loss 2.33014e-06, acc 1\n",
      "2022-03-25T10:42:25.710553: step 9335, loss 0.000123494, acc 1\n",
      "2022-03-25T10:42:25.867684: step 9336, loss 0.000144306, acc 1\n",
      "2022-03-25T10:42:26.017459: step 9337, loss 1.2204e-05, acc 1\n",
      "2022-03-25T10:42:26.165257: step 9338, loss 0.000127983, acc 1\n",
      "2022-03-25T10:42:26.316966: step 9339, loss 1.79424e-05, acc 1\n",
      "2022-03-25T10:42:26.536325: step 9340, loss 0.0012546, acc 1\n",
      "2022-03-25T10:42:26.738800: step 9341, loss 5.34578e-07, acc 1\n",
      "2022-03-25T10:42:26.913288: step 9342, loss 9.4439e-06, acc 1\n",
      "2022-03-25T10:42:27.060909: step 9343, loss 2.80263e-05, acc 1\n",
      "2022-03-25T10:42:27.219943: step 9344, loss 0.000156574, acc 1\n",
      "2022-03-25T10:42:27.379936: step 9345, loss 2.91722e-05, acc 1\n",
      "2022-03-25T10:42:27.540196: step 9346, loss 5.129e-06, acc 1\n",
      "2022-03-25T10:42:27.695227: step 9347, loss 4.12552e-06, acc 1\n",
      "2022-03-25T10:42:27.844553: step 9348, loss 3.77246e-05, acc 1\n",
      "2022-03-25T10:42:27.995416: step 9349, loss 0.00254921, acc 1\n",
      "2022-03-25T10:42:28.151905: step 9350, loss 0.000456726, acc 1\n",
      "2022-03-25T10:42:28.304721: step 9351, loss 1.65954e-06, acc 1\n",
      "2022-03-25T10:42:28.462364: step 9352, loss 0.000146571, acc 1\n",
      "2022-03-25T10:42:28.614200: step 9353, loss 0.0137909, acc 0.984375\n",
      "2022-03-25T10:42:28.761033: step 9354, loss 1.21907e-05, acc 1\n",
      "2022-03-25T10:42:28.907457: step 9355, loss 3.51261e-06, acc 1\n",
      "2022-03-25T10:42:29.070997: step 9356, loss 0.000689192, acc 1\n",
      "2022-03-25T10:42:29.275860: step 9357, loss 6.79438e-06, acc 1\n",
      "2022-03-25T10:42:29.501330: step 9358, loss 0.000104802, acc 1\n",
      "2022-03-25T10:42:29.720266: step 9359, loss 0.000128167, acc 1\n",
      "2022-03-25T10:42:29.879260: step 9360, loss 0.00813849, acc 1\n",
      "2022-03-25T10:42:30.027304: step 9361, loss 3.07652e-05, acc 1\n",
      "2022-03-25T10:42:30.186533: step 9362, loss 0.00010589, acc 1\n",
      "2022-03-25T10:42:30.338731: step 9363, loss 1.21765e-05, acc 1\n",
      "2022-03-25T10:42:30.495822: step 9364, loss 0.000588615, acc 1\n",
      "2022-03-25T10:42:30.655997: step 9365, loss 0.000173479, acc 1\n",
      "2022-03-25T10:42:30.808321: step 9366, loss 0.0136532, acc 0.984375\n",
      "2022-03-25T10:42:30.956578: step 9367, loss 0.00231502, acc 1\n",
      "2022-03-25T10:42:31.115284: step 9368, loss 8.43224e-05, acc 1\n",
      "2022-03-25T10:42:31.272494: step 9369, loss 0.0124631, acc 0.984375\n",
      "2022-03-25T10:42:31.420249: step 9370, loss 0.000104426, acc 1\n",
      "2022-03-25T10:42:31.577165: step 9371, loss 8.3396e-06, acc 1\n",
      "2022-03-25T10:42:31.731456: step 9372, loss 4.26094e-05, acc 1\n",
      "2022-03-25T10:42:31.889932: step 9373, loss 0.000419429, acc 1\n",
      "2022-03-25T10:42:32.045160: step 9374, loss 2.26417e-05, acc 1\n",
      "2022-03-25T10:42:32.190960: step 9375, loss 1.11692e-05, acc 1\n",
      "2022-03-25T10:42:32.339481: step 9376, loss 1.18462e-06, acc 1\n",
      "2022-03-25T10:42:32.499028: step 9377, loss 0.000287494, acc 1\n",
      "2022-03-25T10:42:32.655025: step 9378, loss 0.000137355, acc 1\n",
      "2022-03-25T10:42:32.803965: step 9379, loss 2.78832e-06, acc 1\n",
      "2022-03-25T10:42:32.960404: step 9380, loss 2.10479e-07, acc 1\n",
      "2022-03-25T10:42:33.107995: step 9381, loss 2.02837e-06, acc 1\n",
      "2022-03-25T10:42:33.263168: step 9382, loss 0.00191478, acc 1\n",
      "2022-03-25T10:42:33.414253: step 9383, loss 1.6074e-06, acc 1\n",
      "2022-03-25T10:42:33.570924: step 9384, loss 2.00847e-05, acc 1\n",
      "2022-03-25T10:42:33.734810: step 9385, loss 2.27591e-05, acc 1\n",
      "2022-03-25T10:42:33.886288: step 9386, loss 0.000593617, acc 1\n",
      "2022-03-25T10:42:34.033953: step 9387, loss 1.93956e-05, acc 1\n",
      "2022-03-25T10:42:34.188884: step 9388, loss 2.19435e-05, acc 1\n",
      "2022-03-25T10:42:34.338121: step 9389, loss 6.16498e-06, acc 1\n",
      "2022-03-25T10:42:34.494596: step 9390, loss 1.21132e-05, acc 1\n",
      "2022-03-25T10:42:34.649908: step 9391, loss 1.86798e-05, acc 1\n",
      "2022-03-25T10:42:34.801534: step 9392, loss 4.07918e-07, acc 1\n",
      "2022-03-25T10:42:34.952504: step 9393, loss 0.00125923, acc 1\n",
      "2022-03-25T10:42:35.116040: step 9394, loss 0.000418814, acc 1\n",
      "2022-03-25T10:42:35.267352: step 9395, loss 4.20931e-06, acc 1\n",
      "2022-03-25T10:42:35.412999: step 9396, loss 1.93715e-07, acc 1\n",
      "2022-03-25T10:42:35.566030: step 9397, loss 5.31915e-06, acc 1\n",
      "2022-03-25T10:42:35.731169: step 9398, loss 0.000649116, acc 1\n",
      "2022-03-25T10:42:35.881250: step 9399, loss 0.0142609, acc 0.984375\n",
      "2022-03-25T10:42:36.030502: step 9400, loss 1.75643e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:42:36.165202: step 9400, loss 0.966979, acc 0.918605\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9400\n",
      "\n",
      "2022-03-25T10:42:36.426250: step 9401, loss 2.68688e-05, acc 1\n",
      "2022-03-25T10:42:36.578778: step 9402, loss 0.00138194, acc 1\n",
      "2022-03-25T10:42:36.732368: step 9403, loss 9.17282e-05, acc 1\n",
      "2022-03-25T10:42:36.885401: step 9404, loss 6.41199e-06, acc 1\n",
      "2022-03-25T10:42:37.031469: step 9405, loss 0.000381477, acc 1\n",
      "2022-03-25T10:42:37.179738: step 9406, loss 3.01123e-05, acc 1\n",
      "2022-03-25T10:42:37.329605: step 9407, loss 0.0457042, acc 0.984375\n",
      "2022-03-25T10:42:37.477203: step 9408, loss 3.24093e-06, acc 1\n",
      "2022-03-25T10:42:37.629558: step 9409, loss 4.94012e-05, acc 1\n",
      "2022-03-25T10:42:37.796415: step 9410, loss 3.32995e-05, acc 1\n",
      "2022-03-25T10:42:37.952131: step 9411, loss 0.000694504, acc 1\n",
      "2022-03-25T10:42:38.098601: step 9412, loss 0.000681065, acc 1\n",
      "2022-03-25T10:42:38.254430: step 9413, loss 0.000537078, acc 1\n",
      "2022-03-25T10:42:38.399156: step 9414, loss 0.00277114, acc 1\n",
      "2022-03-25T10:42:38.550799: step 9415, loss 9.5657e-05, acc 1\n",
      "2022-03-25T10:42:38.695166: step 9416, loss 0.000472082, acc 1\n",
      "2022-03-25T10:42:38.860606: step 9417, loss 1.79022e-05, acc 1\n",
      "2022-03-25T10:42:39.011691: step 9418, loss 0.000179622, acc 1\n",
      "2022-03-25T10:42:39.173217: step 9419, loss 0.00254977, acc 1\n",
      "2022-03-25T10:42:39.330193: step 9420, loss 0.000743751, acc 1\n",
      "2022-03-25T10:42:39.487188: step 9421, loss 0.000957395, acc 1\n",
      "2022-03-25T10:42:39.655568: step 9422, loss 0.00159644, acc 1\n",
      "2022-03-25T10:42:39.804303: step 9423, loss 0.000198511, acc 1\n",
      "2022-03-25T10:42:39.967700: step 9424, loss 0.000257373, acc 1\n",
      "2022-03-25T10:42:40.123197: step 9425, loss 5.7986e-05, acc 1\n",
      "2022-03-25T10:42:40.280534: step 9426, loss 0.00581605, acc 1\n",
      "2022-03-25T10:42:40.435878: step 9427, loss 0.000165487, acc 1\n",
      "2022-03-25T10:42:40.597783: step 9428, loss 0.000240747, acc 1\n",
      "2022-03-25T10:42:40.761209: step 9429, loss 2.70444e-05, acc 1\n",
      "2022-03-25T10:42:40.929520: step 9430, loss 0.000159636, acc 1\n",
      "2022-03-25T10:42:41.078183: step 9431, loss 1.75088e-05, acc 1\n",
      "2022-03-25T10:42:41.235195: step 9432, loss 1.83017e-05, acc 1\n",
      "2022-03-25T10:42:41.395562: step 9433, loss 0.0192884, acc 0.984375\n",
      "2022-03-25T10:42:41.561826: step 9434, loss 0.0059654, acc 1\n",
      "2022-03-25T10:42:41.718149: step 9435, loss 2.25921e-05, acc 1\n",
      "2022-03-25T10:42:41.879232: step 9436, loss 5.18933e-05, acc 1\n",
      "2022-03-25T10:42:42.041585: step 9437, loss 0.00043133, acc 1\n",
      "2022-03-25T10:42:42.201717: step 9438, loss 1.96685e-05, acc 1\n",
      "2022-03-25T10:42:42.350887: step 9439, loss 0.00502185, acc 1\n",
      "2022-03-25T10:42:42.517153: step 9440, loss 0.000103969, acc 1\n",
      "2022-03-25T10:42:42.670077: step 9441, loss 1.12209e-05, acc 1\n",
      "2022-03-25T10:42:42.827337: step 9442, loss 2.6374e-06, acc 1\n",
      "2022-03-25T10:42:42.985099: step 9443, loss 4.16692e-05, acc 1\n",
      "2022-03-25T10:42:43.139745: step 9444, loss 0.000612063, acc 1\n",
      "2022-03-25T10:42:43.294389: step 9445, loss 5.20389e-06, acc 1\n",
      "2022-03-25T10:42:43.448133: step 9446, loss 0.00120869, acc 1\n",
      "2022-03-25T10:42:43.605077: step 9447, loss 7.86953e-05, acc 1\n",
      "2022-03-25T10:42:43.760912: step 9448, loss 0.000109918, acc 1\n",
      "2022-03-25T10:42:43.921986: step 9449, loss 0.032661, acc 0.984375\n",
      "2022-03-25T10:42:44.083161: step 9450, loss 2.74651e-05, acc 1\n",
      "2022-03-25T10:42:44.243745: step 9451, loss 1.27382e-05, acc 1\n",
      "2022-03-25T10:42:44.398999: step 9452, loss 0.000105437, acc 1\n",
      "2022-03-25T10:42:44.561107: step 9453, loss 0.00124508, acc 1\n",
      "2022-03-25T10:42:44.718560: step 9454, loss 0.000121896, acc 1\n",
      "2022-03-25T10:42:44.884531: step 9455, loss 0.000114187, acc 1\n",
      "2022-03-25T10:42:45.051925: step 9456, loss 6.64832e-05, acc 1\n",
      "2022-03-25T10:42:45.203357: step 9457, loss 3.81153e-05, acc 1\n",
      "2022-03-25T10:42:45.366214: step 9458, loss 7.55627e-05, acc 1\n",
      "2022-03-25T10:42:45.520625: step 9459, loss 0.00210661, acc 1\n",
      "2022-03-25T10:42:45.676097: step 9460, loss 2.38144e-05, acc 1\n",
      "2022-03-25T10:42:45.834493: step 9461, loss 1.78618e-06, acc 1\n",
      "2022-03-25T10:42:46.000676: step 9462, loss 3.08762e-05, acc 1\n",
      "2022-03-25T10:42:46.162302: step 9463, loss 0.000932546, acc 1\n",
      "2022-03-25T10:42:46.314159: step 9464, loss 0.000101696, acc 1\n",
      "2022-03-25T10:42:46.474223: step 9465, loss 0.0272653, acc 0.984375\n",
      "2022-03-25T10:42:46.632293: step 9466, loss 1.68844e-05, acc 1\n",
      "2022-03-25T10:42:46.792565: step 9467, loss 0.000669985, acc 1\n",
      "2022-03-25T10:42:46.953411: step 9468, loss 5.89658e-05, acc 1\n",
      "2022-03-25T10:42:47.118779: step 9469, loss 0.00625469, acc 1\n",
      "2022-03-25T10:42:47.282518: step 9470, loss 1.74278e-05, acc 1\n",
      "2022-03-25T10:42:47.430754: step 9471, loss 2.09534e-05, acc 1\n",
      "2022-03-25T10:42:47.594986: step 9472, loss 0.00159878, acc 1\n",
      "2022-03-25T10:42:47.746954: step 9473, loss 5.97907e-07, acc 1\n",
      "2022-03-25T10:42:47.912442: step 9474, loss 0.000295268, acc 1\n",
      "2022-03-25T10:42:48.085732: step 9475, loss 1.11571e-06, acc 1\n",
      "2022-03-25T10:42:48.239489: step 9476, loss 1.59999e-06, acc 1\n",
      "2022-03-25T10:42:48.395102: step 9477, loss 7.87444e-06, acc 1\n",
      "2022-03-25T10:42:48.550970: step 9478, loss 2.91679e-06, acc 1\n",
      "2022-03-25T10:42:48.711562: step 9479, loss 0.11446, acc 0.984375\n",
      "2022-03-25T10:42:48.875684: step 9480, loss 0.000781366, acc 1\n",
      "2022-03-25T10:42:49.029925: step 9481, loss 0.0121418, acc 0.984375\n",
      "2022-03-25T10:42:49.197891: step 9482, loss 3.95417e-06, acc 1\n",
      "2022-03-25T10:42:49.355626: step 9483, loss 3.82879e-05, acc 1\n",
      "2022-03-25T10:42:49.509067: step 9484, loss 5.80003e-06, acc 1\n",
      "2022-03-25T10:42:49.675620: step 9485, loss 2.30119e-05, acc 1\n",
      "2022-03-25T10:42:49.839334: step 9486, loss 4.86797e-05, acc 1\n",
      "2022-03-25T10:42:49.994974: step 9487, loss 0.000118753, acc 1\n",
      "2022-03-25T10:42:50.176069: step 9488, loss 3.35022e-05, acc 1\n",
      "2022-03-25T10:42:50.336269: step 9489, loss 0.00128585, acc 1\n",
      "2022-03-25T10:42:50.495017: step 9490, loss 0.0159908, acc 0.984375\n",
      "2022-03-25T10:42:50.658664: step 9491, loss 8.79525e-05, acc 1\n",
      "2022-03-25T10:42:50.819940: step 9492, loss 0.00153897, acc 1\n",
      "2022-03-25T10:42:50.977454: step 9493, loss 1.84953e-05, acc 1\n",
      "2022-03-25T10:42:51.150275: step 9494, loss 0.0106266, acc 1\n",
      "2022-03-25T10:42:51.305610: step 9495, loss 0.00793417, acc 1\n",
      "2022-03-25T10:42:51.463330: step 9496, loss 0.000373357, acc 1\n",
      "2022-03-25T10:42:51.613897: step 9497, loss 2.22342e-05, acc 1\n",
      "2022-03-25T10:42:51.777841: step 9498, loss 0.000307084, acc 1\n",
      "2022-03-25T10:42:51.931632: step 9499, loss 4.91384e-05, acc 1\n",
      "2022-03-25T10:42:52.093163: step 9500, loss 0.00811755, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:42:52.252006: step 9500, loss 0.883488, acc 0.914244\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9500\n",
      "\n",
      "2022-03-25T10:42:52.522378: step 9501, loss 1.67638e-05, acc 1\n",
      "2022-03-25T10:42:52.666906: step 9502, loss 1.97741e-05, acc 1\n",
      "2022-03-25T10:42:52.829876: step 9503, loss 5.4389e-07, acc 1\n",
      "2022-03-25T10:42:52.976033: step 9504, loss 1.41633e-05, acc 1\n",
      "2022-03-25T10:42:53.131190: step 9505, loss 0.000547923, acc 1\n",
      "2022-03-25T10:42:53.259143: step 9506, loss 9.70168e-06, acc 1\n",
      "2022-03-25T10:42:53.415973: step 9507, loss 2.12756e-05, acc 1\n",
      "2022-03-25T10:42:53.573012: step 9508, loss 0.140726, acc 0.984375\n",
      "2022-03-25T10:42:53.725787: step 9509, loss 5.85114e-05, acc 1\n",
      "2022-03-25T10:42:53.889868: step 9510, loss 1.58712e-05, acc 1\n",
      "2022-03-25T10:42:54.038775: step 9511, loss 0.000129243, acc 1\n",
      "2022-03-25T10:42:54.193166: step 9512, loss 0.00084952, acc 1\n",
      "2022-03-25T10:42:54.349089: step 9513, loss 0.00337793, acc 1\n",
      "2022-03-25T10:42:54.503424: step 9514, loss 0.0056745, acc 1\n",
      "2022-03-25T10:42:54.647381: step 9515, loss 7.75837e-06, acc 1\n",
      "2022-03-25T10:42:54.801751: step 9516, loss 1.26611e-05, acc 1\n",
      "2022-03-25T10:42:54.954915: step 9517, loss 2.57073e-05, acc 1\n",
      "2022-03-25T10:42:55.111489: step 9518, loss 8.41155e-05, acc 1\n",
      "2022-03-25T10:42:55.267009: step 9519, loss 7.78993e-05, acc 1\n",
      "2022-03-25T10:42:55.410755: step 9520, loss 3.71402e-06, acc 1\n",
      "2022-03-25T10:42:55.566967: step 9521, loss 3.90769e-06, acc 1\n",
      "2022-03-25T10:42:55.716839: step 9522, loss 2.96254e-05, acc 1\n",
      "2022-03-25T10:42:55.871721: step 9523, loss 0.000619488, acc 1\n",
      "2022-03-25T10:42:56.022956: step 9524, loss 0.00624388, acc 1\n",
      "2022-03-25T10:42:56.177783: step 9525, loss 0.00240949, acc 1\n",
      "2022-03-25T10:42:56.341837: step 9526, loss 3.70074e-05, acc 1\n",
      "2022-03-25T10:42:56.493571: step 9527, loss 0.0545515, acc 0.984375\n",
      "2022-03-25T10:42:56.643798: step 9528, loss 7.97333e-06, acc 1\n",
      "2022-03-25T10:42:56.792346: step 9529, loss 1.27215e-06, acc 1\n",
      "2022-03-25T10:42:56.948170: step 9530, loss 0.003647, acc 1\n",
      "2022-03-25T10:42:57.097322: step 9531, loss 1.94394e-05, acc 1\n",
      "2022-03-25T10:42:57.248565: step 9532, loss 0.00129344, acc 1\n",
      "2022-03-25T10:42:57.402958: step 9533, loss 0.000568275, acc 1\n",
      "2022-03-25T10:42:57.556512: step 9534, loss 3.36915e-05, acc 1\n",
      "2022-03-25T10:42:57.704159: step 9535, loss 1.95864e-05, acc 1\n",
      "2022-03-25T10:42:57.859707: step 9536, loss 5.99184e-06, acc 1\n",
      "2022-03-25T10:42:58.074246: step 9537, loss 1.68133e-05, acc 1\n",
      "2022-03-25T10:42:58.233128: step 9538, loss 2.32815e-06, acc 1\n",
      "2022-03-25T10:42:58.393635: step 9539, loss 0.000264617, acc 1\n",
      "2022-03-25T10:42:58.547913: step 9540, loss 2.64299e-06, acc 1\n",
      "2022-03-25T10:42:58.714283: step 9541, loss 1.36504e-05, acc 1\n",
      "2022-03-25T10:42:58.866387: step 9542, loss 0.0321754, acc 0.984375\n",
      "2022-03-25T10:42:59.017341: step 9543, loss 8.93595e-06, acc 1\n",
      "2022-03-25T10:42:59.168468: step 9544, loss 2.17351e-05, acc 1\n",
      "2022-03-25T10:42:59.323291: step 9545, loss 1.5449e-05, acc 1\n",
      "2022-03-25T10:42:59.473456: step 9546, loss 0.000118119, acc 1\n",
      "2022-03-25T10:42:59.622419: step 9547, loss 0.000126049, acc 1\n",
      "2022-03-25T10:42:59.777155: step 9548, loss 0.0174847, acc 0.984375\n",
      "2022-03-25T10:42:59.930760: step 9549, loss 2.66287e-05, acc 1\n",
      "2022-03-25T10:43:00.080813: step 9550, loss 1.56525e-05, acc 1\n",
      "2022-03-25T10:43:00.237494: step 9551, loss 0.000139551, acc 1\n",
      "2022-03-25T10:43:00.392938: step 9552, loss 3.69784e-05, acc 1\n",
      "2022-03-25T10:43:00.548113: step 9553, loss 2.71811e-05, acc 1\n",
      "2022-03-25T10:43:00.704130: step 9554, loss 3.86876e-05, acc 1\n",
      "2022-03-25T10:43:00.857166: step 9555, loss 0.000376761, acc 1\n",
      "2022-03-25T10:43:01.013718: step 9556, loss 0.0061018, acc 1\n",
      "2022-03-25T10:43:01.172904: step 9557, loss 0.000121514, acc 1\n",
      "2022-03-25T10:43:01.330504: step 9558, loss 0.0208521, acc 0.984375\n",
      "2022-03-25T10:43:01.496908: step 9559, loss 3.70121e-05, acc 1\n",
      "2022-03-25T10:43:01.649985: step 9560, loss 9.52477e-06, acc 1\n",
      "2022-03-25T10:43:01.795293: step 9561, loss 0.000756821, acc 1\n",
      "2022-03-25T10:43:01.942616: step 9562, loss 9.33636e-05, acc 1\n",
      "2022-03-25T10:43:02.100360: step 9563, loss 4.17218e-06, acc 1\n",
      "2022-03-25T10:43:02.251544: step 9564, loss 0.00158927, acc 1\n",
      "2022-03-25T10:43:02.406438: step 9565, loss 3.79679e-05, acc 1\n",
      "2022-03-25T10:43:02.556473: step 9566, loss 4.16597e-05, acc 1\n",
      "2022-03-25T10:43:02.714475: step 9567, loss 0.000257209, acc 1\n",
      "2022-03-25T10:43:02.868348: step 9568, loss 2.83706e-05, acc 1\n",
      "2022-03-25T10:43:03.034872: step 9569, loss 5.04909e-05, acc 1\n",
      "2022-03-25T10:43:03.188118: step 9570, loss 0.000282275, acc 1\n",
      "2022-03-25T10:43:03.335092: step 9571, loss 0.0511679, acc 0.984375\n",
      "2022-03-25T10:43:03.502178: step 9572, loss 5.7067e-06, acc 1\n",
      "2022-03-25T10:43:03.657178: step 9573, loss 0.000689118, acc 1\n",
      "2022-03-25T10:43:03.811578: step 9574, loss 0.000181719, acc 1\n",
      "2022-03-25T10:43:03.962879: step 9575, loss 0.000240097, acc 1\n",
      "2022-03-25T10:43:04.121792: step 9576, loss 0.00374804, acc 1\n",
      "2022-03-25T10:43:04.270630: step 9577, loss 0.00012711, acc 1\n",
      "2022-03-25T10:43:04.420933: step 9578, loss 5.1046e-05, acc 1\n",
      "2022-03-25T10:43:04.585573: step 9579, loss 0.010992, acc 0.984375\n",
      "2022-03-25T10:43:04.737835: step 9580, loss 9.79983e-05, acc 1\n",
      "2022-03-25T10:43:04.893993: step 9581, loss 5.47473e-05, acc 1\n",
      "2022-03-25T10:43:05.041989: step 9582, loss 0.000203313, acc 1\n",
      "2022-03-25T10:43:05.199435: step 9583, loss 4.08637e-06, acc 1\n",
      "2022-03-25T10:43:05.352173: step 9584, loss 2.97639e-06, acc 1\n",
      "2022-03-25T10:43:05.507969: step 9585, loss 0.000231716, acc 1\n",
      "2022-03-25T10:43:05.668776: step 9586, loss 0.000404689, acc 1\n",
      "2022-03-25T10:43:05.819738: step 9587, loss 2.44477e-05, acc 1\n",
      "2022-03-25T10:43:05.969487: step 9588, loss 0.000360665, acc 1\n",
      "2022-03-25T10:43:06.124917: step 9589, loss 0.000485135, acc 1\n",
      "2022-03-25T10:43:06.283392: step 9590, loss 0.00069516, acc 1\n",
      "2022-03-25T10:43:06.434917: step 9591, loss 0.0160202, acc 0.984375\n",
      "2022-03-25T10:43:06.601193: step 9592, loss 0.00116587, acc 1\n",
      "2022-03-25T10:43:06.758645: step 9593, loss 9.20615e-05, acc 1\n",
      "2022-03-25T10:43:06.915162: step 9594, loss 6.33093e-05, acc 1\n",
      "2022-03-25T10:43:07.067315: step 9595, loss 9.69539e-05, acc 1\n",
      "2022-03-25T10:43:07.221275: step 9596, loss 4.43755e-05, acc 1\n",
      "2022-03-25T10:43:07.370886: step 9597, loss 2.24132e-05, acc 1\n",
      "2022-03-25T10:43:07.527087: step 9598, loss 6.22029e-06, acc 1\n",
      "2022-03-25T10:43:07.680143: step 9599, loss 3.37207e-05, acc 1\n",
      "2022-03-25T10:43:07.835525: step 9600, loss 2.58354e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:43:07.971903: step 9600, loss 0.937855, acc 0.920058\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9600\n",
      "\n",
      "2022-03-25T10:43:08.248382: step 9601, loss 0.000262551, acc 1\n",
      "2022-03-25T10:43:08.402222: step 9602, loss 1.7439e-05, acc 1\n",
      "2022-03-25T10:43:08.556839: step 9603, loss 0.000766452, acc 1\n",
      "2022-03-25T10:43:08.713930: step 9604, loss 5.77417e-07, acc 1\n",
      "2022-03-25T10:43:08.864200: step 9605, loss 5.81142e-07, acc 1\n",
      "2022-03-25T10:43:09.016448: step 9606, loss 0.00306625, acc 1\n",
      "2022-03-25T10:43:09.171214: step 9607, loss 0.00024989, acc 1\n",
      "2022-03-25T10:43:09.323864: step 9608, loss 2.6714e-05, acc 1\n",
      "2022-03-25T10:43:09.473185: step 9609, loss 6.6411e-06, acc 1\n",
      "2022-03-25T10:43:09.639155: step 9610, loss 0.000221961, acc 1\n",
      "2022-03-25T10:43:09.797257: step 9611, loss 4.14041e-06, acc 1\n",
      "2022-03-25T10:43:09.955163: step 9612, loss 0.000210064, acc 1\n",
      "2022-03-25T10:43:10.110370: step 9613, loss 1.06266e-05, acc 1\n",
      "2022-03-25T10:43:10.259032: step 9614, loss 4.08166e-05, acc 1\n",
      "2022-03-25T10:43:10.408481: step 9615, loss 6.68448e-06, acc 1\n",
      "2022-03-25T10:43:10.566815: step 9616, loss 0.000653668, acc 1\n",
      "2022-03-25T10:43:10.732553: step 9617, loss 5.06055e-06, acc 1\n",
      "2022-03-25T10:43:10.882498: step 9618, loss 3.97496e-05, acc 1\n",
      "2022-03-25T10:43:11.036887: step 9619, loss 6.12453e-05, acc 1\n",
      "2022-03-25T10:43:11.193519: step 9620, loss 0.0023986, acc 1\n",
      "2022-03-25T10:43:11.348196: step 9621, loss 3.84074e-05, acc 1\n",
      "2022-03-25T10:43:11.500135: step 9622, loss 0.00018209, acc 1\n",
      "2022-03-25T10:43:11.659603: step 9623, loss 0.00114459, acc 1\n",
      "2022-03-25T10:43:11.820318: step 9624, loss 7.7037e-05, acc 1\n",
      "2022-03-25T10:43:11.973569: step 9625, loss 0.00540228, acc 1\n",
      "2022-03-25T10:43:12.134580: step 9626, loss 4.54059e-06, acc 1\n",
      "2022-03-25T10:43:12.294623: step 9627, loss 3.51277e-06, acc 1\n",
      "2022-03-25T10:43:12.450930: step 9628, loss 6.91192e-06, acc 1\n",
      "2022-03-25T10:43:12.608623: step 9629, loss 0.000578977, acc 1\n",
      "2022-03-25T10:43:12.768178: step 9630, loss 6.37859e-06, acc 1\n",
      "2022-03-25T10:43:12.929269: step 9631, loss 1.44942e-05, acc 1\n",
      "2022-03-25T10:43:13.080485: step 9632, loss 0.00274008, acc 1\n",
      "2022-03-25T10:43:13.234316: step 9633, loss 7.66985e-06, acc 1\n",
      "2022-03-25T10:43:13.394469: step 9634, loss 8.64355e-05, acc 1\n",
      "2022-03-25T10:43:13.549811: step 9635, loss 0.00985538, acc 1\n",
      "2022-03-25T10:43:13.711797: step 9636, loss 0.00604037, acc 1\n",
      "2022-03-25T10:43:13.868139: step 9637, loss 1.22002e-06, acc 1\n",
      "2022-03-25T10:43:14.023269: step 9638, loss 0.00221706, acc 1\n",
      "2022-03-25T10:43:14.172785: step 9639, loss 1.80336e-05, acc 1\n",
      "2022-03-25T10:43:14.327097: step 9640, loss 1.86386e-05, acc 1\n",
      "2022-03-25T10:43:14.479417: step 9641, loss 2.07488e-06, acc 1\n",
      "2022-03-25T10:43:14.639234: step 9642, loss 3.55006e-06, acc 1\n",
      "2022-03-25T10:43:14.802308: step 9643, loss 0.000573225, acc 1\n",
      "2022-03-25T10:43:14.963826: step 9644, loss 0.000337522, acc 1\n",
      "2022-03-25T10:43:15.116580: step 9645, loss 0.000103847, acc 1\n",
      "2022-03-25T10:43:15.261752: step 9646, loss 0.0208104, acc 0.984375\n",
      "2022-03-25T10:43:15.416178: step 9647, loss 0.000217197, acc 1\n",
      "2022-03-25T10:43:15.574777: step 9648, loss 5.72301e-06, acc 1\n",
      "2022-03-25T10:43:15.722290: step 9649, loss 0.000435398, acc 1\n",
      "2022-03-25T10:43:15.882346: step 9650, loss 1.8774e-05, acc 1\n",
      "2022-03-25T10:43:16.034581: step 9651, loss 1.12585e-05, acc 1\n",
      "2022-03-25T10:43:16.190193: step 9652, loss 0.00048697, acc 1\n",
      "2022-03-25T10:43:16.348850: step 9653, loss 0.000341423, acc 1\n",
      "2022-03-25T10:43:16.498686: step 9654, loss 1.33408e-05, acc 1\n",
      "2022-03-25T10:43:16.659368: step 9655, loss 0.0893868, acc 0.984375\n",
      "2022-03-25T10:43:16.825562: step 9656, loss 4.44024e-05, acc 1\n",
      "2022-03-25T10:43:16.975939: step 9657, loss 0.0491676, acc 0.984375\n",
      "2022-03-25T10:43:17.142973: step 9658, loss 7.56896e-05, acc 1\n",
      "2022-03-25T10:43:17.296830: step 9659, loss 4.69634e-05, acc 1\n",
      "2022-03-25T10:43:17.449170: step 9660, loss 1.30671e-05, acc 1\n",
      "2022-03-25T10:43:17.601950: step 9661, loss 2.56486e-05, acc 1\n",
      "2022-03-25T10:43:17.763214: step 9662, loss 0.00301622, acc 1\n",
      "2022-03-25T10:43:17.913657: step 9663, loss 1.18703e-05, acc 1\n",
      "2022-03-25T10:43:18.061948: step 9664, loss 0.0845874, acc 0.984375\n",
      "2022-03-25T10:43:18.208899: step 9665, loss 1.12314e-06, acc 1\n",
      "2022-03-25T10:43:18.368713: step 9666, loss 1.50311e-06, acc 1\n",
      "2022-03-25T10:43:18.515894: step 9667, loss 0.00505798, acc 1\n",
      "2022-03-25T10:43:18.668171: step 9668, loss 5.29991e-05, acc 1\n",
      "2022-03-25T10:43:18.823863: step 9669, loss 2.11858e-05, acc 1\n",
      "2022-03-25T10:43:18.984828: step 9670, loss 0.000370764, acc 1\n",
      "2022-03-25T10:43:19.138782: step 9671, loss 5.39944e-06, acc 1\n",
      "2022-03-25T10:43:19.295375: step 9672, loss 2.27654e-05, acc 1\n",
      "2022-03-25T10:43:19.443840: step 9673, loss 7.19276e-06, acc 1\n",
      "2022-03-25T10:43:19.594091: step 9674, loss 2.61267e-05, acc 1\n",
      "2022-03-25T10:43:19.749617: step 9675, loss 4.61062e-05, acc 1\n",
      "2022-03-25T10:43:19.911557: step 9676, loss 5.43586e-05, acc 1\n",
      "2022-03-25T10:43:20.063210: step 9677, loss 0.000778427, acc 1\n",
      "2022-03-25T10:43:20.210905: step 9678, loss 1.3607e-05, acc 1\n",
      "2022-03-25T10:43:20.367882: step 9679, loss 0.000101302, acc 1\n",
      "2022-03-25T10:43:20.518384: step 9680, loss 1.47323e-05, acc 1\n",
      "2022-03-25T10:43:20.670793: step 9681, loss 1.41561e-05, acc 1\n",
      "2022-03-25T10:43:20.823972: step 9682, loss 8.23935e-05, acc 1\n",
      "2022-03-25T10:43:20.982326: step 9683, loss 1.32311e-05, acc 1\n",
      "2022-03-25T10:43:21.128414: step 9684, loss 9.78455e-05, acc 1\n",
      "2022-03-25T10:43:21.280902: step 9685, loss 9.24635e-05, acc 1\n",
      "2022-03-25T10:43:21.437818: step 9686, loss 5.15718e-06, acc 1\n",
      "2022-03-25T10:43:21.581702: step 9687, loss 1.5282e-05, acc 1\n",
      "2022-03-25T10:43:21.732178: step 9688, loss 1.43607e-06, acc 1\n",
      "2022-03-25T10:43:21.876678: step 9689, loss 2.59492e-05, acc 1\n",
      "2022-03-25T10:43:22.025193: step 9690, loss 4.90693e-05, acc 1\n",
      "2022-03-25T10:43:22.170167: step 9691, loss 7.19967e-06, acc 1\n",
      "2022-03-25T10:43:22.308820: step 9692, loss 1.52363e-06, acc 1\n",
      "2022-03-25T10:43:22.458651: step 9693, loss 0.000219526, acc 1\n",
      "2022-03-25T10:43:22.599631: step 9694, loss 0.000406973, acc 1\n",
      "2022-03-25T10:43:22.747028: step 9695, loss 2.05258e-06, acc 1\n",
      "2022-03-25T10:43:22.893989: step 9696, loss 7.67759e-05, acc 1\n",
      "2022-03-25T10:43:23.042008: step 9697, loss 1.31264e-05, acc 1\n",
      "2022-03-25T10:43:23.182916: step 9698, loss 4.23914e-06, acc 1\n",
      "2022-03-25T10:43:23.335738: step 9699, loss 1.72946e-05, acc 1\n",
      "2022-03-25T10:43:23.448001: step 9700, loss 2.78321e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2022-03-25T10:43:23.586223: step 9700, loss 0.924325, acc 0.915698\n",
      "\n",
      "Saved model checkpoint to /content/runs/1648203506/checkpoints/model-9700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(x_train, y_train, vocab_processor, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "H53_p38arMie",
    "outputId": "566bd4db-441a-4faa-f7f9-b9bf69890928"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(train_losses[::100], label=\"Training\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('loss.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "c7IMFT_ExZbG",
    "outputId": "31498c83-47a0-4299-da28-057a8a8759e0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(train_accs[::100], label=\"Training\")\n",
    "plt.plot(val_accs, label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ichu2_TwREm",
    "outputId": "8daf12e5-3600-48d4-c0a3-f5ad0ef7e9d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9700"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjqBVSFT0eF5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "FvnOTQ5z0lZ6",
    "outputId": "f9bf1bfd-17a2-40cc-84f3-73b18790c662"
   },
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bfbc286de37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positive_data_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/rt-polaritydata/rt-polarity.pos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data source for the positive data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative_data_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/rt-polaritydata/rt-polarity.neg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data source for the negative data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Eval Parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Batch Size (default: 64)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, required, **args)\u001b[0m\n\u001b[1;32m    292\u001b[0m       \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m       **args)\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, required, **args)\u001b[0m\n\u001b[1;32m    104\u001b[0m   return DEFINE_flag(\n\u001b[1;32m    105\u001b[0m       \u001b[0m_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       module_name, required)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name, required)\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'positive_data_file' is defined twice. First from /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py, Second from /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py.  Description from first occurrence: Data source for the positive data."
     ]
    }
   ],
   "source": [
    "#tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "#tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", False, \"Evaluate on all training data\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKdJQurj0rcG"
   },
   "outputs": [],
   "source": [
    "# CHANGE THIS: Load data. Load your own data here\n",
    "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/thesis_PQAI/grammer_error_dataset/test.csv')\n",
    "x_raw = df_test.abstracts.values\n",
    "y_test = np.eye(2)[df_test.grammatically_incorrect.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hZDW4RS0sF-"
   },
   "outputs": [],
   "source": [
    "# Map data into vocabulary\n",
    "vocab_path = os.path.join('/content/runs/1648203506/checkpoints', \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fY_PfBtd0xhI",
    "outputId": "ab4d1ac1-a062-4b54-a411-5f998f080b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /content/runs/1648203506/checkpoints/model-9700\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating...\\n\")\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint('/content/runs/1648203506/checkpoints')\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = batch_iter(list(x_test), 64, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltHTpQ_74FgU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3z1mflU33Gp",
    "outputId": "62eef19b-9c4c-40ab-99ae-2c0cf4236b85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9010043041606887"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(y_test,axis=1),all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LO_GtL8y4c8l",
    "outputId": "8b9d8fc0-f710-4999-8e87-34b48927c292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8201438848920863"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(np.argmax(y_test,axis=1),all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoYlpZ8T4da3",
    "outputId": "7aa8f14d-917a-441a-f282-38081fd6f61b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7215189873417721"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(np.argmax(y_test,axis=1),all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bORcjCrx4rqe",
    "outputId": "bdf83478-4633-4de6-daab-cb9861780476"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676767676767676"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.argmax(y_test,axis=1),all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l98OApiP42KU",
    "outputId": "b8871ac6-1875-469f-f33e-6cf8b4e11cac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[514,  25],\n",
       "       [ 44, 114]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(np.argmax(y_test,axis=1),all_predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "d7uBU4XG2ngG",
    "outputId": "08cd6095-596a-4aea-df76-16a5d4c71d4d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap='PuBuGn'); #annot=True to annotate cells\n",
    "sns.set(font_scale=1.4)\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels').set_fontsize('18')\n",
    "ax.set_ylabel('True labels').set_fontsize('18')\n",
    "ax.set_title('Confusion Matrix - Baseline',pad = 30).set_fontsize('20')\n",
    "ax.xaxis.set_ticklabels(['Gramm. Correct','Gramm. Incorrect'],rotation=0)\n",
    "ax.yaxis.set_ticklabels(['Gramm. Correct','Gramm. Incorrect'],rotation=0)\n",
    "plt.savefig('cm.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvHK1FQw5H-t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "013f9ea7fe5d4250a73d70335f4fae23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "046489e804ed43a083268204cd680585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "079868b647f84c9abe244c211927ad87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2afe42dbbbc44f886ecf44e47e6af11",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c977b73879d448c9157f84841f4a810",
      "value": 0
     }
    },
    "0b04362cc9e74eea94b02bd61c675de4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b864a89a684486d8c4fdb21c326a149": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b98666d4e6f45318838ef815df1733a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1995bda78a184218bfd9c3ecfe3eac72",
      "placeholder": "​",
      "style": "IPY_MODEL_70aefddc86304776ab0ea575af02c1cc",
      "value": ""
     }
    },
    "0c977b73879d448c9157f84841f4a810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "111b052a5afb42e89700f8605e9a4b1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32ab58b7457547138c3320f4bda6f418",
      "placeholder": "​",
      "style": "IPY_MODEL_d65d791428784b84abf983ed73c80e90",
      "value": " 329k/329k [00:00&lt;00:00, 656kB/s]"
     }
    },
    "15b0f907018e49adb5280bcb9c584e2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1995bda78a184218bfd9c3ecfe3eac72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fcd3afb6cfa4dd6a96647a9301c1e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "228ac4df07c64e0fa7c437cd41f9c989": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fce305678b1a429bb91d1994ae3f06d8",
      "placeholder": "​",
      "style": "IPY_MODEL_b893dbebfd524633bb209688e7efd543",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "2ffa06c691ac47bd8433ed37f323aa5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32ab58b7457547138c3320f4bda6f418": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e839a1ae58742358aef97945644dfb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_460a41290c7a4fe0bac5fa3465f0f856",
      "placeholder": "​",
      "style": "IPY_MODEL_56437b6cfa574086b0c735c1ffa76610",
      "value": "Downloading: 100%"
     }
    },
    "435750df0bbb4852bbeeb2652c6d098c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "460a41290c7a4fe0bac5fa3465f0f856": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bc17b26fb58460fb1481b26c4273e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68138007ea8d4831bf070709133d1044",
       "IPY_MODEL_551f59f1100548ad93bb38169dc7796c",
       "IPY_MODEL_111b052a5afb42e89700f8605e9a4b1d"
      ],
      "layout": "IPY_MODEL_e0f115bc99a549f6b61e874f4b51f5ee"
     }
    },
    "551f59f1100548ad93bb38169dc7796c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ffa06c691ac47bd8433ed37f323aa5a",
      "max": 329241,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c3e9b59739124ea0b3fbfe0897cb94c0",
      "value": 329241
     }
    },
    "56437b6cfa574086b0c735c1ffa76610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d348978a2c34704afb30160669301d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68138007ea8d4831bf070709133d1044": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb9bd771ca224433bb8f70435175f4e7",
      "placeholder": "​",
      "style": "IPY_MODEL_b09ccec106ae4d49bf097b315aae9ecc",
      "value": "Downloading: 100%"
     }
    },
    "6fb47e74166d4625810fc6fab9225a48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da9cd7a96c3d40e28d51d06ad0987337",
       "IPY_MODEL_8dae5d96462a4c99b927b9c0c3e81cfc",
       "IPY_MODEL_802ca04b8d6a4960ac766fe7d41fb046"
      ],
      "layout": "IPY_MODEL_0b864a89a684486d8c4fdb21c326a149"
     }
    },
    "6ff74b5bd64c4d33ab223183f7770a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b6df9a148a349e2bfae507c82690606",
      "max": 327,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bc100704a10a47dfbcc10eb05c0e8da6",
      "value": 327
     }
    },
    "70aefddc86304776ab0ea575af02c1cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "802ca04b8d6a4960ac766fe7d41fb046": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15b0f907018e49adb5280bcb9c584e2d",
      "placeholder": "​",
      "style": "IPY_MODEL_1fcd3afb6cfa4dd6a96647a9301c1e38",
      "value": " 1.38G/1.38G [00:31&lt;00:00, 59.0MB/s]"
     }
    },
    "8dae5d96462a4c99b927b9c0c3e81cfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b04362cc9e74eea94b02bd61c675de4",
      "max": 1383349812,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e549ee1e7d544af5ad6f767d798bda57",
      "value": 1383349812
     }
    },
    "98c521a59c8549a4899803b7e4d78cd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b6df9a148a349e2bfae507c82690606": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b09ccec106ae4d49bf097b315aae9ecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b893dbebfd524633bb209688e7efd543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc100704a10a47dfbcc10eb05c0e8da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1fb1df089a7451c9706df2ca25a71ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_435750df0bbb4852bbeeb2652c6d098c",
      "placeholder": "​",
      "style": "IPY_MODEL_5d348978a2c34704afb30160669301d5",
      "value": " 327/327 [00:00&lt;00:00, 2.58kB/s]"
     }
    },
    "c3e9b59739124ea0b3fbfe0897cb94c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cad52571456142d88142a33c1879d6c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0b98666d4e6f45318838ef815df1733a",
       "IPY_MODEL_079868b647f84c9abe244c211927ad87",
       "IPY_MODEL_228ac4df07c64e0fa7c437cd41f9c989"
      ],
      "layout": "IPY_MODEL_e060ea4f3b6c468aa6cb42025affb08e"
     }
    },
    "cb9bd771ca224433bb8f70435175f4e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cccc4c58c0ee421c8f960e27148d5c60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e839a1ae58742358aef97945644dfb0",
       "IPY_MODEL_6ff74b5bd64c4d33ab223183f7770a1e",
       "IPY_MODEL_c1fb1df089a7451c9706df2ca25a71ae"
      ],
      "layout": "IPY_MODEL_98c521a59c8549a4899803b7e4d78cd6"
     }
    },
    "d2afe42dbbbc44f886ecf44e47e6af11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d65d791428784b84abf983ed73c80e90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da9cd7a96c3d40e28d51d06ad0987337": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_046489e804ed43a083268204cd680585",
      "placeholder": "​",
      "style": "IPY_MODEL_013f9ea7fe5d4250a73d70335f4fae23",
      "value": "Downloading: 100%"
     }
    },
    "e060ea4f3b6c468aa6cb42025affb08e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0f115bc99a549f6b61e874f4b51f5ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e549ee1e7d544af5ad6f767d798bda57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fce305678b1a429bb91d1994ae3f06d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
